{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e1c8e9",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0630d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Settings\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pymongo\")\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5141d0",
   "metadata": {},
   "source": [
    "### Main analysis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd2ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: CombinedParliamentaryAnalyzer Class\n",
    "class CombinedParliamentaryAnalyzer:\n",
    "    def __init__(self, honorific_dict: Dict):\n",
    "        self.honorific_dict = honorific_dict\n",
    "        self.pattern_discoveries = {\n",
    "            'decade_headers': defaultdict(Counter),\n",
    "            'honorific_variations': defaultdict(Counter),\n",
    "            'language_code_switching': defaultdict(Counter),\n",
    "        }\n",
    "        self.completion_stats = {\n",
    "            'header': {'found': 0, 'created': 0, 'failed': 0},\n",
    "            'attendance': {'found': 0, 'created': 0, 'failed': 0},\n",
    "            'discussion_start': {'found': 0, 'created': 0, 'failed': 0}\n",
    "        }\n",
    "\n",
    "    def run_complete_analysis(self, docs: List[Dict], sample_size: int = 500) -> Dict:\n",
    "        completed = self.complete_all_documents(docs)\n",
    "        patterns = self.analyze_document_patterns(completed, sample_size)\n",
    "        return {\n",
    "            'data_completion': {\n",
    "                'original_document_count': len(docs),\n",
    "                'completed_document_count': len(completed),\n",
    "                'completion_statistics': self.completion_stats,\n",
    "                'field_coverage': self.verify_field_coverage(completed)\n",
    "            },\n",
    "            'pattern_analysis': patterns\n",
    "        }\n",
    "\n",
    "    def complete_all_documents(self, docs: List[Dict]) -> List[Dict]:\n",
    "        result = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"  Completing {i+1}/{len(docs)}\")\n",
    "            d = doc.copy()\n",
    "            d['header'] = self.ensure_header(doc)\n",
    "            d['attendance'] = self.ensure_attendance(doc)\n",
    "            d['discussion_start'] = self.ensure_discussion_start(doc)\n",
    "            result.append(d)\n",
    "        self.print_completion_summary(len(docs))\n",
    "        return result\n",
    "\n",
    "    def ensure_header(self, doc: Dict) -> str:\n",
    "        if (h := doc.get('header', '')) and len(h.strip()) > 5:\n",
    "            self.completion_stats['header']['found'] += 1\n",
    "            return h\n",
    "        if (txt := doc.get('full_text', '')):\n",
    "            if (ex := self.extract_header_from_text(txt)):\n",
    "                self.completion_stats['header']['created'] += 1\n",
    "                return ex\n",
    "        fb = self.create_header_from_metadata(doc) or \"DEWAN RAKYAT\"\n",
    "        self.completion_stats['header']['created'] += 1\n",
    "        return fb\n",
    "\n",
    "    def ensure_attendance(self, doc: Dict) -> List[Dict]:\n",
    "        if doc.get('attendance'):\n",
    "            self.completion_stats['attendance']['found'] += 1\n",
    "            return doc['attendance']\n",
    "        for field in ['full_text', 'content_text', 'text']:\n",
    "            if (txt := doc.get(field, '')):\n",
    "                if (ex := self.extract_attendance_from_text(txt)):\n",
    "                    self.completion_stats['attendance']['created'] += 1\n",
    "                    return ex\n",
    "        self.completion_stats['attendance']['created'] += 1\n",
    "        return [{'note': 'Attendance not found'}]\n",
    "\n",
    "    def ensure_discussion_start(self, doc: Dict) -> str:\n",
    "        if (s := doc.get('discussion_start', '')) and len(s.strip()) > 5:\n",
    "            self.completion_stats['discussion_start']['found'] += 1\n",
    "            return s\n",
    "        if (txt := doc.get('full_text', '')):\n",
    "            if (ex := self.extract_discussion_start_from_text(txt)):\n",
    "                self.completion_stats['discussion_start']['created'] += 1\n",
    "                return ex\n",
    "        date = doc.get('hansardDate', '')\n",
    "        fb = f\"Mesyuarat dimulakan pada {date}\" if date else \"Mesyuarat dimulakan\"\n",
    "        self.completion_stats['discussion_start']['created'] += 1\n",
    "        return fb\n",
    "\n",
    "    def extract_header_from_text(self, txt: str) -> Optional[str]:\n",
    "        lines = txt.split('\\n')[:20]\n",
    "        header = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            if (len(line) < 120 and\n",
    "                (line.isupper() or\n",
    "                 any(k in line.upper() for k in ['DEWAN', 'PARLIMEN', 'MESYUARAT', 'BIL']) or\n",
    "                 re.match(r'^(Bil|DR|Page|No)[\\.\\s]*\\d+', line, re.I))):\n",
    "                header.append(line)\n",
    "            elif ':' in line and any(t in line for t in ['Yang Berhormat', 'Dato']):\n",
    "                break\n",
    "        return '\\n'.join(header) if header else None\n",
    "\n",
    "    def create_header_from_metadata(self, doc: Dict) -> str:\n",
    "        parts = ['DEWAN RAKYAT']\n",
    "        if (date := doc.get('hansardDate')):\n",
    "            try:\n",
    "                d = datetime.strptime(date[:10], '%Y-%m-%d')\n",
    "                malay_days = ['Isnin', 'Selasa', 'Rabu', 'Khamis', 'Jumaat', 'Sabtu', 'Ahad']\n",
    "                malay_months = ['Januari', 'Februari', 'Mac', 'April', 'Mei', 'Jun',\n",
    "                                'Julai', 'Ogos', 'September', 'Oktober', 'November', 'Disember']\n",
    "                parts.append(f\"{malay_days[d.weekday()]}, {d.day} {malay_months[d.month-1]} {d.year}\")\n",
    "            except: parts.append(str(date))\n",
    "        return '\\n'.join(parts)\n",
    "\n",
    "    def extract_discussion_start_from_text(self, txt: str) -> Optional[str]:\n",
    "        lines = txt.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line or len(line) < 20 or any(k in line.upper() for k in ['DEWAN', 'KEHADIRAN']): continue\n",
    "            if any(p in line.lower() for p in ['yang berhormat', 'tuan speaker']) and ':' in line:\n",
    "                return '\\n'.join(lines[max(0,i-1):i+2]).strip()\n",
    "        return None\n",
    "\n",
    "    def extract_attendance_from_text(self, txt: str) -> Optional[List[Dict]]:\n",
    "        modern = [\n",
    "            r'KEHADIRAN\\s+AHLI[\\s-]*AHLI\\s+PARLIMEN[\\s\\S]*?(?=Ahli[\\s-]*Ahli\\s+Yang\\s+Tidak|Senator|PERTANYAAN)',\n",
    "            r'Ahli[\\s-]*Ahli\\s+Yang\\s+Hadir\\s*[:\\-][\\s\\S]*?(?=Ahli[\\s-]*Ahli\\s+Yang\\s+Tidak|Senator)',\n",
    "        ]\n",
    "        for p in modern:\n",
    "            if (m := re.search(p, txt, re.I)):\n",
    "                parsed = self._parse_modern_attendance(m.group(0))\n",
    "                if len(parsed) >= 50: return parsed\n",
    "\n",
    "        historical = [\n",
    "            r'PRESENT\\s*[:\\-][\\s\\S]*?(?=ABSENT|QUESTIONS|The sitting)',\n",
    "            r'MEMBERS\\s+PRESENT[\\s\\S]*?(?=MEMBERS\\s+ABSENT|ABSENT)',\n",
    "        ]\n",
    "        for p in historical:\n",
    "            if (m := re.search(p, txt, re.I)):\n",
    "                parsed = self._parse_historical_attendance(m.group(0))\n",
    "                if len(parsed) >= 30: return parsed\n",
    "        return None\n",
    "\n",
    "    def _parse_modern_attendance(self, block: str) -> List[Dict]:\n",
    "        entries = []\n",
    "        lines = [l.strip() for l in block.split('\\n') if l.strip()]\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            if any(h in line.upper() for h in ['KEHADIRAN', 'AHLI', 'HADIR']): \n",
    "                i += 1; continue\n",
    "            m = re.match(r'(\\d+)\\.\\s*(.+)', line)\n",
    "            if not m: \n",
    "                i += 1; continue\n",
    "            num, content = m.groups()\n",
    "            i += 1\n",
    "            while i < len(lines) and not re.match(r'^\\d+\\.', lines[i]):\n",
    "                content += \" \" + lines[i].strip()\n",
    "                i += 1\n",
    "            const = re.search(r'[\\[\\(]([^]\\)]+)[\\]\\)]', content)\n",
    "            constituency = const.group(1).strip() if const else \"\"\n",
    "            name_part = re.sub(r'[\\[\\(][^]\\)]+[\\]\\)]', '', content).strip()\n",
    "            title, name = self._split_title_name(name_part)\n",
    "            entries.append({\n",
    "                'number': int(num),\n",
    "                'title': title,\n",
    "                'name': name or name_part,\n",
    "                'constituency': constituency,\n",
    "                'party': self._extract_party(constituency),\n",
    "                'format': 'modern'\n",
    "            })\n",
    "        return self._deduplicate_entries(entries)\n",
    "\n",
    "    def _parse_historical_attendance(self, block: str) -> List[Dict]:\n",
    "        entries = []\n",
    "        for line in block.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if any(skip in line.upper() for skip in ['PRESENT', 'MEMBERS']): continue\n",
    "            m = re.match(r\"(?:The Honourable )?([A-Z]['A-Z\\s]+)\\s*\\(([^)]+)\\)\", line, re.I)\n",
    "            if not m:\n",
    "                m = re.match(r\"([A-Z]['A-Z\\s]+)\\s*\\(([^)]+)\\)\", line, re.I)\n",
    "            if not m: continue\n",
    "            name_part, constituency = m.groups()\n",
    "            title, name = self._split_title_name(name_part)\n",
    "            entries.append({\n",
    "                'number': len(entries) + 1,\n",
    "                'title': title,\n",
    "                'name': name or name_part,\n",
    "                'constituency': constituency,\n",
    "                'party': None,\n",
    "                'format': 'historical'\n",
    "            })\n",
    "        return self._deduplicate_entries(entries)\n",
    "\n",
    "    def _split_title_name(self, text: str) -> tuple[str, str]:\n",
    "        matched_titles = []\n",
    "        remaining = text\n",
    "        for raw, std in sorted(self.honorific_dict.items(), key=lambda x: len(x[0]), reverse=True):\n",
    "            pattern = re.escape(raw)\n",
    "            if re.search(rf'\\b{pattern}\\b', text, re.I):\n",
    "                matched_titles.append(std)\n",
    "                remaining = re.sub(rf'\\b{pattern}\\b', '', remaining, flags=re.I)\n",
    "        title_str = ' '.join(matched_titles)\n",
    "        name = re.sub(r'\\s+', ' ', remaining.strip())\n",
    "        return title_str, name\n",
    "\n",
    "    def _extract_party(self, const: str) -> Optional[str]:\n",
    "        m = re.search(r'-\\s*([A-Z]{2,6})\\b', const) or re.search(r'\\[([A-Z]{2,6})\\]', const)\n",
    "        return m.group(1) if m else None\n",
    "\n",
    "    def _deduplicate_entries(self, entries: List[Dict]) -> List[Dict]:\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        for e in entries:\n",
    "            key = f\"{e['name'].lower()}_{e['constituency'].lower()}\"\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique.append(e)\n",
    "        return unique\n",
    "\n",
    "    def analyze_document_patterns(self, docs: List[Dict], sample_size: int = 500) -> Dict:\n",
    "        sample = random.sample(docs, min(sample_size, len(docs)))\n",
    "        processed = 0\n",
    "        decade_dist = defaultdict(int)\n",
    "        for doc in sample:\n",
    "            txt = doc.get('full_text', '')\n",
    "            if not txt: continue\n",
    "            if not self._assess_document_quality(txt)['usable']: continue\n",
    "            decade = self._extract_decade_from_document(doc)\n",
    "            decade_dist[decade] += 1\n",
    "            self._analyze_patterns(decade, txt)\n",
    "            processed += 1\n",
    "        return {\n",
    "            'processing_summary': {\n",
    "                'successfully_processed': processed,\n",
    "                'decade_distribution': dict(decade_dist)\n",
    "            },\n",
    "            'discovered_patterns': dict(self.pattern_discoveries)\n",
    "        }\n",
    "\n",
    "    def _analyze_patterns(self, decade: str, txt: str):\n",
    "        lines = txt.split('\\n')\n",
    "        for line in lines[:25]:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            if self._is_header(line):\n",
    "                self.pattern_discoveries['decade_headers'][decade][line] += 1\n",
    "        for line in self._extract_speakers(txt)[:15]:\n",
    "            for raw, std in self.honorific_dict.items():\n",
    "                if re.search(rf'\\b{re.escape(raw)}\\b', line, re.I):\n",
    "                    self.pattern_discoveries['honorific_variations'][decade][std] += 1\n",
    "        self._analyze_language(txt, decade)\n",
    "\n",
    "    def _is_header(self, line: str) -> bool:\n",
    "        return len(line) < 120 and (line.isupper() or any(k in line.upper() for k in ['DEWAN','PARLIMEN','BIL']))\n",
    "\n",
    "    def _extract_speakers(self, txt: str) -> List[str]:\n",
    "        lines = txt.split('\\n')\n",
    "        speaker_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or len(line) > 250 or line.startswith('('): continue\n",
    "            if ':' not in line: continue\n",
    "            if any(re.search(rf'\\b{re.escape(raw)}\\b', line, re.I) for raw in self.honorific_dict.keys()):\n",
    "                speaker_lines.append(line)\n",
    "        return speaker_lines\n",
    "\n",
    "    def _analyze_language(self, txt: str, decade: str):\n",
    "        malay = sum(len(re.findall(rf'\\b{w}\\b', txt.lower())) for w in ['yang','dan','adalah','akan'])\n",
    "        eng = sum(len(re.findall(rf'\\b{w}\\b', txt.lower())) for w in ['the','and','is','with'])\n",
    "        total = malay + eng\n",
    "        if total > 10:\n",
    "            if malay > eng * 1.5:\n",
    "                self.pattern_discoveries['language_code_switching'][decade]['malay'] += 1\n",
    "            elif eng > malay * 1.5:\n",
    "                self.pattern_discoveries['language_code_switching'][decade]['english'] += 1\n",
    "            else:\n",
    "                self.pattern_discoveries['language_code_switching'][decade]['mixed'] += 1\n",
    "\n",
    "    def _assess_document_quality(self, txt: str) -> Dict:\n",
    "        wc = len(txt.split())\n",
    "        rel = sum(1 for w in ['yang berhormat','dewan rakyat','parlimen','soalan'] if w in txt.lower()) / 4.0\n",
    "        score = max(0.0, min(1.0, wc/2000)*0.4 + rel*0.6)\n",
    "        return {'usable': score > 0.25 and wc > 50}\n",
    "\n",
    "    def _extract_decade_from_document(self, doc: Dict) -> str:\n",
    "        date = doc.get('hansardDate')\n",
    "        if not date:\n",
    "            return \"unknown\"\n",
    "        try:\n",
    "            match = re.search(r'(19[5-9]\\d|20[0-2]\\d)', str(date))\n",
    "            if match:\n",
    "                year = int(match.group(1))\n",
    "                return f\"{(year // 10) * 10}s\"\n",
    "        except (ValueError, AttributeError):\n",
    "            pass\n",
    "        return \"unknown\"\n",
    "\n",
    "    def print_completion_summary(self, total: int):\n",
    "        print(\"\\nFIELD COMPLETION\")\n",
    "        for f, s in self.completion_stats.items():\n",
    "            cov = (s['found'] + s['created']) / total * 100\n",
    "            print(f\"{f.title():15}: {cov:5.1f}%\")\n",
    "\n",
    "    def verify_field_coverage(self, docs: List[Dict]) -> Dict:\n",
    "        return {f: {'percentage': sum(1 for d in docs if d.get(f))/len(docs)*100} for f in ['header','attendance','discussion_start']}\n",
    "\n",
    "    def display_combined_results(self, res: Dict):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        c = res['data_completion']\n",
    "        print(f\"Docs: {c['original_document_count']}\")\n",
    "        for f, cov in c['field_coverage'].items():\n",
    "            print(f\"  {f:18}: {cov['percentage']:5.1f}%\")\n",
    "        print(f\"Pattern docs: {res['pattern_analysis']['processing_summary']['successfully_processed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032c081",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851d5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: EDA Function\n",
    "def run_eda(results: Dict, out_dir: str = \"eda_plots\"):\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    p = results['pattern_analysis']['discovered_patterns']\n",
    "    s = results['pattern_analysis']['processing_summary']\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.barplot(x=list(s['decade_distribution'].keys()), y=list(s['decade_distribution'].values()))\n",
    "    plt.title(\"Documents per Decade\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_dir}/decade.png\"); plt.close()\n",
    "\n",
    "    data = [(d, h[:60], f) for d, c in p['decade_headers'].items() for h, f in c.most_common(5)]\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, columns=['Decade','Header','Freq'])\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(data=df, x='Freq', y='Header', hue='Decade')\n",
    "        plt.title(\"Top Headers\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{out_dir}/headers.png\"); plt.close()\n",
    "\n",
    "    data = [(d, h, f) for d, c in p['honorific_variations'].items() for h, f in c.most_common(5)]\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, columns=['Decade','Honorific','Freq'])\n",
    "        plt.figure(figsize=(9,5))\n",
    "        sns.barplot(data=df, x='Freq', y='Honorific', hue='Decade')\n",
    "        plt.title(\"Top Honorifics\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{out_dir}/honorifics.png\"); plt.close()\n",
    "\n",
    "    data = [(d, c.most_common(1)[0][0] if c else 'unknown') for d, c in p['language_code_switching'].items()]\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, columns=['Decade','Lang'])\n",
    "        plt.figure(figsize=(7,4))\n",
    "        sns.countplot(data=df, x='Decade', hue='Lang')\n",
    "        plt.title(\"Language Dominance\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{out_dir}/language.png\"); plt.close()\n",
    "\n",
    "    print(f\"EDA saved to {out_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01f472",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3db282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Execution\n",
    "import traceback\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "def run_combined_analysis():\n",
    "    print(\"HANSARD ANALYZER (with honorific_dictionary)\")\n",
    "    print(\"=\"*55)\n",
    "    try:\n",
    "        from pymongo import MongoClient\n",
    "        # Find the correct .env path relative to this notebook\n",
    "        env_path = os.path.abspath(os.path.join(os.getcwd(), \"../3_app_system/backend/.env\"))\n",
    "        if not os.path.exists(env_path):\n",
    "            raise FileNotFoundError(f\".env file not found at {env_path}\")\n",
    "        load_dotenv(env_path)\n",
    "\n",
    "        URI = os.getenv(\"MONGODB_URI\")\n",
    "        if not URI:\n",
    "            raise ValueError(\"MONGODB_URI not found in .env file\")\n",
    "        client = MongoClient(URI)\n",
    "        db = client[\"MyParliament\"]\n",
    "\n",
    "        # === Load honorific_dictionary ===\n",
    "        honorific_docs = list(db[\"honorific_dictionary\"].find({}))\n",
    "        honorific_dict = {}\n",
    "        for doc in honorific_docs:\n",
    "            categories = doc.get(\"categories\", {})\n",
    "            for cat_name, titles in categories.items():\n",
    "                for title in titles:\n",
    "                    honorific_dict[title] = title \n",
    "        print(f\"Loaded {len(honorific_dict)} honorifics from categories\")\n",
    "\n",
    "        # === Load hansard ===\n",
    "        docs = list(db[\"hansard_core500\"].find({}))\n",
    "        print(f\"Loaded {len(docs)} hansard docs\")\n",
    "\n",
    "        analyzer = CombinedParliamentaryAnalyzer(honorific_dict)\n",
    "        results = analyzer.run_complete_analysis(docs)\n",
    "        analyzer.display_combined_results(results)\n",
    "\n",
    "        # === Save ===\n",
    "        with open(\"combined_parliament_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"data_completion\": results[\"data_completion\"],\n",
    "                \"pattern_analysis\": results[\"pattern_analysis\"]\n",
    "            }, f, indent=2, default=str, ensure_ascii=False)\n",
    "        print(\"Saved: combined_parliament_analysis.json\")\n",
    "\n",
    "        run_eda(results)\n",
    "\n",
    "        print(\"\\nDone. Ready for cleaning.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd04033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HANSARD ANALYZER (with honorific_dictionary)\n",
      "=======================================================\n",
      "Loaded 20 honorifics from categories\n",
      "Loaded 500 hansard docs\n",
      "  Completing 1/500\n",
      "  Completing 101/500\n",
      "  Completing 201/500\n",
      "  Completing 301/500\n",
      "  Completing 401/500\n",
      "\n",
      "FIELD COMPLETION\n",
      "Header         : 100.0%\n",
      "Attendance     : 100.0%\n",
      "Discussion_Start: 100.0%\n",
      "\n",
      "============================================================\n",
      "ANALYSIS SUMMARY\n",
      "============================================================\n",
      "Docs: 500\n",
      "  header            : 100.0%\n",
      "  attendance        : 100.0%\n",
      "  discussion_start  : 100.0%\n",
      "Pattern docs: 500\n",
      "Saved: combined_parliament_analysis.json\n",
      "EDA saved to eda_plots/\n",
      "\n",
      "Done. Ready for cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run Analysis\n",
    "results = run_combined_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
