{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f9b442",
   "metadata": {},
   "source": [
    "# CPATF: Code-switched Parliament-Aware Token Filtering\n",
    "\n",
    "## Definition\n",
    "**CPATF** (Code-switched Parliament-Aware Token Filtering) is a domain-specific preprocessing module designed for bilingual (Malay-English) Malaysian parliamentary Hansard texts. It filters out noise (e.g., attendance lists, OCR artifacts, repeated honorifics) while selectively retaining content-rich tokens essential for downstream topic modeling, entity extraction, and speaker identification.\n",
    "\n",
    "## Core Concept\n",
    "Hansard proceedings are highly structured yet noisy due to:\n",
    "- Frequent code-switching between Malay and English\n",
    "- Repetitive honorific titles (YB, Dato', Datuk Seri, etc.)\n",
    "- Historical OCR errors and attendance lists in older documents (1970s–1990s)\n",
    "\n",
    "CPATF addresses these challenges by assigning each token a **retention score** based on linguistic, syntactic, and domain-specific signals, preserving only high-information tokens.\n",
    "\n",
    "## Processing Flow\n",
    "1. **Attendance List Detection**  \n",
    "   Skip entire segments identified as attendance lists using regex patterns (e.g., multiple \". \" separated names or numbered lines).\n",
    "\n",
    "2. **Tokenization & Linguistic Analysis**  \n",
    "   - spaCy pipeline with custom Malay POS fallback  \n",
    "   - FastText language identification per token (ms/en confidence > 0.7)\n",
    "\n",
    "3. **Retention Score Calculation**  \n",
    "   Compute score for each token and retain only those exceeding threshold.\n",
    "\n",
    "4. **Lemmatization & Output**  \n",
    "   Lemmatize retained tokens and concatenate into cleaned text.\n",
    "\n",
    "## Mathematical Formulation\n",
    "For each token \\( t \\):\n",
    "\n",
    "Retention score:\n",
    "$$\n",
    "r(t) = w_1 \\cdot \\mathbb{I}_{\\text{lang}}(t) + w_2 \\cdot \\mathbb{I}_{\\text{POS}}(t) + w_3 \\cdot \\mathbb{I}_{\\text{NER}}(t) - w_4 \\cdot \\text{red}(t)\n",
    "$$\n",
    "\n",
    "Token is retained if \\( r(t) \\geq \\theta \\).\n",
    "\n",
    "### Indicators:\n",
    "- **I_lang(t)**: 1 if FastText predicts \"ms\" or \"en\" with confidence > 0.7, else 0\n",
    "- **I_POS(t)**: 1 if POS ∈ {NOUN, PROPN, VERB, ADJ, ADV}, else 0\n",
    "- **I_NER(t)**: 1 if NER type ∈ {PERSON, ORG, LOC, BILL/LAW}, else 0\n",
    "- **red(t)**: Redundancy penalty = (sum of repeated honorifics beyond first occurrence in local window) / window length\n",
    "\n",
    "### Optimized Parameters (via ablation study on core500 dataset):\n",
    "- w₁ = 0.25, w₂ = 0.25, w₃ = 0.40, w₄ = 0.10\n",
    "- Threshold θ = 0.6\n",
    "\n",
    "## Role in Overall Pipeline\n",
    "CPATF produces clean, content-focused segments that serve as standardized input for all topic modeling pipelines, ensuring fair evaluation and high-quality input for MEHTC clustering and XLM-RoBERTa fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f58c52",
   "metadata": {},
   "source": [
    "### ===========================================================================================================================================\n",
    "### Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef7641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready.\n",
      "CPU Cores: 32 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "import pymongo\n",
    "import spacy\n",
    "import fasttext\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "project_root = Path.cwd().parents[1] if 'parents' in dir(Path.cwd()) else Path.cwd()\n",
    "backend_env_path = project_root / \"3_app_system\" / \"backend\" / \".env\"\n",
    "load_dotenv(backend_env_path)\n",
    "\n",
    "# FastText language model\n",
    "FASTTEXT_MODEL_PATH = 'lid.176.bin'\n",
    "if not os.path.exists(FASTTEXT_MODEL_PATH):\n",
    "    print(\"Downloading FastText language model...\")\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\", FASTTEXT_MODEL_PATH)\n",
    "\n",
    "# Load FastText model\n",
    "ft_model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_ner = spacy.load(\"xx_ent_wiki_sm\")      # Multilingual NER\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")       # English lemmatizer\n",
    "\n",
    "# Common Malay function words (for rule-based POS filtering)\n",
    "MALAY_FUNCTION_WORDS = {\n",
    "    'yang', 'di', 'dan', 'untuk', 'dari', 'pada', 'dalam', 'adalah', 'ialah', 'bahawa',\n",
    "    'sebagai', 'oleh', 'kepada', 'dengan', 'atau', 'jika', 'kerana', 'supaya', 'walaupun',\n",
    "    'serta', 'tetapi', 'manakala', 'itu', 'ini', 'tersebut', 'akan', 'telah', 'sudah'\n",
    "}\n",
    "\n",
    "print(\"Environment ready.\")\n",
    "print(f\"CPU Cores: {os.cpu_count()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ed329",
   "metadata": {},
   "source": [
    "### Database Connection and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb3725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 unique honorifics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading segmented documents: 500it [00:14, 35.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 500 documents.\n",
      "\n",
      "Sample documents check:\n",
      "Doc ID: 6947d3ebdaaf821ec476383b\n",
      "  Date: 1959-09-11 00:00:00\n",
      "  Segments: 16\n",
      "  Parlimen: 1, Penggal: 1, Mesyuarat: 1\n",
      "  Decade: pre1970\n",
      "---\n",
      "Doc ID: 6947d3ebdaaf821ec476383c\n",
      "  Date: 1961-10-19 00:00:00\n",
      "  Segments: 67\n",
      "  Parlimen: 1, Penggal: 3, Mesyuarat: 1\n",
      "  Decade: pre1970\n",
      "---\n",
      "Doc ID: 6947d3ebdaaf821ec476383d\n",
      "  Date: 1961-04-24 00:00:00\n",
      "  Segments: 56\n",
      "  Parlimen: 1, Penggal: 3, Mesyuarat: 1\n",
      "  Decade: pre1970\n",
      "---\n",
      "Doc ID: 6947d3ebdaaf821ec476383e\n",
      "  Date: 1961-04-28 00:00:00\n",
      "  Segments: 61\n",
      "  Parlimen: 1, Penggal: 3, Mesyuarat: 1\n",
      "  Decade: pre1970\n",
      "---\n",
      "Doc ID: 6947d3ebdaaf821ec476383f\n",
      "  Date: 1963-08-15 00:00:00\n",
      "  Segments: 42\n",
      "  Parlimen: 1, Penggal: 5, Mesyuarat: 1\n",
      "  Decade: pre1970\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "client = pymongo.MongoClient(os.getenv(\"MONGO_URI\"))\n",
    "db = client[\"MyParliament\"]\n",
    "\n",
    "# Collections\n",
    "segmented_col = db[\"hansard_segmented500\"]\n",
    "honorific_col = db[\"honorific_dictionary\"]\n",
    "cpatf_col = db[\"hansard_cpatf500\"]\n",
    "\n",
    "# Load honorifics\n",
    "honorific_dict = honorific_col.find_one({}, {\"categories\": 1})\n",
    "if not honorific_dict:\n",
    "    raise ValueError(\"Honorific dictionary not found\")\n",
    "all_honorifics = set()\n",
    "for titles in honorific_dict.get(\"categories\", {}).values():\n",
    "    all_honorifics.update([t.lower() for t in titles])\n",
    "\n",
    "print(f\"Loaded {len(all_honorifics)} unique honorifics.\")\n",
    "\n",
    "# Load FastText model\n",
    "ft_model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n",
    "\n",
    "# Load documents with correct date field\n",
    "all_docs = list(tqdm(\n",
    "    segmented_col.find(\n",
    "        {}, \n",
    "        {\n",
    "            \"_id\": 1, \n",
    "            \"segmentation_output\": 1,   \n",
    "            \"hansardDate\": 1,          \n",
    "            \"mesyuarat\": 1,\n",
    "            \"parlimen\": 1,\n",
    "            \"penggal\": 1,\n",
    "            \"decade\": 1\n",
    "        }\n",
    "    ),\n",
    "    desc=\"Loading segmented documents\"\n",
    "))\n",
    "\n",
    "print(f\"Successfully loaded {len(all_docs)} documents.\")\n",
    "\n",
    "# Check samples with correct date field\n",
    "print(\"\\nSample documents check:\")\n",
    "for doc in all_docs[:5]:\n",
    "    print(f\"Doc ID: {doc['_id']}\")\n",
    "    print(f\"  Date: {doc.get('hansardDate', 'Missing')}\")\n",
    "    print(f\"  Segments: {len(doc.get('segmentation_output', []))}\")\n",
    "    print(f\"  Parlimen: {doc.get('parlimen')}, Penggal: {doc.get('penggal')}, Mesyuarat: {doc.get('mesyuarat')}\")\n",
    "    print(f\"  Decade: {doc.get('decade')}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51678141",
   "metadata": {},
   "source": [
    "### CPATF Core Functions (Bilingual Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c32ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_LANG = 0.30\n",
    "W_POS  = 0.30\n",
    "W_RED  = 0.15\n",
    "THRESHOLD = 0.40 \n",
    "LANG_CONF_THRESHOLD = 0.6\n",
    "REDUNDANCY_WINDOW = 15\n",
    "\n",
    "CONTENT_POS_TAGS = {'NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV', 'NUM'}\n",
    "\n",
    "def is_attendance_list(text: str) -> bool:\n",
    "    dot_pattern = re.compile(r'(\\.\\s+[A-Z][a-z]+){5,}')\n",
    "    num_pattern = re.compile(r'^\\d+\\.', re.MULTILINE)\n",
    "    return bool(dot_pattern.search(text) or len(num_pattern.findall(text)) > 5)\n",
    "\n",
    "@lru_cache(maxsize=30000)\n",
    "def get_lang_indicator(token: str) -> int:\n",
    "    pred = ft_model.predict(token.replace('\\n', ' '), k=1)\n",
    "    lang, conf = pred[0][0].replace('__label__', ''), pred[1][0]\n",
    "    return 1 if lang in ['ms', 'en', 'id'] and conf > LANG_CONF_THRESHOLD else 0\n",
    "\n",
    "def get_redundancy_penalty(tokens: List[str], idx: int) -> float:\n",
    "    start = max(0, idx - REDUNDANCY_WINDOW // 2)\n",
    "    end = min(len(tokens), idx + REDUNDANCY_WINDOW // 2 + 1)\n",
    "    window = [t.lower() for t in tokens[start:end]]\n",
    "    repeated = sum(max(0, window.count(h) - 1) for h in all_honorifics if h in window)\n",
    "    return min(repeated * 0.15, 0.4)  # Stronger penalty for honorifics\n",
    "\n",
    "def simple_malay_stem(word: str) -> str:\n",
    "    word_lower = word.lower()\n",
    "    if len(word_lower) <= 4:\n",
    "        return word_lower\n",
    "    suffixes = ['kan', 'an', 'i', 'lah', 'kah', 'nya', 'tah', 'pun', 'mu', 'ku']\n",
    "    for suffix in suffixes:\n",
    "        if word_lower.endswith(suffix):\n",
    "            return word_lower[:-len(suffix)]\n",
    "    return word_lower\n",
    "\n",
    "def rule_based_pos(word: str) -> str:\n",
    "    if word and word[0].isupper():\n",
    "        return 'PROPN'\n",
    "    if word.isdigit() or re.match(r'^\\d', word):\n",
    "        return 'NUM'\n",
    "    if word.lower().endswith(('kan', 'i', 'lah', 'nya', 'tah')):\n",
    "        return 'VERB'\n",
    "    return 'NOUN'\n",
    "\n",
    "def process_segment(segment: str) -> str:\n",
    "    if isinstance(segment, list):\n",
    "        segment = \" \".join([s.strip() for s in segment if s.strip()])\n",
    "\n",
    "    if not segment or not segment.strip():\n",
    "        return \"\"\n",
    "\n",
    "    if is_attendance_list(segment):\n",
    "        return \"\"\n",
    "\n",
    "    segment = segment[:6000]\n",
    "    words = segment.split()\n",
    "    if not words:\n",
    "        return \"\"\n",
    "\n",
    "    pos_tags = [rule_based_pos(word) for word in words]\n",
    "\n",
    "    retained = []\n",
    "    for idx, word in enumerate(words):\n",
    "        lang_ind = get_lang_indicator(word)\n",
    "        pos_ind = 1 if pos_tags[idx] in CONTENT_POS_TAGS else 0\n",
    "        red_pen = get_redundancy_penalty(words, idx)\n",
    "\n",
    "        score = W_LANG * lang_ind + W_POS * pos_ind - W_RED * red_pen\n",
    "\n",
    "        # Force retain only critical entities\n",
    "        force_retain = (\n",
    "            pos_tags[idx] == 'PROPN' or \n",
    "            pos_tags[idx] == 'NUM' or \n",
    "            len(word) > 8  # Longer words likely entities\n",
    "        )\n",
    "\n",
    "        if force_retain or score >= THRESHOLD:\n",
    "            word_lower = word.lower()\n",
    "            if lang_ind == 1 and not word[0].isupper():\n",
    "                normalized = simple_malay_stem(word_lower)\n",
    "            else:\n",
    "                normalized = word_lower\n",
    "            retained.append(normalized)\n",
    "\n",
    "    return \" \".join(retained)\n",
    "\n",
    "def process_long_segment(segment: str, max_chunk_tokens: int = 2000) -> str:\n",
    "    words = segment.split()\n",
    "    total_tokens = len(words)\n",
    "    \n",
    "    if total_tokens <= max_chunk_tokens:\n",
    "        return process_segment(segment)\n",
    "    \n",
    "    print(f\"Long segment ({total_tokens} tokens) → chunking (max {max_chunk_tokens})...\")\n",
    "    \n",
    "    retained_words = []\n",
    "    overlap = 200\n",
    "    start = 0\n",
    "    while start < total_tokens:\n",
    "        end = min(start + max_chunk_tokens, total_tokens)\n",
    "        chunk_text = \" \".join(words[start:end])\n",
    "        cleaned_chunk = process_segment(chunk_text)\n",
    "        chunk_words = cleaned_chunk.split()\n",
    "        if chunk_words:\n",
    "            if retained_words and chunk_words[:50] == retained_words[-50:]:\n",
    "                retained_words.extend(chunk_words[50:])\n",
    "            else:\n",
    "                retained_words.extend(chunk_words)\n",
    "        start = end - overlap if end < total_tokens else end\n",
    "    \n",
    "    return \" \".join(retained_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380048d4",
   "metadata": {},
   "source": [
    "### Test on 10 Random Real Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d2b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPATF Test: Before & After on 10 Random Real Segments ===\n",
      "\n",
      "[1/10] Doc ID: 6947dae7daaf821ec47638bb | Date: 1981-10-27\n",
      "Raw segmentation_output items: 93\n",
      "\n",
      "【Original Segment】\n",
      "Terima kasih, hend Tuan Yang di-Pertua. Oleh yang demikian iala bagi menyelesaikan Rancangan Makanan seko Tambahan yang sangat berfaedah ini adalah muri dicadangkan mana-mana sekolah yang Wala terpilih hendaklah diberi kepada semua murid dala sekolah tersebut dan yang kedua, bantuan 25 ingi sen itu hendaklah ditimbangkan semula oleh S kerana memandangkan harga bahan-bahan kera makanan yang melambung hari ini adalah seba tidak sesuai. mera Tuan Yang di-Pertua, berhubung dengan pent Skim Pinjaman Buku Teks pula, saya hendak seba menyentuh dan memberi sedikit pandangan akan iaitu mengikut pengala...\n",
      "\n",
      "Long segment (5716 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "terima kasih, tuan yang di-pertua. oleh yang iala bagi menyelesaikan rancangan makanan tambahan yang sangat berfaedah ini ada dicadangkan mana-mana seko yang wala terpilih hendaklah kepada semua seko tersebut dan yang kedua, 25 itu hendaklah ditimbangkan semula oleh s memandang bahan-bahan yang melambung ini ada seba tidak tuan yang di-pertua, berhubung deng skim pinjaman buku teks saya seba menyentuh dan pandangan akan mengikut pengalaman saya seko menengah memandang kesulitan untuk mendapat buku-bu yang sekolah-sekolah, terutama sekal ibubapa yang tinggal bandar yang pert tidak mendapat skim...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities preserved: 27/120 (22.5% if orig_entities else 0) \n",
      "Sample entities kept: ['mekah', 'libya', 'keselamatan', 'umpamanya', 'akan']\n",
      "Tokens: 5716 → 3935 (reduced by 1781, 31.2%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[2/10] Doc ID: 6947dae7daaf821ec47638cb | Date: 1982-03-19\n",
      "Raw segmentation_output items: 174\n",
      "\n",
      "【Original Segment】\n",
      "apa guna undi kepada Kerajaan, apa dia boleh bagi? Kalau undi kami pun sama juga. Dalam keadaan semacam itu, dia seolah-olah menggalak pengundi-pengundi bertanya 1339 19 MAC kepada calon-calon Kerajaan, apa Kerajaan m hendak bagi. Bila pengundi-pengundi o bertanya maka terpaksa Kerajaan membuat s penerangan. Calon Kerajaan ini bertanya. r kalau kami jadi Kerajaan maka kami dapat t mengadakan ini, ini dan ini, dan oleh kerana s pengundi mendapat ini dianya berpendapat m bahawa satu cara untuk mengugut calon- p calon Kerajaan ialah dengan berkata, “Kalau tidak bagi ini, kamu undi parti Pembangka...\n",
      "\n",
      "Long segment (1920 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "undi kepada kerajaan, boleh bagi? kalau undi dalam itu, seolah-o menggalak pengundi-pengundi 1339 19 mac kepada calon-calon kerajaan, kerajaan bagi. bila pengundi-pengundi kerajaan membuat penerangan. calon kerajaan ini bertanya. kerajaan dapat mengada ini dan dan oleh mendapat ini berpendapat satu untuk kerajaan ia deng “kalau tidak bagi kamu undi pembangkang.” ini merupa yang digalakkan oleh pembangkang inilah yang saya masyarakat tidak kepada tidak akan kalau masyarakat! tidak mempunya pembangkang tidak payah menang selalu oleh ada s pembangkang ini d pengundi-pengundi seolah-o dalam mengga...\n",
      "\n",
      "Entities preserved: 10/42 (23.8% if orig_entities else 0) \n",
      "Sample entities kept: ['jkr', 'langkah-langkah', 'tidak', 'katanya', 'ini']\n",
      "Tokens: 1920 → 1260 (reduced by 660, 34.4%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[3/10] Doc ID: 6947d3ebdaaf821ec4763869 | Date: 1964-07-06\n",
      "Raw segmentation_output items: 251\n",
      "\n",
      "【Original Segment】\n",
      "“On the understanding that the Foreign Ministers Meeting will start simultaneously with the beginning of the withdrawal, Indonesia is prepared to begin the withdrawal from an area in Sarawak through one checkpoint on the Malaysian side, across the border to another check-point on the Indonesian side of the border. Indonesia and Malaysia have agreed to designate and prepare another check-point for withdrawal from Sabah. Withdrawal generally will require additional check-points which will be a matter for discussion by the Foreign Ministers. Both sides agree that guerillas activities as well as c...\n",
      "\n",
      "Long segment (2819 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "“on the understanding that the foreign ministers meeting will simultaneously with the beginning of the withdrawal, indonesia is prepared to the withdrawal from an in sarawak through one checkpoint on the malaysian across the to another check-point on the indonesian of the indonesia and malaysia have agreed to designate and another check-point for withdrawal from sabah. withdrawal generally will additional check-points which will matter for discussion by the foreign ministers. both agree that guerillas activities as well as operations against these activities will cease along the route related ...\n",
      "\n",
      "Entities preserved: 24/71 (33.8% if orig_entities else 0) \n",
      "Sample entities kept: ['south-east asia', 'jeopardy', 'sarawak', 'ottawa', 'africa']\n",
      "Tokens: 2819 → 2919 (reduced by -100, -3.5%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[4/10] Doc ID: 6947fb2ddaaf821ec4763972 | Date: 2002-03-25\n",
      "Raw segmentation_output items: 733\n",
      "\n",
      "【Original Segment】\n",
      "Apakah kedudukan ICT mereka? Sebenarnya, mereka lebih daif daripada kedudukan kita pada hari ini. Jadi, jangan kita kotak-katikkan sangat, jangan perlekehkan sangat kita punya kedudukan, seolah-olah kedudukan kita ini, daif sangat. Itu tidak betul. ICT ini adalah satu perkara yang baru bagi seluruh dunia dan sebenarnya di Malaysia ini, kita sudah pergi ke hadapan. Cuma kita di sini memikirkan bahawa hari ini harus ada ICT di setiap sekolah di negara kita, barulah kita puas hati. Tetapi, itu perlu kita buat secara langkah demi langkah. Yang Berhormat bagi Sri Aman memohon supaya pengajaran Baha...\n",
      "\n",
      "Long segment (1307 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "apakah kedudu ict sebenarnya, lebih daif daripada kedudu pada ini. jadi, kotak-katikkan perlekehkan sangat pu kedudukan, seolah-o kedudu daif itu tidak ict ini ada satu yang baru bagi dunia dan sebenar malaysia sudah cuma memikir ini ada ict seko negara tetapi, itu lang yang berhormat bagi sri aman pengajar bahasa iban diteruskan tingkatan empat dan lima sepert dalam kementerian pendidikan. untuk pertanya ahli yang berhormat, pada 19 mac 2002 yang lalu, kementerian ini te meluluskan pelajar bahasa iban seko dan dr.25.3.2002 85 seko menengah rendah. sebelum pengajar dan pembelajar ini diperluas...\n",
      "\n",
      "Entities preserved: 14/39 (35.9% if orig_entities else 0) \n",
      "Sample entities kept: ['united kingdom', 'sekolah sea park', 'sekolah', 'lima', 'kedudukan']\n",
      "Tokens: 1307 → 949 (reduced by 358, 27.4%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[5/10] Doc ID: 6947d7a1daaf821ec4763884 | Date: 1974-05-02\n",
      "Raw segmentation_output items: 210\n",
      "\n",
      "【Original Segment】\n",
      "Tuan Yang di-Pertua, saya suka hendak menyambung lagi perbahasan ber- kenaan dengan Rang Undang-undang Lembaga Kemajuan Trengganu Tengah. Semalam saya telah meny rancangan-rancangan serupa ini rancangan-rancangan tanah ataupu cangan-rancangan untuk sesuatu pertanian hendaklah meneliti bukan untuk 5 tahun yang akan datang ataupun 10 tahun yang akan datang, tetapi mestilah difikirkan projek itu untuk masa akan datang, bahkan 100 tahun yang akan datang kita tentukan apa yang akan berlaku di tempat itu. Saya sudah memberi misalan berkenaan jalanraya dari Petaling Batu Tiga, dari Batu Tiga ke Kelan...\n",
      "\n",
      "Long segment (3056 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "tuan yang di-pertua, saya menyambung perbahas ber- deng rang undang-undang lembaga kemajuan trengganu tengah. semalam saya te meny rancangan-rancang serupa ini rancangan-rancang ataupu cangan-rancang untuk sesuatu pertanian hendaklah untuk 5 tahun yang akan atau 10 tahun yang akan difikirkan itu untuk akan bah 100 tahun yang akan tentu yang akan tempat itu. saya sudah berkenaan jalanraya dari petaling batu tiga, dari batu tiga kelan port kelang. pada jal itu te sudah bukanlah kesalahan engineer, ada tahu sahaja. ini sa orang-orang merancang jal itu tidak ada foresight. pada ini jalanantara kua...\n",
      "\n",
      "Entities preserved: 15/63 (23.8% if orig_entities else 0) \n",
      "Sample entities kept: ['peninsular malaysia', 'hong kong', 'taiwan', 'kuala lumpur', 'port kelang']\n",
      "Tokens: 3056 → 2195 (reduced by 861, 28.2%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[6/10] Doc ID: 6947fb2ddaaf821ec4763991 | Date: 2005-09-29\n",
      "Raw segmentation_output items: 592\n",
      "\n",
      "【Original Segment】\n",
      "Terima kasih Tuan Yang di-Pertua saya juga ingin terlibat dalam perbahasan untuk meminda Perlembagaan Persekutuan sebagaimana yang dibentangkan oleh Yang Berhormat Menteri sebentar tadi. Saya ingin menyentuh beberapa perkara, pertamanya Fasal 2 yang melibatkan pertambahan kerusi-kerusi Dewan Rakyat dan juga Dewan Undangan Negeri untuk negeri Sarawak iaitu menambah daripada di Parlimennya tiga yang baru dan di peringkat DUNnya sembilan tambahan anggotanya. Point saya mudah sahaja Tuan Yang di-Pertua iaitu rasanya, seboleh-bolehnya jadikanlah ini amalan terakhir dalam Parlimen kita ini iaitu mem...\n",
      "\n",
      "Long segment (1511 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "terima kasih tuan yang di-pertua saya juga ingin dalam perbahas untuk meminda perlembagaan persekutuan sebagaimana yang dibentang oleh yang berhormat menteri saya ingin menyentuh pertamanya fasal 2 yang melibat pertambah kerusi-kerusi dewan rakyat dan juga dewan undangan negeri untuk sarawak daripada parlimennya tiga yang baru dan peringkat dunnya anggotanya. point saya tuan yang di-pertua seboleh-boleh jadikanlah ini terakhir dalam parlimen ini membuat persempadanan dan pinda perlembagaan untuk mana-mana dalam negara ini. pada masa-masa yang akan saya ingin persempadanan semula dilaku wide de...\n",
      "\n",
      "Entities preserved: 7/38 (18.4% if orig_entities else 0) \n",
      "Sample entities kept: ['kuala lumpur', 'ketua hakim', 'pemecatan hakim-hakim', 'malaysia', 'sarawak']\n",
      "Tokens: 1511 → 1087 (reduced by 424, 28.1%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[7/10] Doc ID: 6947ece7daaf821ec4763963 | Date: 2000-03-27\n",
      "Raw segmentation_output items: 396\n",
      "\n",
      "【Original Segment】\n",
      "Terima kasih, Tuan Pengerusi. Saya ingin mengambil bahagian untuk membincangkan Maksud B.22 06700 berkenaan dengan Lembaga Penggalakan Pelancongan Malaysia. Perkara pertama ialah berkaitan dengan pelancong-pelancong daripada Jepun. Mengikut pengamatan ringkas saya, sebahagian daripada pelancong Jepun melancong setiap tahun ke beberapa negara ASEAN terutama sekali Singapura tetapi pelancong ini adalah sebahagian kecil saja, kalau tidak silap saya lebih kurang 20% sahaja daripada pelancong-pelancong Jepun yang telah melawat Singapura melanjutkan lawatan mereka ke Semenanjung ataupun ke Malaysia....\n",
      "\n",
      "Long segment (1238 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "terima kasih, tuan pengerusi. saya ingin mengambil untuk membincang maksud b.22 06700 berkenaan deng lembaga penggalakan pelancongan malaysia. perkara pertama ia berkaitan deng pelancong-pelancong daripada jepun. mengikut pengamatan ringkas sebahagi daripada pelancong jepun melancong tahun negara asean terutama sekal singapura pelancong ini ada sebahagi kecil tidak silap saya lebih kurang 20% daripada pelancong-pelancong jepun yang te singapura melanjut lawat semenanjung atau malaysia. saya kementerian ini dapat membuat satu terperinci mengapa pelancong jepun ada kecil negara clan apa langkah-...\n",
      "\n",
      "Entities preserved: 6/23 (26.1% if orig_entities else 0) \n",
      "Sample entities kept: ['perkara-perkara', 'china', 'malaysia', 'saya', 'korea']\n",
      "Tokens: 1238 → 893 (reduced by 345, 27.9%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[8/10] Doc ID: 6947d3ebdaaf821ec476384c | Date: 1960-12-09\n",
      "Raw segmentation_output items: 105\n",
      "\n",
      "【Original Segment】\n",
      "Tuan Yang di-Pertua, pada pagi tadi banyak Ahli? Yang Berhormat telah memberi uchapan tahniah kapada Menteri Pertanian. Maka saya berdiri di-sini sa-orang daripada-nya, menguchapkan tahniah kapada Menteri Pertanian Sharikat Bekerjasama di-atas jasa? baik-nya hendak memajukan hal pertanian dalam negeri ini. Saya suka juga menarek perhatian kapada Kementerian ini di-atas muka 64, Item (345) Co-operative Officers. Ibu Pejabat Sharikat Bekerjasama di- Pahang barat ia-lah di-Raub. Saya dapati hanya sa-orang sahaja pegawai Sharikat Bekerjasama di-tempat itu. Saya dapati oleh sebab keadaan di- daerah...\n",
      "\n",
      "Long segment (1330 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "tuan yang di-pertua, pada tadi ahli? yang berhormat te menteri pertanian. maka saya berdir daripada-nya, menguchap menteri pertanian sharikat bekerjasama baik- memaju pertanian dalam ini. saya juga perhati kementerian ini 64, item co-operative officers. ibu pejabat sharikat bekerjasama pahang barat ia- saya ha sharikat bekerjasama di-tempat itu. saya oleh sebab daerah pahang barat itu—kkawasan ulu sungai, ulu tembeling sangat-lah pendudok? di-tempat? itu menuboh sharikat bekerjasama. maka saya pegawai sharikat bekerjasama ada di-tempat itu mema 3, 4 maka oleh sebab itu saya berharap-lah kement...\n",
      "\n",
      "Entities preserved: 11/45 (24.4% if orig_entities else 0) \n",
      "Sample entities kept: ['highlands', 'china', 'b saya', 'terengkap', 'akan']\n",
      "Tokens: 1330 → 894 (reduced by 436, 32.8%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[9/10] Doc ID: 6947df26daaf821ec47638eb | Date: 1986-10-14\n",
      "Raw segmentation_output items: 195\n",
      "\n",
      "【Original Segment】\n",
      "Ketua Pembang Saya juga ingin mengambil bahagian daripada awal dalam menyokong Usul Ucapan mengkritik Menjunjung Kasih Kebawah Duli terutamanya Yang Maha Mulia Seri Paduka berhubung den Baginda Yang di-Pertuan Agong atas yang saya ras Titah Ucapan perasmian Parlimen rasa syak yang ke-7 baru-baru ini. Ucapan te 726 rakyat Malaysia yang ra ini tanpa berbelah doa agar Kerajaan an terus memerintah an saksama dengan ik dan hidayat dariNya. i Yang Maha Mulia Seri inda Yang di-Pertuan k bersatu di bawah ang Amat Berhormat ri sekarang seharusnya atian yang serius dari ak termasuk pihak ntuk menjamin m...\n",
      "\n",
      "Long segment (1982 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "ketua pembang saya juga ingin mengambil daripada dalam menyokong usul ucapan mengkritik menjunjung kasih kebawah duli terutama yang maha mulia seri paduka berhubung baginda yang di-pertuan agong atas yang saya titah ucapan perasmian parlimen syak yang ke-7 baru-baru ini. ucapan 726 malaysia yang ini berbe kerajaan an memerintah an deng dan yang maha mulia seri yang di-pertuan bawah amat berhormat sekarang seharus yang dari termasuk ntuk yang di-pertua, utama an dalam tersebut. masalah bukan bersama dan juga pemimpin ini termasuk an juga penyokong tidak letak syarat dapat mengemukakan dasar eko...\n",
      "\n",
      "Entities preserved: 12/59 (20.3% if orig_entities else 0) \n",
      "Sample entities kept: ['baru', 'mic', 'cina', 'malaysia', 'ketua pembang']\n",
      "Tokens: 1982 → 1330 (reduced by 652, 32.9%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "[10/10] Doc ID: 6947d7a1daaf821ec476387a | Date: 1972-12-08\n",
      "Raw segmentation_output items: 149\n",
      "\n",
      "【Original Segment】\n",
      "(Dengan The izin) First of all, Sir, the Bill before us, for Britis the first time, discusses about planning. We Mogul do not completely oppose this Bill. We may State have some reservations and some obervations buildi to make at this juncture. Sir, this Bill seeks to pre absolute powers for the Commissioner— to thi which I may term as sweeping powers—in proble the performance of the functions of his office. sure, to dev Sir, before I go into the details of this Bill, at the I wonder whether there had been any requir planning at all in the past in Kuala Lumpur. as a r If there had been plannin...\n",
      "\n",
      "Long segment (3212 tokens) → chunking (max 1000)...\n",
      "【CPATF Cleaned (Rule-based)】\n",
      "the first of sir, the bill before for britis the first time, discusses about planning. we mogul not completely oppose this bill. we may state have some reservations and some obervations build to make at this juncture. sir, this bill to powers for the commissioner— to thi which i may term as powers—in the performance of the functions of his office. sure, to sir, before i go into the details of this bill, at the i wonder there had been planning at all in the past in kuala lumpur. as if there had been planning, i positively will sure, we would not facing the many having complex problems that we a...\n",
      "\n",
      "Entities preserved: 12/49 (24.5% if orig_entities else 0) \n",
      "Sample entities kept: ['us', 'klang river', 'kuala lumpur', 'negara', 'singapore']\n",
      "Tokens: 3212 → 3207 (reduced by 5, 0.2%)\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "=== CPATF Test Summary ===\n",
      "Documents with valid text: 10/10\n",
      "Total tokens before CPATF : 24091\n",
      "Total tokens after CPATF  : 18669\n",
      "Overall token reduction   : 5422 tokens (22.5%)\n",
      "Entity preservation rate  : 25.1% (138/549 entities kept)\n",
      "\n",
      "→ Reduction ~22.5% with ~25.1% entity retention\n",
      "\n",
      "=== CPATF Test Completed Successfully ===\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 10 documents for testing\n",
    "test_docs = random.sample(all_docs, min(10, len(all_docs)))\n",
    "\n",
    "print(\"=== CPATF Test: Before & After on 10 Random Real Segments ===\\n\")\n",
    "\n",
    "total_before = 0\n",
    "total_after = 0\n",
    "valid_count = 0\n",
    "total_entities_before = 0\n",
    "total_entities_after = 0\n",
    "\n",
    "for i, doc in enumerate(test_docs, 1):\n",
    "    doc_id = doc[\"_id\"]\n",
    "    date_str = doc.get(\"hansardDate\")\n",
    "    if isinstance(date_str, datetime):\n",
    "        date_str = date_str.strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        date_str = str(date_str) if date_str else \"Unknown Date\"\n",
    "    \n",
    "    raw_segments = doc.get(\"segmentation_output\", [])\n",
    "\n",
    "    print(f\"[{i}/10] Doc ID: {doc_id} | Date: {date_str}\")\n",
    "    print(f\"Raw segmentation_output items: {len(raw_segments)}\\n\")\n",
    "\n",
    "    # Extract clean text strings\n",
    "    segments = []\n",
    "    for item in raw_segments:\n",
    "        if isinstance(item, str):\n",
    "            text = item.strip()\n",
    "        elif isinstance(item, dict):\n",
    "            text = item.get(\"text\", \"\") or item.get(\"content\", \"\")\n",
    "            text = text.strip()\n",
    "        else:\n",
    "            continue\n",
    "        if text:\n",
    "            segments.append(text)\n",
    "\n",
    "    if not segments:\n",
    "        print(\"No valid text segments found in this document.\\n\")\n",
    "        print(\"-\" * 90 + \"\\n\")\n",
    "        continue\n",
    "\n",
    "    valid_count += 1\n",
    "\n",
    "    # Pick the longest segment for demonstration\n",
    "    sample_seg = max(segments, key=len)\n",
    "\n",
    "    print(\"【Original Segment】\")\n",
    "    print(sample_seg[:600] + (\"...\" if len(sample_seg) > 600 else \"\") + \"\\n\")\n",
    "\n",
    "    # Use robust chunking - rule-based is fast, so larger chunks OK\n",
    "    cleaned = process_long_segment(sample_seg, max_chunk_tokens=1000)\n",
    "\n",
    "    print(\"【CPATF Cleaned (Rule-based)】\")\n",
    "    print(cleaned[:600] + (\"...\" if len(cleaned) > 600 else \"\") + \"\\n\")\n",
    "\n",
    "    # Token count\n",
    "    before_tokens = len(sample_seg.split())\n",
    "    after_tokens = len(cleaned.split())\n",
    "    total_before += before_tokens\n",
    "    total_after += after_tokens\n",
    "\n",
    "    # Simple entity preservation check (using spaCy NER on original and cleaned)\n",
    "    try:\n",
    "        orig_doc = nlp_ner(sample_seg)\n",
    "        clean_doc = nlp_ner(cleaned)\n",
    "        \n",
    "        orig_entities = {ent.text.lower() for ent in orig_doc.ents \n",
    "                        if ent.label_ in ['PERSON', 'ORG', 'LOC']}\n",
    "        clean_entities = {ent.text.lower() for ent in clean_doc.ents \n",
    "                         if ent.label_ in ['PERSON', 'ORG', 'LOC']}\n",
    "        \n",
    "        preserved = len(orig_entities & clean_entities)\n",
    "        total_entities_before += len(orig_entities)\n",
    "        total_entities_after += preserved\n",
    "        \n",
    "        print(f\"Entities preserved: {preserved}/{len(orig_entities)} \"\n",
    "              f\"({100 * preserved / len(orig_entities):.1f}% if orig_entities else 0) \")\n",
    "        if orig_entities:\n",
    "            print(f\"Sample entities kept: {list(orig_entities & clean_entities)[:5]}\")\n",
    "    except:\n",
    "        print(\"Entity check skipped (NER error)\")\n",
    "\n",
    "    reduction = before_tokens - after_tokens\n",
    "    reduction_pct = (reduction / before_tokens * 100) if before_tokens > 0 else 0\n",
    "    print(f\"Tokens: {before_tokens} → {after_tokens} \"\n",
    "          f\"(reduced by {reduction}, {reduction_pct:.1f}%)\")\n",
    "    print(\"-\" * 90 + \"\\n\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"=== CPATF Test Summary ===\")\n",
    "print(f\"Documents with valid text: {valid_count}/10\")\n",
    "\n",
    "if total_before > 0:\n",
    "    overall_reduction = total_before - total_after\n",
    "    overall_pct = 100 * overall_reduction / total_before\n",
    "    entity_preserve_rate = (100 * total_entities_after / total_entities_before \n",
    "                           if total_entities_before > 0 else 0)\n",
    "    \n",
    "    print(f\"Total tokens before CPATF : {total_before}\")\n",
    "    print(f\"Total tokens after CPATF  : {total_after}\")\n",
    "    print(f\"Overall token reduction   : {overall_reduction} tokens ({overall_pct:.1f}%)\")\n",
    "    print(f\"Entity preservation rate  : {entity_preserve_rate:.1f}% \"\n",
    "          f\"({total_entities_after}/{total_entities_before} entities kept)\")\n",
    "    print(f\"\\n→ Reduction ~{overall_pct:.1f}% with ~{entity_preserve_rate:.1f}% entity retention\")\n",
    "else:\n",
    "    print(\"No valid tokens found in sampled documents.\")\n",
    "\n",
    "print(\"\\n=== CPATF Test Completed Successfully ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738209f",
   "metadata": {},
   "source": [
    "### Full Preprocessing on 500 Documents using Best CPATF Parameters (Multi-threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c82bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 documents in segmented_col.\n",
      "Resume-safe preprocessing: skipping already processed parent_doc_id\n",
      "Found 0 already processed parent_doc_ids - skipping them\n",
      "Need to process 500 new documents\n",
      "Long segment (1354 tokens) → chunking (max 1200)...\n",
      "Long segment (3773 tokens) → chunking (max 1200)...\n",
      "Long segment (1262 tokens) → chunking (max 1200)...\n",
      "Long segment (2426 tokens) → chunking (max 1200)...\n",
      "Long segment (6704 tokens) → chunking (max 1200)...\n",
      "Long segment (1323 tokens) → chunking (max 1200)...\n",
      "Long segment (1207 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (3021 tokens) → chunking (max 1200)...\n",
      "Long segment (1785 tokens) → chunking (max 1200)...\n",
      "Long segment (1504 tokens) → chunking (max 1200)...\n",
      "Long segment (1560 tokens) → chunking (max 1200)...\n",
      "Long segment (1772 tokens) → chunking (max 1200)...\n",
      "Long segment (1218 tokens) → chunking (max 1200)...\n",
      "Long segment (1404 tokens) → chunking (max 1200)...\n",
      "Long segment (1496 tokens) → chunking (max 1200)...\n",
      "Long segment (1493 tokens) → chunking (max 1200)...\n",
      "Long segment (2015 tokens) → chunking (max 1200)...\n",
      "Long segment (2664 tokens) → chunking (max 1200)...\n",
      "Long segment (1225 tokens) → chunking (max 1200)...\n",
      "Long segment (2540 tokens) → chunking (max 1200)...\n",
      "Long segment (1971 tokens) → chunking (max 1200)...\n",
      "Long segment (1433 tokens) → chunking (max 1200)...\n",
      "Long segment (1386 tokens) → chunking (max 1200)...\n",
      "Long segment (2694 tokens) → chunking (max 1200)...\n",
      "Long segment (2015 tokens) → chunking (max 1200)...\n",
      "Long segment (7968 tokens) → chunking (max 1200)...\n",
      "Long segment (1308 tokens) → chunking (max 1200)...\n",
      "Long segment (1456 tokens) → chunking (max 1200)...\n",
      "Long segment (2591 tokens) → chunking (max 1200)...\n",
      "Long segment (1377 tokens) → chunking (max 1200)...\n",
      "Long segment (1443 tokens) → chunking (max 1200)...\n",
      "Long segment (1216 tokens) → chunking (max 1200)...\n",
      "Long segment (1363 tokens) → chunking (max 1200)...\n",
      "Long segment (1317 tokens) → chunking (max 1200)...\n",
      "Long segment (2784 tokens) → chunking (max 1200)...\n",
      "Long segment (1537 tokens) → chunking (max 1200)...\n",
      "Long segment (1241 tokens) → chunking (max 1200)...\n",
      "Long segment (2917 tokens) → chunking (max 1200)...\n",
      "Long segment (2906 tokens) → chunking (max 1200)...\n",
      "Long segment (1462 tokens) → chunking (max 1200)...\n",
      "Long segment (2392 tokens) → chunking (max 1200)...\n",
      "Long segment (2336 tokens) → chunking (max 1200)...\n",
      "Long segment (1726 tokens) → chunking (max 1200)...\n",
      "Long segment (1361 tokens) → chunking (max 1200)...\n",
      "Long segment (2633 tokens) → chunking (max 1200)...\n",
      "Long segment (1923 tokens) → chunking (max 1200)...\n",
      "Long segment (1618 tokens) → chunking (max 1200)...\n",
      "Long segment (1834 tokens) → chunking (max 1200)...\n",
      "Long segment (1398 tokens) → chunking (max 1200)...\n",
      "Long segment (3852 tokens) → chunking (max 1200)...\n",
      "Long segment (2596 tokens) → chunking (max 1200)...\n",
      "Long segment (1887 tokens) → chunking (max 1200)...\n",
      "Long segment (1888 tokens) → chunking (max 1200)...\n",
      "Long segment (1423 tokens) → chunking (max 1200)...\n",
      "Long segment (1880 tokens) → chunking (max 1200)...\n",
      "Long segment (5833 tokens) → chunking (max 1200)...\n",
      "Long segment (1556 tokens) → chunking (max 1200)...\n",
      "Long segment (2337 tokens) → chunking (max 1200)...\n",
      "Long segment (1330 tokens) → chunking (max 1200)...\n",
      "Long segment (1963 tokens) → chunking (max 1200)...\n",
      "Long segment (2075 tokens) → chunking (max 1200)...\n",
      "Long segment (2016 tokens) → chunking (max 1200)...\n",
      "Long segment (1408 tokens) → chunking (max 1200)...\n",
      "Long segment (2673 tokens) → chunking (max 1200)...\n",
      "Long segment (2629 tokens) → chunking (max 1200)...\n",
      "Long segment (1363 tokens) → chunking (max 1200)...\n",
      "Long segment (1345 tokens) → chunking (max 1200)...\n",
      "Long segment (1752 tokens) → chunking (max 1200)...\n",
      "Long segment (1489 tokens) → chunking (max 1200)...\n",
      "Long segment (3966 tokens) → chunking (max 1200)...\n",
      "Long segment (1237 tokens) → chunking (max 1200)...\n",
      "Long segment (1704 tokens) → chunking (max 1200)...\n",
      "Long segment (1613 tokens) → chunking (max 1200)...\n",
      "Long segment (1476 tokens) → chunking (max 1200)...\n",
      "Long segment (1510 tokens) → chunking (max 1200)...\n",
      "Long segment (1952 tokens) → chunking (max 1200)...\n",
      "Long segment (1780 tokens) → chunking (max 1200)...\n",
      "Long segment (1892 tokens) → chunking (max 1200)...\n",
      "Long segment (1610 tokens) → chunking (max 1200)...\n",
      "Long segment (1832 tokens) → chunking (max 1200)...\n",
      "Long segment (2464 tokens) → chunking (max 1200)...\n",
      "Long segment (2119 tokens) → chunking (max 1200)...\n",
      "Long segment (1524 tokens) → chunking (max 1200)...\n",
      "Long segment (1899 tokens) → chunking (max 1200)...\n",
      "Long segment (1720 tokens) → chunking (max 1200)...\n",
      "Long segment (1846 tokens) → chunking (max 1200)...\n",
      "Long segment (2173 tokens) → chunking (max 1200)...\n",
      "Long segment (1300 tokens) → chunking (max 1200)...\n",
      "Long segment (1289 tokens) → chunking (max 1200)...\n",
      "Long segment (1245 tokens) → chunking (max 1200)...\n",
      "Long segment (3938 tokens) → chunking (max 1200)...\n",
      "Long segment (1214 tokens) → chunking (max 1200)...\n",
      "Long segment (1986 tokens) → chunking (max 1200)...\n",
      "Long segment (1482 tokens) → chunking (max 1200)...\n",
      "Long segment (2461 tokens) → chunking (max 1200)...\n",
      "Long segment (1449 tokens) → chunking (max 1200)...\n",
      "Long segment (2374 tokens) → chunking (max 1200)...\n",
      "Long segment (2322 tokens) → chunking (max 1200)...\n",
      "Long segment (1302 tokens) → chunking (max 1200)...\n",
      "Long segment (1402 tokens) → chunking (max 1200)...\n",
      "Long segment (1641 tokens) → chunking (max 1200)...\n",
      "Long segment (3304 tokens) → chunking (max 1200)...\n",
      "Long segment (1800 tokens) → chunking (max 1200)...\n",
      "Long segment (1260 tokens) → chunking (max 1200)...\n",
      "Long segment (1496 tokens) → chunking (max 1200)...\n",
      "Long segment (2664 tokens) → chunking (max 1200)...\n",
      "Long segment (1826 tokens) → chunking (max 1200)...\n",
      "Long segment (2708 tokens) → chunking (max 1200)...\n",
      "Long segment (1204 tokens) → chunking (max 1200)...\n",
      "Long segment (1699 tokens) → chunking (max 1200)...\n",
      "Long segment (1553 tokens) → chunking (max 1200)...\n",
      "Long segment (1482 tokens) → chunking (max 1200)...\n",
      "Long segment (1219 tokens) → chunking (max 1200)...\n",
      "Long segment (1710 tokens) → chunking (max 1200)...\n",
      "Long segment (1929 tokens) → chunking (max 1200)...\n",
      "Long segment (2445 tokens) → chunking (max 1200)...\n",
      "Long segment (1891 tokens) → chunking (max 1200)...\n",
      "Long segment (2491 tokens) → chunking (max 1200)...\n",
      "Long segment (1760 tokens) → chunking (max 1200)...\n",
      "Long segment (1216 tokens) → chunking (max 1200)...\n",
      "Long segment (2361 tokens) → chunking (max 1200)...\n",
      "Long segment (4907 tokens) → chunking (max 1200)...\n",
      "Long segment (2133 tokens) → chunking (max 1200)...\n",
      "Long segment (1683 tokens) → chunking (max 1200)...\n",
      "Long segment (3426 tokens) → chunking (max 1200)...\n",
      "Long segment (1706 tokens) → chunking (max 1200)...\n",
      "Long segment (1560 tokens) → chunking (max 1200)...\n",
      "Long segment (1445 tokens) → chunking (max 1200)...\n",
      "Long segment (2628 tokens) → chunking (max 1200)...\n",
      "Long segment (4342 tokens) → chunking (max 1200)...\n",
      "Long segment (2430 tokens) → chunking (max 1200)...\n",
      "Long segment (2557 tokens) → chunking (max 1200)...\n",
      "Long segment (1709 tokens) → chunking (max 1200)...\n",
      "Long segment (1622 tokens) → chunking (max 1200)...\n",
      "Long segment (1575 tokens) → chunking (max 1200)...\n",
      "Long segment (1743 tokens) → chunking (max 1200)...\n",
      "Long segment (1443 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   4%|▎         | 18/500 [00:00<00:14, 32.83doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1309 tokens) → chunking (max 1200)...\n",
      "Long segment (1397 tokens) → chunking (max 1200)...\n",
      "Long segment (1670 tokens) → chunking (max 1200)...\n",
      "Long segment (1905 tokens) → chunking (max 1200)...\n",
      "Long segment (2399 tokens) → chunking (max 1200)...\n",
      "Long segment (1423 tokens) → chunking (max 1200)...\n",
      "Long segment (1348 tokens) → chunking (max 1200)...\n",
      "Long segment (2080 tokens) → chunking (max 1200)...\n",
      "Long segment (1759 tokens) → chunking (max 1200)...\n",
      "Long segment (1918 tokens) → chunking (max 1200)...\n",
      "Long segment (1653 tokens) → chunking (max 1200)...\n",
      "Long segment (1414 tokens) → chunking (max 1200)...\n",
      "Long segment (1263 tokens) → chunking (max 1200)...\n",
      "Long segment (8538 tokens) → chunking (max 1200)...\n",
      "Long segment (1438 tokens) → chunking (max 1200)...\n",
      "Long segment (6201 tokens) → chunking (max 1200)...\n",
      "Long segment (1719 tokens) → chunking (max 1200)...\n",
      "Long segment (2434 tokens) → chunking (max 1200)...\n",
      "Long segment (2356 tokens) → chunking (max 1200)...\n",
      "Long segment (1583 tokens) → chunking (max 1200)...\n",
      "Long segment (1440 tokens) → chunking (max 1200)...\n",
      "Long segment (1262 tokens) → chunking (max 1200)...\n",
      "Long segment (1233 tokens) → chunking (max 1200)...\n",
      "Long segment (2624 tokens) → chunking (max 1200)...\n",
      "Long segment (1481 tokens) → chunking (max 1200)...\n",
      "Long segment (3883 tokens) → chunking (max 1200)...\n",
      "Long segment (1812 tokens) → chunking (max 1200)...\n",
      "Long segment (1442 tokens) → chunking (max 1200)...\n",
      "Long segment (1227 tokens) → chunking (max 1200)...\n",
      "Long segment (1445 tokens) → chunking (max 1200)...\n",
      "Long segment (3992 tokens) → chunking (max 1200)...\n",
      "Long segment (1314 tokens) → chunking (max 1200)...\n",
      "Long segment (1941 tokens) → chunking (max 1200)...\n",
      "Long segment (1553 tokens) → chunking (max 1200)...\n",
      "Long segment (2200 tokens) → chunking (max 1200)...\n",
      "Long segment (1681 tokens) → chunking (max 1200)...\n",
      "Long segment (3237 tokens) → chunking (max 1200)...\n",
      "Long segment (1449 tokens) → chunking (max 1200)...\n",
      "Long segment (1220 tokens) → chunking (max 1200)...\n",
      "Long segment (1280 tokens) → chunking (max 1200)...\n",
      "Long segment (1616 tokens) → chunking (max 1200)...\n",
      "Long segment (1450 tokens) → chunking (max 1200)...\n",
      "Long segment (1369 tokens) → chunking (max 1200)...\n",
      "Long segment (1448 tokens) → chunking (max 1200)...\n",
      "Long segment (1350 tokens) → chunking (max 1200)...\n",
      "Long segment (2326 tokens) → chunking (max 1200)...\n",
      "Long segment (1656 tokens) → chunking (max 1200)...\n",
      "Long segment (2937 tokens) → chunking (max 1200)...\n",
      "Long segment (3232 tokens) → chunking (max 1200)...\n",
      "Long segment (1321 tokens) → chunking (max 1200)...\n",
      "Long segment (3062 tokens) → chunking (max 1200)...\n",
      "Long segment (1272 tokens) → chunking (max 1200)...\n",
      "Long segment (2686 tokens) → chunking (max 1200)...\n",
      "Long segment (2469 tokens) → chunking (max 1200)...\n",
      "Long segment (2440 tokens) → chunking (max 1200)...\n",
      "Long segment (2332 tokens) → chunking (max 1200)...\n",
      "Long segment (2218 tokens) → chunking (max 1200)...\n",
      "Long segment (1676 tokens) → chunking (max 1200)...\n",
      "Long segment (2228 tokens) → chunking (max 1200)...\n",
      "Long segment (2648 tokens) → chunking (max 1200)...\n",
      "Long segment (1284 tokens) → chunking (max 1200)...\n",
      "Long segment (1309 tokens) → chunking (max 1200)...\n",
      "Long segment (2037 tokens) → chunking (max 1200)...\n",
      "Long segment (1946 tokens) → chunking (max 1200)...\n",
      "Long segment (1532 tokens) → chunking (max 1200)...\n",
      "Long segment (6793 tokens) → chunking (max 1200)...\n",
      "Long segment (1471 tokens) → chunking (max 1200)...\n",
      "Long segment (2121 tokens) → chunking (max 1200)...\n",
      "Long segment (1814 tokens) → chunking (max 1200)...\n",
      "Long segment (1485 tokens) → chunking (max 1200)...\n",
      "Long segment (1526 tokens) → chunking (max 1200)...\n",
      "Long segment (2498 tokens) → chunking (max 1200)...\n",
      "Long segment (13073 tokens) → chunking (max 1200)...\n",
      "Long segment (1534 tokens) → chunking (max 1200)...\n",
      "Long segment (4554 tokens) → chunking (max 1200)...\n",
      "Long segment (1226 tokens) → chunking (max 1200)...\n",
      "Long segment (1226 tokens) → chunking (max 1200)...\n",
      "Long segment (1677 tokens) → chunking (max 1200)...\n",
      "Long segment (1528 tokens) → chunking (max 1200)...\n",
      "Long segment (1726 tokens) → chunking (max 1200)...\n",
      "Long segment (3611 tokens) → chunking (max 1200)...\n",
      "Long segment (8136 tokens) → chunking (max 1200)...\n",
      "Long segment (1364 tokens) → chunking (max 1200)...\n",
      "Long segment (1249 tokens) → chunking (max 1200)...\n",
      "Long segment (1307 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   5%|▍         | 24/500 [00:02<00:56,  8.47doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1499 tokens) → chunking (max 1200)...Long segment (1962 tokens) → chunking (max 1200)...\n",
      "Long segment (1246 tokens) → chunking (max 1200)...\n",
      "Long segment (1452 tokens) → chunking (max 1200)...\n",
      "Long segment (1355 tokens) → chunking (max 1200)...\n",
      "Long segment (3638 tokens) → chunking (max 1200)...\n",
      "Long segment (1281 tokens) → chunking (max 1200)...\n",
      "Long segment (1545 tokens) → chunking (max 1200)...\n",
      "Long segment (2828 tokens) → chunking (max 1200)...\n",
      "Long segment (14768 tokens) → chunking (max 1200)...\n",
      "Long segment (1661 tokens) → chunking (max 1200)...\n",
      "Long segment (1254 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1500 tokens) → chunking (max 1200)...\n",
      "Long segment (3448 tokens) → chunking (max 1200)...\n",
      "Long segment (2098 tokens) → chunking (max 1200)...\n",
      "Long segment (2819 tokens) → chunking (max 1200)...\n",
      "Long segment (2264 tokens) → chunking (max 1200)...\n",
      "Long segment (1253 tokens) → chunking (max 1200)...\n",
      "Long segment (2395 tokens) → chunking (max 1200)...\n",
      "Long segment (1739 tokens) → chunking (max 1200)...\n",
      "Long segment (2077 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   5%|▌         | 26/500 [00:02<01:12,  6.56doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2675 tokens) → chunking (max 1200)...\n",
      "Long segment (1810 tokens) → chunking (max 1200)...\n",
      "Long segment (1788 tokens) → chunking (max 1200)...\n",
      "Long segment (2510 tokens) → chunking (max 1200)...\n",
      "Long segment (1263 tokens) → chunking (max 1200)...\n",
      "Long segment (3012 tokens) → chunking (max 1200)...\n",
      "Long segment (5046 tokens) → chunking (max 1200)...\n",
      "Long segment (1356 tokens) → chunking (max 1200)...\n",
      "Long segment (1690 tokens) → chunking (max 1200)...\n",
      "Long segment (1208 tokens) → chunking (max 1200)...\n",
      "Long segment (1476 tokens) → chunking (max 1200)...\n",
      "Long segment (1376 tokens) → chunking (max 1200)...\n",
      "Long segment (1330 tokens) → chunking (max 1200)...\n",
      "Long segment (1778 tokens) → chunking (max 1200)...\n",
      "Long segment (2654 tokens) → chunking (max 1200)...\n",
      "Long segment (1271 tokens) → chunking (max 1200)...\n",
      "Long segment (2929 tokens) → chunking (max 1200)...\n",
      "Long segment (1800 tokens) → chunking (max 1200)...\n",
      "Long segment (1494 tokens) → chunking (max 1200)...\n",
      "Long segment (1476 tokens) → chunking (max 1200)...\n",
      "Long segment (1408 tokens) → chunking (max 1200)...\n",
      "Long segment (1521 tokens) → chunking (max 1200)...\n",
      "Long segment (4668 tokens) → chunking (max 1200)...\n",
      "Long segment (1925 tokens) → chunking (max 1200)...\n",
      "Long segment (1772 tokens) → chunking (max 1200)...\n",
      "Long segment (3364 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (1368 tokens) → chunking (max 1200)...\n",
      "Long segment (2700 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   6%|▌         | 28/500 [00:07<04:56,  1.59doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1439 tokens) → chunking (max 1200)...\n",
      "Long segment (1286 tokens) → chunking (max 1200)...\n",
      "Long segment (1992 tokens) → chunking (max 1200)...\n",
      "Long segment (1682 tokens) → chunking (max 1200)...\n",
      "Long segment (1604 tokens) → chunking (max 1200)...\n",
      "Long segment (2651 tokens) → chunking (max 1200)...\n",
      "Long segment (2618 tokens) → chunking (max 1200)...\n",
      "Long segment (3669 tokens) → chunking (max 1200)...\n",
      "Long segment (2082 tokens) → chunking (max 1200)...\n",
      "Long segment (2472 tokens) → chunking (max 1200)...\n",
      "Long segment (2202 tokens) → chunking (max 1200)...\n",
      "Long segment (3887 tokens) → chunking (max 1200)...\n",
      "Long segment (3423 tokens) → chunking (max 1200)...\n",
      "Long segment (1362 tokens) → chunking (max 1200)...\n",
      "Long segment (2786 tokens) → chunking (max 1200)...\n",
      "Long segment (1402 tokens) → chunking (max 1200)...\n",
      "Long segment (1319 tokens) → chunking (max 1200)...\n",
      "Long segment (1609 tokens) → chunking (max 1200)...\n",
      "Long segment (2454 tokens) → chunking (max 1200)...\n",
      "Long segment (2686 tokens) → chunking (max 1200)...\n",
      "Long segment (1213 tokens) → chunking (max 1200)...\n",
      "Long segment (1804 tokens) → chunking (max 1200)...\n",
      "Long segment (1231 tokens) → chunking (max 1200)...\n",
      "Long segment (2894 tokens) → chunking (max 1200)...\n",
      "Long segment (1295 tokens) → chunking (max 1200)...\n",
      "Long segment (2016 tokens) → chunking (max 1200)...\n",
      "Long segment (4409 tokens) → chunking (max 1200)...\n",
      "Long segment (1398 tokens) → chunking (max 1200)...\n",
      "Long segment (1675 tokens) → chunking (max 1200)...\n",
      "Long segment (1394 tokens) → chunking (max 1200)...\n",
      "Long segment (1442 tokens) → chunking (max 1200)...\n",
      "Long segment (1441 tokens) → chunking (max 1200)...\n",
      "Long segment (1831 tokens) → chunking (max 1200)...\n",
      "Long segment (1371 tokens) → chunking (max 1200)...\n",
      "Long segment (1934 tokens) → chunking (max 1200)...\n",
      "Long segment (1849 tokens) → chunking (max 1200)...\n",
      "Long segment (1736 tokens) → chunking (max 1200)...\n",
      "Long segment (1516 tokens) → chunking (max 1200)...\n",
      "Long segment (2382 tokens) → chunking (max 1200)...\n",
      "Long segment (1479 tokens) → chunking (max 1200)...\n",
      "Long segment (1348 tokens) → chunking (max 1200)...\n",
      "Long segment (1616 tokens) → chunking (max 1200)...\n",
      "Long segment (1575 tokens) → chunking (max 1200)...\n",
      "Long segment (2052 tokens) → chunking (max 1200)...\n",
      "Long segment (2290 tokens) → chunking (max 1200)...\n",
      "Long segment (1310 tokens) → chunking (max 1200)...\n",
      "Long segment (2932 tokens) → chunking (max 1200)...\n",
      "Long segment (4237 tokens) → chunking (max 1200)...\n",
      "Long segment (1755 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (1469 tokens) → chunking (max 1200)...\n",
      "Long segment (1659 tokens) → chunking (max 1200)...\n",
      "Long segment (1649 tokens) → chunking (max 1200)...\n",
      "Long segment (1228 tokens) → chunking (max 1200)...\n",
      "Long segment (3768 tokens) → chunking (max 1200)...\n",
      "Long segment (1375 tokens) → chunking (max 1200)...\n",
      "Long segment (2546 tokens) → chunking (max 1200)...\n",
      "Long segment (2280 tokens) → chunking (max 1200)...\n",
      "Long segment (1442 tokens) → chunking (max 1200)...\n",
      "Long segment (2008 tokens) → chunking (max 1200)...\n",
      "Long segment (2550 tokens) → chunking (max 1200)...\n",
      "Long segment (1271 tokens) → chunking (max 1200)...\n",
      "Long segment (1886 tokens) → chunking (max 1200)...\n",
      "Long segment (2664 tokens) → chunking (max 1200)...\n",
      "Long segment (1369 tokens) → chunking (max 1200)...\n",
      "Long segment (1255 tokens) → chunking (max 1200)...\n",
      "Long segment (2392 tokens) → chunking (max 1200)...\n",
      "Long segment (1957 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   6%|▌         | 30/500 [00:10<07:30,  1.04doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1714 tokens) → chunking (max 1200)...\n",
      "Long segment (1934 tokens) → chunking (max 1200)...\n",
      "Long segment (2567 tokens) → chunking (max 1200)...\n",
      "Long segment (1285 tokens) → chunking (max 1200)...\n",
      "Long segment (1217 tokens) → chunking (max 1200)...\n",
      "Long segment (3764 tokens) → chunking (max 1200)...\n",
      "Long segment (1781 tokens) → chunking (max 1200)...\n",
      "Long segment (2620 tokens) → chunking (max 1200)...\n",
      "Long segment (1286 tokens) → chunking (max 1200)...\n",
      "Long segment (5179 tokens) → chunking (max 1200)...\n",
      "Long segment (1210 tokens) → chunking (max 1200)...\n",
      "Long segment (1341 tokens) → chunking (max 1200)...\n",
      "Long segment (2004 tokens) → chunking (max 1200)...\n",
      "Long segment (1567 tokens) → chunking (max 1200)...\n",
      "Long segment (2480 tokens) → chunking (max 1200)...\n",
      "Long segment (1466 tokens) → chunking (max 1200)...\n",
      "Long segment (2309 tokens) → chunking (max 1200)...\n",
      "Long segment (1496 tokens) → chunking (max 1200)...\n",
      "Long segment (1746 tokens) → chunking (max 1200)...\n",
      "Long segment (2219 tokens) → chunking (max 1200)...\n",
      "Long segment (1364 tokens) → chunking (max 1200)...\n",
      "Long segment (1829 tokens) → chunking (max 1200)...\n",
      "Long segment (1425 tokens) → chunking (max 1200)...\n",
      "Long segment (3247 tokens) → chunking (max 1200)...\n",
      "Long segment (1870 tokens) → chunking (max 1200)...\n",
      "Long segment (1672 tokens) → chunking (max 1200)...\n",
      "Long segment (1282 tokens) → chunking (max 1200)...\n",
      "Long segment (1919 tokens) → chunking (max 1200)...\n",
      "Long segment (1662 tokens) → chunking (max 1200)...\n",
      "Long segment (2500 tokens) → chunking (max 1200)...\n",
      "Long segment (1408 tokens) → chunking (max 1200)...\n",
      "Long segment (2686 tokens) → chunking (max 1200)...\n",
      "Long segment (1527 tokens) → chunking (max 1200)...\n",
      "Long segment (1214 tokens) → chunking (max 1200)...\n",
      "Long segment (1643 tokens) → chunking (max 1200)...\n",
      "Long segment (2053 tokens) → chunking (max 1200)...\n",
      "Long segment (2746 tokens) → chunking (max 1200)...\n",
      "Long segment (1655 tokens) → chunking (max 1200)...\n",
      "Long segment (1683 tokens) → chunking (max 1200)...\n",
      "Long segment (1715 tokens) → chunking (max 1200)...\n",
      "Long segment (1206 tokens) → chunking (max 1200)...\n",
      "Long segment (3212 tokens) → chunking (max 1200)...\n",
      "Long segment (1316 tokens) → chunking (max 1200)...\n",
      "Long segment (1221 tokens) → chunking (max 1200)...\n",
      "Long segment (1336 tokens) → chunking (max 1200)...\n",
      "Long segment (1655 tokens) → chunking (max 1200)...\n",
      "Long segment (1415 tokens) → chunking (max 1200)...\n",
      "Long segment (2375 tokens) → chunking (max 1200)...\n",
      "Long segment (1921 tokens) → chunking (max 1200)...\n",
      "Long segment (1755 tokens) → chunking (max 1200)...\n",
      "Long segment (2924 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   6%|▌         | 31/500 [00:11<07:34,  1.03doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2364 tokens) → chunking (max 1200)...\n",
      "Long segment (3758 tokens) → chunking (max 1200)...\n",
      "Long segment (3532 tokens) → chunking (max 1200)...\n",
      "Long segment (1318 tokens) → chunking (max 1200)...\n",
      "Long segment (4993 tokens) → chunking (max 1200)...\n",
      "Long segment (1324 tokens) → chunking (max 1200)...\n",
      "Long segment (3227 tokens) → chunking (max 1200)...\n",
      "Long segment (1819 tokens) → chunking (max 1200)...\n",
      "Long segment (1375 tokens) → chunking (max 1200)...\n",
      "Long segment (5101 tokens) → chunking (max 1200)...\n",
      "Long segment (1696 tokens) → chunking (max 1200)...\n",
      "Long segment (3056 tokens) → chunking (max 1200)...\n",
      "Long segment (2265 tokens) → chunking (max 1200)...\n",
      "Long segment (1305 tokens) → chunking (max 1200)...\n",
      "Long segment (3645 tokens) → chunking (max 1200)...\n",
      "Long segment (1622 tokens) → chunking (max 1200)...\n",
      "Long segment (1571 tokens) → chunking (max 1200)...\n",
      "Long segment (3579 tokens) → chunking (max 1200)...\n",
      "Long segment (3080 tokens) → chunking (max 1200)...\n",
      "Long segment (1994 tokens) → chunking (max 1200)...\n",
      "Long segment (1754 tokens) → chunking (max 1200)...\n",
      "Long segment (2328 tokens) → chunking (max 1200)...\n",
      "Long segment (2332 tokens) → chunking (max 1200)...\n",
      "Long segment (1404 tokens) → chunking (max 1200)...\n",
      "Long segment (1303 tokens) → chunking (max 1200)...\n",
      "Long segment (1270 tokens) → chunking (max 1200)...\n",
      "Long segment (1629 tokens) → chunking (max 1200)...\n",
      "Long segment (1420 tokens) → chunking (max 1200)...\n",
      "Long segment (1205 tokens) → chunking (max 1200)...\n",
      "Long segment (3268 tokens) → chunking (max 1200)...\n",
      "Long segment (1797 tokens) → chunking (max 1200)...\n",
      "Long segment (1718 tokens) → chunking (max 1200)...\n",
      "Long segment (2266 tokens) → chunking (max 1200)...\n",
      "Long segment (2166 tokens) → chunking (max 1200)...\n",
      "Long segment (1844 tokens) → chunking (max 1200)...\n",
      "Long segment (1771 tokens) → chunking (max 1200)...\n",
      "Long segment (1425 tokens) → chunking (max 1200)...\n",
      "Long segment (1665 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   7%|▋         | 33/500 [00:12<06:50,  1.14doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1607 tokens) → chunking (max 1200)...\n",
      "Long segment (1728 tokens) → chunking (max 1200)...\n",
      "Long segment (1855 tokens) → chunking (max 1200)...\n",
      "Long segment (2100 tokens) → chunking (max 1200)...\n",
      "Long segment (1633 tokens) → chunking (max 1200)...\n",
      "Long segment (1619 tokens) → chunking (max 1200)...\n",
      "Long segment (1378 tokens) → chunking (max 1200)...\n",
      "Long segment (2002 tokens) → chunking (max 1200)...\n",
      "Long segment (1203 tokens) → chunking (max 1200)...\n",
      "Long segment (7898 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   7%|▋         | 34/500 [00:13<06:31,  1.19doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1318 tokens) → chunking (max 1200)...\n",
      "Long segment (3815 tokens) → chunking (max 1200)...\n",
      "Long segment (2008 tokens) → chunking (max 1200)...\n",
      "Long segment (2004 tokens) → chunking (max 1200)...\n",
      "Long segment (1883 tokens) → chunking (max 1200)...\n",
      "Long segment (3825 tokens) → chunking (max 1200)...\n",
      "Long segment (2004 tokens) → chunking (max 1200)...\n",
      "Long segment (1444 tokens) → chunking (max 1200)...\n",
      "Long segment (2733 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   7%|▋         | 36/500 [00:14<06:10,  1.25doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1456 tokens) → chunking (max 1200)...\n",
      "Long segment (1553 tokens) → chunking (max 1200)...\n",
      "Long segment (1788 tokens) → chunking (max 1200)...\n",
      "Long segment (1508 tokens) → chunking (max 1200)...\n",
      "Long segment (1462 tokens) → chunking (max 1200)...\n",
      "Long segment (1824 tokens) → chunking (max 1200)...\n",
      "Long segment (2325 tokens) → chunking (max 1200)...\n",
      "Long segment (2495 tokens) → chunking (max 1200)...\n",
      "Long segment (1235 tokens) → chunking (max 1200)...\n",
      "Long segment (1643 tokens) → chunking (max 1200)...\n",
      "Long segment (1397 tokens) → chunking (max 1200)...\n",
      "Long segment (2206 tokens) → chunking (max 1200)...\n",
      "Long segment (1721 tokens) → chunking (max 1200)...\n",
      "Long segment (2015 tokens) → chunking (max 1200)...\n",
      "Long segment (1768 tokens) → chunking (max 1200)...\n",
      "Long segment (1322 tokens) → chunking (max 1200)...\n",
      "Long segment (1210 tokens) → chunking (max 1200)...\n",
      "Long segment (1219 tokens) → chunking (max 1200)...\n",
      "Long segment (1780 tokens) → chunking (max 1200)...\n",
      "Long segment (1941 tokens) → chunking (max 1200)...\n",
      "Long segment (1327 tokens) → chunking (max 1200)...\n",
      "Long segment (1398 tokens) → chunking (max 1200)...\n",
      "Long segment (1366 tokens) → chunking (max 1200)...\n",
      "Long segment (1653 tokens) → chunking (max 1200)...\n",
      "Long segment (1463 tokens) → chunking (max 1200)...\n",
      "Long segment (1833 tokens) → chunking (max 1200)...\n",
      "Long segment (1388 tokens) → chunking (max 1200)...\n",
      "Long segment (1247 tokens) → chunking (max 1200)...\n",
      "Long segment (3260 tokens) → chunking (max 1200)...\n",
      "Long segment (1494 tokens) → chunking (max 1200)...\n",
      "Long segment (3636 tokens) → chunking (max 1200)...\n",
      "Long segment (1201 tokens) → chunking (max 1200)...\n",
      "Long segment (1361 tokens) → chunking (max 1200)...\n",
      "Long segment (4200 tokens) → chunking (max 1200)...\n",
      "Long segment (1566 tokens) → chunking (max 1200)...\n",
      "Long segment (2513 tokens) → chunking (max 1200)...\n",
      "Long segment (1429 tokens) → chunking (max 1200)...\n",
      "Long segment (1788 tokens) → chunking (max 1200)...\n",
      "Long segment (1636 tokens) → chunking (max 1200)...\n",
      "Long segment (1532 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   8%|▊         | 38/500 [00:17<07:58,  1.03s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1424 tokens) → chunking (max 1200)...\n",
      "Long segment (3901 tokens) → chunking (max 1200)...\n",
      "Long segment (5203 tokens) → chunking (max 1200)...\n",
      "Long segment (1421 tokens) → chunking (max 1200)...\n",
      "Long segment (1840 tokens) → chunking (max 1200)...\n",
      "Long segment (4120 tokens) → chunking (max 1200)...\n",
      "Long segment (1345 tokens) → chunking (max 1200)...\n",
      "Long segment (1838 tokens) → chunking (max 1200)...\n",
      "Long segment (1687 tokens) → chunking (max 1200)...\n",
      "Long segment (1944 tokens) → chunking (max 1200)...\n",
      "Long segment (2849 tokens) → chunking (max 1200)...\n",
      "Long segment (1428 tokens) → chunking (max 1200)...\n",
      "Long segment (1337 tokens) → chunking (max 1200)...\n",
      "Long segment (3049 tokens) → chunking (max 1200)...\n",
      "Long segment (2294 tokens) → chunking (max 1200)...\n",
      "Long segment (3021 tokens) → chunking (max 1200)...\n",
      "Long segment (1943 tokens) → chunking (max 1200)...\n",
      "Long segment (1577 tokens) → chunking (max 1200)...\n",
      "Long segment (1211 tokens) → chunking (max 1200)...\n",
      "Long segment (1363 tokens) → chunking (max 1200)...\n",
      "Long segment (1651 tokens) → chunking (max 1200)...\n",
      "Long segment (1423 tokens) → chunking (max 1200)...\n",
      "Long segment (2610 tokens) → chunking (max 1200)...\n",
      "Long segment (2344 tokens) → chunking (max 1200)...\n",
      "Long segment (1203 tokens) → chunking (max 1200)...\n",
      "Long segment (2086 tokens) → chunking (max 1200)...\n",
      "Long segment (1751 tokens) → chunking (max 1200)...\n",
      "Long segment (5374 tokens) → chunking (max 1200)...\n",
      "Long segment (2391 tokens) → chunking (max 1200)...\n",
      "Long segment (2811 tokens) → chunking (max 1200)...\n",
      "Long segment (1397 tokens) → chunking (max 1200)...\n",
      "Long segment (2603 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   8%|▊         | 40/500 [00:20<08:35,  1.12s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (3989 tokens) → chunking (max 1200)...Long segment (2124 tokens) → chunking (max 1200)...\n",
      "Long segment (1476 tokens) → chunking (max 1200)...\n",
      "Long segment (1227 tokens) → chunking (max 1200)...\n",
      "Long segment (1228 tokens) → chunking (max 1200)...\n",
      "Long segment (2484 tokens) → chunking (max 1200)...\n",
      "Long segment (1426 tokens) → chunking (max 1200)...\n",
      "Long segment (1745 tokens) → chunking (max 1200)...\n",
      "Long segment (1414 tokens) → chunking (max 1200)...\n",
      "Long segment (1898 tokens) → chunking (max 1200)...\n",
      "Long segment (1322 tokens) → chunking (max 1200)...\n",
      "Long segment (3229 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1599 tokens) → chunking (max 1200)...\n",
      "Long segment (1399 tokens) → chunking (max 1200)...\n",
      "Long segment (1305 tokens) → chunking (max 1200)...\n",
      "Long segment (1205 tokens) → chunking (max 1200)...\n",
      "Long segment (1666 tokens) → chunking (max 1200)...\n",
      "Long segment (1327 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (1906 tokens) → chunking (max 1200)...\n",
      "Long segment (1439 tokens) → chunking (max 1200)...\n",
      "Long segment (1586 tokens) → chunking (max 1200)...\n",
      "Long segment (1362 tokens) → chunking (max 1200)...\n",
      "Long segment (1803 tokens) → chunking (max 1200)...\n",
      "Long segment (1329 tokens) → chunking (max 1200)...\n",
      "Long segment (1253 tokens) → chunking (max 1200)...\n",
      "Long segment (2287 tokens) → chunking (max 1200)...\n",
      "Long segment (1606 tokens) → chunking (max 1200)...\n",
      "Long segment (1360 tokens) → chunking (max 1200)...\n",
      "Long segment (1872 tokens) → chunking (max 1200)...\n",
      "Long segment (2159 tokens) → chunking (max 1200)...\n",
      "Long segment (1726 tokens) → chunking (max 1200)...\n",
      "Long segment (1277 tokens) → chunking (max 1200)...\n",
      "Long segment (1291 tokens) → chunking (max 1200)...\n",
      "Long segment (1905 tokens) → chunking (max 1200)...\n",
      "Long segment (2746 tokens) → chunking (max 1200)...\n",
      "Long segment (2475 tokens) → chunking (max 1200)...\n",
      "Long segment (1676 tokens) → chunking (max 1200)...\n",
      "Long segment (1308 tokens) → chunking (max 1200)...\n",
      "Long segment (2503 tokens) → chunking (max 1200)...\n",
      "Long segment (1574 tokens) → chunking (max 1200)...\n",
      "Long segment (1253 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   8%|▊         | 42/500 [00:23<10:26,  1.37s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1395 tokens) → chunking (max 1200)...\n",
      "Long segment (1234 tokens) → chunking (max 1200)...\n",
      "Long segment (1380 tokens) → chunking (max 1200)...\n",
      "Long segment (1258 tokens) → chunking (max 1200)...\n",
      "Long segment (2027 tokens) → chunking (max 1200)...\n",
      "Long segment (2040 tokens) → chunking (max 1200)...\n",
      "Long segment (4461 tokens) → chunking (max 1200)...\n",
      "Long segment (2202 tokens) → chunking (max 1200)...\n",
      "Long segment (1291 tokens) → chunking (max 1200)...\n",
      "Long segment (3247 tokens) → chunking (max 1200)...\n",
      "Long segment (1868 tokens) → chunking (max 1200)...\n",
      "Long segment (1453 tokens) → chunking (max 1200)...\n",
      "Long segment (2628 tokens) → chunking (max 1200)...\n",
      "Long segment (1258 tokens) → chunking (max 1200)...\n",
      "Long segment (1715 tokens) → chunking (max 1200)...\n",
      "Long segment (1529 tokens) → chunking (max 1200)...\n",
      "Long segment (1465 tokens) → chunking (max 1200)...\n",
      "Long segment (2025 tokens) → chunking (max 1200)...\n",
      "Long segment (2380 tokens) → chunking (max 1200)...\n",
      "Long segment (1586 tokens) → chunking (max 1200)...\n",
      "Long segment (1614 tokens) → chunking (max 1200)...\n",
      "Long segment (1725 tokens) → chunking (max 1200)...\n",
      "Long segment (1371 tokens) → chunking (max 1200)...\n",
      "Long segment (2193 tokens) → chunking (max 1200)...\n",
      "Long segment (1231 tokens) → chunking (max 1200)...\n",
      "Long segment (2989 tokens) → chunking (max 1200)...\n",
      "Long segment (2092 tokens) → chunking (max 1200)...\n",
      "Long segment (1477 tokens) → chunking (max 1200)...\n",
      "Long segment (1228 tokens) → chunking (max 1200)...\n",
      "Long segment (2366 tokens) → chunking (max 1200)...\n",
      "Long segment (1202 tokens) → chunking (max 1200)...\n",
      "Long segment (2035 tokens) → chunking (max 1200)...\n",
      "Long segment (1470 tokens) → chunking (max 1200)...\n",
      "Long segment (1977 tokens) → chunking (max 1200)...\n",
      "Long segment (2227 tokens) → chunking (max 1200)...\n",
      "Long segment (3919 tokens) → chunking (max 1200)...\n",
      "Long segment (1400 tokens) → chunking (max 1200)...\n",
      "Long segment (1423 tokens) → chunking (max 1200)...\n",
      "Long segment (1600 tokens) → chunking (max 1200)...\n",
      "Long segment (1740 tokens) → chunking (max 1200)...\n",
      "Long segment (2582 tokens) → chunking (max 1200)...\n",
      "Long segment (1467 tokens) → chunking (max 1200)...\n",
      "Long segment (1231 tokens) → chunking (max 1200)...\n",
      "Long segment (1453 tokens) → chunking (max 1200)...\n",
      "Long segment (1724 tokens) → chunking (max 1200)...\n",
      "Long segment (1924 tokens) → chunking (max 1200)...\n",
      "Long segment (3486 tokens) → chunking (max 1200)...\n",
      "Long segment (1555 tokens) → chunking (max 1200)...\n",
      "Long segment (1557 tokens) → chunking (max 1200)...\n",
      "Long segment (2802 tokens) → chunking (max 1200)...\n",
      "Long segment (4104 tokens) → chunking (max 1200)...\n",
      "Long segment (2851 tokens) → chunking (max 1200)...\n",
      "Long segment (1586 tokens) → chunking (max 1200)...\n",
      "Long segment (5716 tokens) → chunking (max 1200)...\n",
      "Long segment (2575 tokens) → chunking (max 1200)...\n",
      "Long segment (1877 tokens) → chunking (max 1200)...\n",
      "Long segment (1687 tokens) → chunking (max 1200)...\n",
      "Long segment (1651 tokens) → chunking (max 1200)...\n",
      "Long segment (1820 tokens) → chunking (max 1200)...\n",
      "Long segment (2346 tokens) → chunking (max 1200)...\n",
      "Long segment (1233 tokens) → chunking (max 1200)...\n",
      "Long segment (1271 tokens) → chunking (max 1200)...\n",
      "Long segment (2068 tokens) → chunking (max 1200)...\n",
      "Long segment (2022 tokens) → chunking (max 1200)...\n",
      "Long segment (2284 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   9%|▊         | 43/500 [00:24<09:29,  1.25s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1288 tokens) → chunking (max 1200)...Long segment (2140 tokens) → chunking (max 1200)...\n",
      "Long segment (1696 tokens) → chunking (max 1200)...\n",
      "Long segment (1438 tokens) → chunking (max 1200)...\n",
      "Long segment (1211 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1321 tokens) → chunking (max 1200)...\n",
      "Long segment (1551 tokens) → chunking (max 1200)...\n",
      "Long segment (1990 tokens) → chunking (max 1200)...\n",
      "Long segment (1366 tokens) → chunking (max 1200)...\n",
      "Long segment (1277 tokens) → chunking (max 1200)...\n",
      "Long segment (1717 tokens) → chunking (max 1200)...\n",
      "Long segment (1735 tokens) → chunking (max 1200)...\n",
      "Long segment (1326 tokens) → chunking (max 1200)...\n",
      "Long segment (1796 tokens) → chunking (max 1200)...\n",
      "Long segment (1961 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   9%|▉         | 45/500 [00:26<09:45,  1.29s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (3477 tokens) → chunking (max 1200)...\n",
      "Long segment (1256 tokens) → chunking (max 1200)...\n",
      "Long segment (1282 tokens) → chunking (max 1200)...\n",
      "Long segment (1267 tokens) → chunking (max 1200)...\n",
      "Long segment (2086 tokens) → chunking (max 1200)...\n",
      "Long segment (1628 tokens) → chunking (max 1200)...\n",
      "Long segment (1335 tokens) → chunking (max 1200)...\n",
      "Long segment (1438 tokens) → chunking (max 1200)...\n",
      "Long segment (2392 tokens) → chunking (max 1200)...\n",
      "Long segment (1218 tokens) → chunking (max 1200)...\n",
      "Long segment (1430 tokens) → chunking (max 1200)...\n",
      "Long segment (1251 tokens) → chunking (max 1200)...\n",
      "Long segment (1367 tokens) → chunking (max 1200)...\n",
      "Long segment (2043 tokens) → chunking (max 1200)...\n",
      "Long segment (1299 tokens) → chunking (max 1200)...\n",
      "Long segment (1316 tokens) → chunking (max 1200)...\n",
      "Long segment (1714 tokens) → chunking (max 1200)...\n",
      "Long segment (2426 tokens) → chunking (max 1200)...\n",
      "Long segment (1289 tokens) → chunking (max 1200)...\n",
      "Long segment (2236 tokens) → chunking (max 1200)...\n",
      "Long segment (2005 tokens) → chunking (max 1200)...\n",
      "Long segment (1713 tokens) → chunking (max 1200)...\n",
      "Long segment (1273 tokens) → chunking (max 1200)...\n",
      "Long segment (2763 tokens) → chunking (max 1200)...\n",
      "Long segment (3595 tokens) → chunking (max 1200)...\n",
      "Long segment (1319 tokens) → chunking (max 1200)...\n",
      "Long segment (4601 tokens) → chunking (max 1200)...\n",
      "Long segment (1478 tokens) → chunking (max 1200)...\n",
      "Long segment (5092 tokens) → chunking (max 1200)...\n",
      "Long segment (2218 tokens) → chunking (max 1200)...\n",
      "Long segment (1720 tokens) → chunking (max 1200)...\n",
      "Long segment (2270 tokens) → chunking (max 1200)...\n",
      "Long segment (1519 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   9%|▉         | 46/500 [00:28<09:46,  1.29s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1361 tokens) → chunking (max 1200)...\n",
      "Long segment (1231 tokens) → chunking (max 1200)...\n",
      "Long segment (1559 tokens) → chunking (max 1200)...\n",
      "Long segment (1675 tokens) → chunking (max 1200)...\n",
      "Long segment (2135 tokens) → chunking (max 1200)...\n",
      "Long segment (1311 tokens) → chunking (max 1200)...\n",
      "Long segment (1652 tokens) → chunking (max 1200)...\n",
      "Long segment (2109 tokens) → chunking (max 1200)...\n",
      "Long segment (2044 tokens) → chunking (max 1200)...\n",
      "Long segment (1525 tokens) → chunking (max 1200)...\n",
      "Long segment (1512 tokens) → chunking (max 1200)...\n",
      "Long segment (1508 tokens) → chunking (max 1200)...\n",
      "Long segment (1470 tokens) → chunking (max 1200)...\n",
      "Long segment (2100 tokens) → chunking (max 1200)...\n",
      "Long segment (1821 tokens) → chunking (max 1200)...\n",
      "Long segment (2182 tokens) → chunking (max 1200)...\n",
      "Long segment (3058 tokens) → chunking (max 1200)...\n",
      "Long segment (1250 tokens) → chunking (max 1200)...\n",
      "Long segment (1793 tokens) → chunking (max 1200)...\n",
      "Long segment (2511 tokens) → chunking (max 1200)...\n",
      "Long segment (2246 tokens) → chunking (max 1200)...\n",
      "Long segment (2206 tokens) → chunking (max 1200)...\n",
      "Long segment (1577 tokens) → chunking (max 1200)...\n",
      "Long segment (1320 tokens) → chunking (max 1200)...\n",
      "Long segment (1201 tokens) → chunking (max 1200)...\n",
      "Long segment (3966 tokens) → chunking (max 1200)...\n",
      "Long segment (2219 tokens) → chunking (max 1200)...\n",
      "Long segment (1427 tokens) → chunking (max 1200)...\n",
      "Long segment (3134 tokens) → chunking (max 1200)...\n",
      "Long segment (2716 tokens) → chunking (max 1200)...\n",
      "Long segment (1350 tokens) → chunking (max 1200)...\n",
      "Long segment (1505 tokens) → chunking (max 1200)...\n",
      "Long segment (2010 tokens) → chunking (max 1200)...\n",
      "Long segment (2168 tokens) → chunking (max 1200)...\n",
      "Long segment (1323 tokens) → chunking (max 1200)...\n",
      "Long segment (1761 tokens) → chunking (max 1200)...\n",
      "Long segment (1432 tokens) → chunking (max 1200)...\n",
      "Long segment (2506 tokens) → chunking (max 1200)...\n",
      "Long segment (2789 tokens) → chunking (max 1200)...\n",
      "Long segment (1371 tokens) → chunking (max 1200)...\n",
      "Long segment (2583 tokens) → chunking (max 1200)...\n",
      "Long segment (1757 tokens) → chunking (max 1200)...\n",
      "Long segment (1514 tokens) → chunking (max 1200)...\n",
      "Long segment (2611 tokens) → chunking (max 1200)...\n",
      "Long segment (1374 tokens) → chunking (max 1200)...\n",
      "Long segment (1453 tokens) → chunking (max 1200)...\n",
      "Long segment (1364 tokens) → chunking (max 1200)...\n",
      "Long segment (1803 tokens) → chunking (max 1200)...\n",
      "Long segment (1370 tokens) → chunking (max 1200)...\n",
      "Long segment (2071 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:   9%|▉         | 47/500 [00:30<11:41,  1.55s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2088 tokens) → chunking (max 1200)...\n",
      "Long segment (1396 tokens) → chunking (max 1200)...\n",
      "Long segment (2044 tokens) → chunking (max 1200)...\n",
      "Long segment (2101 tokens) → chunking (max 1200)...\n",
      "Long segment (1320 tokens) → chunking (max 1200)...\n",
      "Long segment (1444 tokens) → chunking (max 1200)...\n",
      "Long segment (1297 tokens) → chunking (max 1200)...\n",
      "Long segment (1433 tokens) → chunking (max 1200)...\n",
      "Long segment (1453 tokens) → chunking (max 1200)...\n",
      "Long segment (2418 tokens) → chunking (max 1200)...\n",
      "Long segment (1351 tokens) → chunking (max 1200)...\n",
      "Long segment (1721 tokens) → chunking (max 1200)...\n",
      "Long segment (1216 tokens) → chunking (max 1200)...\n",
      "Long segment (1593 tokens) → chunking (max 1200)...\n",
      "Long segment (8104 tokens) → chunking (max 1200)...\n",
      "Long segment (3274 tokens) → chunking (max 1200)...\n",
      "Long segment (1687 tokens) → chunking (max 1200)...\n",
      "Long segment (1502 tokens) → chunking (max 1200)...\n",
      "Long segment (1480 tokens) → chunking (max 1200)...\n",
      "Long segment (1756 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  10%|▉         | 49/500 [00:32<08:47,  1.17s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1427 tokens) → chunking (max 1200)...\n",
      "Long segment (1291 tokens) → chunking (max 1200)...\n",
      "Long segment (2881 tokens) → chunking (max 1200)...\n",
      "Long segment (1261 tokens) → chunking (max 1200)...\n",
      "Long segment (2171 tokens) → chunking (max 1200)...\n",
      "Long segment (1552 tokens) → chunking (max 1200)...\n",
      "Long segment (1623 tokens) → chunking (max 1200)...\n",
      "Long segment (2114 tokens) → chunking (max 1200)...\n",
      "Long segment (2630 tokens) → chunking (max 1200)...\n",
      "Long segment (1227 tokens) → chunking (max 1200)...\n",
      "Long segment (1734 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  10%|█         | 51/500 [00:35<10:07,  1.35s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1373 tokens) → chunking (max 1200)...\n",
      "Long segment (1983 tokens) → chunking (max 1200)...\n",
      "Long segment (1920 tokens) → chunking (max 1200)...\n",
      "Long segment (1592 tokens) → chunking (max 1200)...\n",
      "Long segment (1586 tokens) → chunking (max 1200)...\n",
      "Long segment (1373 tokens) → chunking (max 1200)...\n",
      "Long segment (1265 tokens) → chunking (max 1200)...\n",
      "Long segment (1845 tokens) → chunking (max 1200)...\n",
      "Long segment (4198 tokens) → chunking (max 1200)...\n",
      "Long segment (1352 tokens) → chunking (max 1200)...\n",
      "Long segment (1611 tokens) → chunking (max 1200)...\n",
      "Long segment (1500 tokens) → chunking (max 1200)...\n",
      "Long segment (1264 tokens) → chunking (max 1200)...\n",
      "Long segment (1759 tokens) → chunking (max 1200)...\n",
      "Long segment (1289 tokens) → chunking (max 1200)...\n",
      "Long segment (1236 tokens) → chunking (max 1200)...\n",
      "Long segment (1567 tokens) → chunking (max 1200)...\n",
      "Long segment (1646 tokens) → chunking (max 1200)...\n",
      "Long segment (1333 tokens) → chunking (max 1200)...\n",
      "Long segment (2091 tokens) → chunking (max 1200)...\n",
      "Long segment (3143 tokens) → chunking (max 1200)...\n",
      "Long segment (1620 tokens) → chunking (max 1200)...\n",
      "Long segment (2478 tokens) → chunking (max 1200)...\n",
      "Long segment (2525 tokens) → chunking (max 1200)...\n",
      "Long segment (1419 tokens) → chunking (max 1200)...\n",
      "Long segment (1584 tokens) → chunking (max 1200)...\n",
      "Long segment (1467 tokens) → chunking (max 1200)...\n",
      "Long segment (1556 tokens) → chunking (max 1200)...\n",
      "Long segment (1982 tokens) → chunking (max 1200)...\n",
      "Long segment (1507 tokens) → chunking (max 1200)...\n",
      "Long segment (1323 tokens) → chunking (max 1200)...\n",
      "Long segment (1455 tokens) → chunking (max 1200)...\n",
      "Long segment (1427 tokens) → chunking (max 1200)...\n",
      "Long segment (1694 tokens) → chunking (max 1200)...\n",
      "Long segment (1863 tokens) → chunking (max 1200)...\n",
      "Long segment (1513 tokens) → chunking (max 1200)...\n",
      "Long segment (1898 tokens) → chunking (max 1200)...\n",
      "Long segment (1806 tokens) → chunking (max 1200)...\n",
      "Long segment (1464 tokens) → chunking (max 1200)...\n",
      "Long segment (1931 tokens) → chunking (max 1200)...\n",
      "Long segment (1230 tokens) → chunking (max 1200)...\n",
      "Long segment (1314 tokens) → chunking (max 1200)...\n",
      "Long segment (1427 tokens) → chunking (max 1200)...\n",
      "Long segment (3562 tokens) → chunking (max 1200)...\n",
      "Long segment (1320 tokens) → chunking (max 1200)...\n",
      "Long segment (1740 tokens) → chunking (max 1200)...\n",
      "Long segment (2641 tokens) → chunking (max 1200)...\n",
      "Long segment (1950 tokens) → chunking (max 1200)...\n",
      "Long segment (1385 tokens) → chunking (max 1200)...\n",
      "Long segment (2665 tokens) → chunking (max 1200)...\n",
      "Long segment (1742 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  11%|█         | 53/500 [00:36<08:10,  1.10s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1712 tokens) → chunking (max 1200)...\n",
      "Long segment (1679 tokens) → chunking (max 1200)...\n",
      "Long segment (1440 tokens) → chunking (max 1200)...\n",
      "Long segment (1729 tokens) → chunking (max 1200)...\n",
      "Long segment (1203 tokens) → chunking (max 1200)...\n",
      "Long segment (2887 tokens) → chunking (max 1200)...\n",
      "Long segment (1223 tokens) → chunking (max 1200)...\n",
      "Long segment (1334 tokens) → chunking (max 1200)...\n",
      "Long segment (2225 tokens) → chunking (max 1200)...\n",
      "Long segment (1272 tokens) → chunking (max 1200)...\n",
      "Long segment (2018 tokens) → chunking (max 1200)...\n",
      "Long segment (1895 tokens) → chunking (max 1200)...\n",
      "Long segment (1635 tokens) → chunking (max 1200)...\n",
      "Long segment (1483 tokens) → chunking (max 1200)...\n",
      "Long segment (1649 tokens) → chunking (max 1200)...\n",
      "Long segment (1439 tokens) → chunking (max 1200)...\n",
      "Long segment (1315 tokens) → chunking (max 1200)...\n",
      "Long segment (1744 tokens) → chunking (max 1200)...\n",
      "Long segment (1588 tokens) → chunking (max 1200)...\n",
      "Long segment (1346 tokens) → chunking (max 1200)...\n",
      "Long segment (1243 tokens) → chunking (max 1200)...\n",
      "Long segment (1244 tokens) → chunking (max 1200)...\n",
      "Long segment (1385 tokens) → chunking (max 1200)...\n",
      "Long segment (1758 tokens) → chunking (max 1200)...\n",
      "Long segment (1433 tokens) → chunking (max 1200)...\n",
      "Long segment (1332 tokens) → chunking (max 1200)...\n",
      "Long segment (1415 tokens) → chunking (max 1200)...\n",
      "Long segment (1207 tokens) → chunking (max 1200)...\n",
      "Long segment (1596 tokens) → chunking (max 1200)...\n",
      "Long segment (1270 tokens) → chunking (max 1200)...\n",
      "Long segment (2045 tokens) → chunking (max 1200)...\n",
      "Long segment (1282 tokens) → chunking (max 1200)...\n",
      "Long segment (1805 tokens) → chunking (max 1200)...\n",
      "Long segment (1491 tokens) → chunking (max 1200)...\n",
      "Long segment (1845 tokens) → chunking (max 1200)...\n",
      "Long segment (3033 tokens) → chunking (max 1200)...\n",
      "Long segment (1348 tokens) → chunking (max 1200)...\n",
      "Long segment (1346 tokens) → chunking (max 1200)...\n",
      "Long segment (1266 tokens) → chunking (max 1200)...\n",
      "Long segment (1971 tokens) → chunking (max 1200)...\n",
      "Long segment (1278 tokens) → chunking (max 1200)...\n",
      "Long segment (1291 tokens) → chunking (max 1200)...\n",
      "Long segment (5190 tokens) → chunking (max 1200)...\n",
      "Long segment (1355 tokens) → chunking (max 1200)...\n",
      "Long segment (2569 tokens) → chunking (max 1200)...\n",
      "Long segment (2035 tokens) → chunking (max 1200)...\n",
      "Long segment (1994 tokens) → chunking (max 1200)...\n",
      "Long segment (1211 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  11%|█         | 55/500 [00:39<09:51,  1.33s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2039 tokens) → chunking (max 1200)...Long segment (2195 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1644 tokens) → chunking (max 1200)...\n",
      "Long segment (1356 tokens) → chunking (max 1200)...\n",
      "Long segment (1321 tokens) → chunking (max 1200)...\n",
      "Long segment (2598 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (1267 tokens) → chunking (max 1200)...\n",
      "Long segment (1372 tokens) → chunking (max 1200)...\n",
      "Long segment (1387 tokens) → chunking (max 1200)...\n",
      "Long segment (1717 tokens) → chunking (max 1200)...\n",
      "Long segment (1267 tokens) → chunking (max 1200)...\n",
      "Long segment (2770 tokens) → chunking (max 1200)...\n",
      "Long segment (2516 tokens) → chunking (max 1200)...\n",
      "Long segment (1317 tokens) → chunking (max 1200)...\n",
      "Long segment (1549 tokens) → chunking (max 1200)...\n",
      "Long segment (1201 tokens) → chunking (max 1200)...\n",
      "Long segment (1371 tokens) → chunking (max 1200)...\n",
      "Long segment (2199 tokens) → chunking (max 1200)...\n",
      "Long segment (1666 tokens) → chunking (max 1200)...\n",
      "Long segment (2380 tokens) → chunking (max 1200)...\n",
      "Long segment (3287 tokens) → chunking (max 1200)...\n",
      "Long segment (1389 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (1347 tokens) → chunking (max 1200)...\n",
      "Long segment (1318 tokens) → chunking (max 1200)...\n",
      "Long segment (1268 tokens) → chunking (max 1200)...\n",
      "Long segment (1224 tokens) → chunking (max 1200)...\n",
      "Long segment (1651 tokens) → chunking (max 1200)...\n",
      "Long segment (2157 tokens) → chunking (max 1200)...\n",
      "Long segment (1330 tokens) → chunking (max 1200)...\n",
      "Long segment (2102 tokens) → chunking (max 1200)...\n",
      "Long segment (2062 tokens) → chunking (max 1200)...\n",
      "Long segment (1451 tokens) → chunking (max 1200)...\n",
      "Long segment (1507 tokens) → chunking (max 1200)...\n",
      "Long segment (1671 tokens) → chunking (max 1200)...\n",
      "Long segment (1926 tokens) → chunking (max 1200)...\n",
      "Long segment (2087 tokens) → chunking (max 1200)...\n",
      "Long segment (2108 tokens) → chunking (max 1200)...\n",
      "Long segment (1774 tokens) → chunking (max 1200)...\n",
      "Long segment (1499 tokens) → chunking (max 1200)...\n",
      "Long segment (1434 tokens) → chunking (max 1200)...\n",
      "Long segment (2372 tokens) → chunking (max 1200)...\n",
      "Long segment (1655 tokens) → chunking (max 1200)...\n",
      "Long segment (5305 tokens) → chunking (max 1200)...\n",
      "Long segment (2025 tokens) → chunking (max 1200)...\n",
      "Long segment (3063 tokens) → chunking (max 1200)...\n",
      "Long segment (1655 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  11%|█▏        | 57/500 [00:42<09:29,  1.29s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1330 tokens) → chunking (max 1200)...\n",
      "Long segment (1862 tokens) → chunking (max 1200)...\n",
      "Long segment (1859 tokens) → chunking (max 1200)...\n",
      "Long segment (1703 tokens) → chunking (max 1200)...\n",
      "Long segment (2899 tokens) → chunking (max 1200)...\n",
      "Long segment (1537 tokens) → chunking (max 1200)...\n",
      "Long segment (1860 tokens) → chunking (max 1200)...\n",
      "Long segment (2844 tokens) → chunking (max 1200)...\n",
      "Long segment (1872 tokens) → chunking (max 1200)...\n",
      "Long segment (1309 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  40%|████      | 202/500 [00:45<00:12, 23.63doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1418 tokens) → chunking (max 1200)...\n",
      "Long segment (1690 tokens) → chunking (max 1200)...\n",
      "Long segment (1322 tokens) → chunking (max 1200)...\n",
      "Long segment (1381 tokens) → chunking (max 1200)...\n",
      "Long segment (1715 tokens) → chunking (max 1200)...\n",
      "Long segment (1622 tokens) → chunking (max 1200)...\n",
      "Long segment (1824 tokens) → chunking (max 1200)...\n",
      "Long segment (1741 tokens) → chunking (max 1200)...\n",
      "Long segment (1395 tokens) → chunking (max 1200)...\n",
      "Long segment (1660 tokens) → chunking (max 1200)...\n",
      "Long segment (2765 tokens) → chunking (max 1200)...\n",
      "Long segment (1347 tokens) → chunking (max 1200)...\n",
      "Long segment (1909 tokens) → chunking (max 1200)...\n",
      "Long segment (2391 tokens) → chunking (max 1200)...\n",
      "Long segment (1700 tokens) → chunking (max 1200)...\n",
      "Long segment (3451 tokens) → chunking (max 1200)...\n",
      "Long segment (1840 tokens) → chunking (max 1200)...\n",
      "Long segment (1621 tokens) → chunking (max 1200)...\n",
      "Long segment (1515 tokens) → chunking (max 1200)...\n",
      "Long segment (1508 tokens) → chunking (max 1200)...\n",
      "Long segment (1371 tokens) → chunking (max 1200)...\n",
      "Long segment (2189 tokens) → chunking (max 1200)...\n",
      "Long segment (2401 tokens) → chunking (max 1200)...\n",
      "Long segment (1531 tokens) → chunking (max 1200)...\n",
      "Long segment (1298 tokens) → chunking (max 1200)...\n",
      "Long segment (2047 tokens) → chunking (max 1200)...\n",
      "Long segment (4137 tokens) → chunking (max 1200)...\n",
      "Long segment (1493 tokens) → chunking (max 1200)...\n",
      "Long segment (1487 tokens) → chunking (max 1200)...\n",
      "Long segment (1239 tokens) → chunking (max 1200)...\n",
      "Long segment (1715 tokens) → chunking (max 1200)...\n",
      "Long segment (1680 tokens) → chunking (max 1200)...\n",
      "Long segment (2380 tokens) → chunking (max 1200)...\n",
      "Long segment (1679 tokens) → chunking (max 1200)...\n",
      "Long segment (3207 tokens) → chunking (max 1200)...\n",
      "Long segment (1774 tokens) → chunking (max 1200)...\n",
      "Long segment (1355 tokens) → chunking (max 1200)...\n",
      "Long segment (2063 tokens) → chunking (max 1200)...\n",
      "Long segment (1884 tokens) → chunking (max 1200)...\n",
      "Long segment (1254 tokens) → chunking (max 1200)...\n",
      "Long segment (2237 tokens) → chunking (max 1200)...\n",
      "Long segment (1783 tokens) → chunking (max 1200)...\n",
      "Long segment (1360 tokens) → chunking (max 1200)...\n",
      "Long segment (2129 tokens) → chunking (max 1200)...\n",
      "Long segment (1958 tokens) → chunking (max 1200)...\n",
      "Long segment (1324 tokens) → chunking (max 1200)...\n",
      "Long segment (1508 tokens) → chunking (max 1200)...\n",
      "Long segment (2592 tokens) → chunking (max 1200)...\n",
      "Long segment (1677 tokens) → chunking (max 1200)...\n",
      "Long segment (2657 tokens) → chunking (max 1200)...\n",
      "Long segment (1311 tokens) → chunking (max 1200)...\n",
      "Long segment (2202 tokens) → chunking (max 1200)...\n",
      "Long segment (2386 tokens) → chunking (max 1200)...\n",
      "Long segment (3070 tokens) → chunking (max 1200)...\n",
      "Long segment (1315 tokens) → chunking (max 1200)...\n",
      "Long segment (1266 tokens) → chunking (max 1200)...\n",
      "Long segment (1333 tokens) → chunking (max 1200)...\n",
      "Long segment (2062 tokens) → chunking (max 1200)...\n",
      "Long segment (1310 tokens) → chunking (max 1200)...\n",
      "Long segment (1828 tokens) → chunking (max 1200)...\n",
      "Long segment (1274 tokens) → chunking (max 1200)...\n",
      "Long segment (1251 tokens) → chunking (max 1200)...\n",
      "Long segment (1886 tokens) → chunking (max 1200)...\n",
      "Long segment (1481 tokens) → chunking (max 1200)...\n",
      "Long segment (1327 tokens) → chunking (max 1200)...\n",
      "Long segment (1871 tokens) → chunking (max 1200)...\n",
      "Long segment (1201 tokens) → chunking (max 1200)...\n",
      "Long segment (1546 tokens) → chunking (max 1200)...\n",
      "Long segment (3291 tokens) → chunking (max 1200)...\n",
      "Long segment (1581 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  45%|████▌     | 226/500 [00:50<00:22, 12.30doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1407 tokens) → chunking (max 1200)...\n",
      "Long segment (1430 tokens) → chunking (max 1200)...\n",
      "Long segment (1203 tokens) → chunking (max 1200)...\n",
      "Long segment (1415 tokens) → chunking (max 1200)...\n",
      "Long segment (1430 tokens) → chunking (max 1200)...\n",
      "Long segment (5303 tokens) → chunking (max 1200)...\n",
      "Long segment (6521 tokens) → chunking (max 1200)...\n",
      "Long segment (2968 tokens) → chunking (max 1200)...\n",
      "Long segment (1554 tokens) → chunking (max 1200)...\n",
      "Long segment (1201 tokens) → chunking (max 1200)...\n",
      "Long segment (1392 tokens) → chunking (max 1200)...\n",
      "Long segment (1401 tokens) → chunking (max 1200)...\n",
      "Long segment (1377 tokens) → chunking (max 1200)...\n",
      "Long segment (5254 tokens) → chunking (max 1200)...\n",
      "Long segment (1259 tokens) → chunking (max 1200)...\n",
      "Long segment (1290 tokens) → chunking (max 1200)...\n",
      "Long segment (1833 tokens) → chunking (max 1200)...\n",
      "Long segment (1569 tokens) → chunking (max 1200)...\n",
      "Long segment (1637 tokens) → chunking (max 1200)...\n",
      "Long segment (1725 tokens) → chunking (max 1200)...\n",
      "Long segment (2980 tokens) → chunking (max 1200)...\n",
      "Long segment (1731 tokens) → chunking (max 1200)...\n",
      "Long segment (1307 tokens) → chunking (max 1200)...\n",
      "Long segment (2000 tokens) → chunking (max 1200)...\n",
      "Long segment (1911 tokens) → chunking (max 1200)...\n",
      "Long segment (1504 tokens) → chunking (max 1200)...\n",
      "Long segment (2283 tokens) → chunking (max 1200)...\n",
      "Long segment (1229 tokens) → chunking (max 1200)...\n",
      "Long segment (1468 tokens) → chunking (max 1200)...\n",
      "Long segment (1511 tokens) → chunking (max 1200)...\n",
      "Long segment (2223 tokens) → chunking (max 1200)...\n",
      "Long segment (2126 tokens) → chunking (max 1200)...\n",
      "Long segment (1942 tokens) → chunking (max 1200)...\n",
      "Long segment (1458 tokens) → chunking (max 1200)...\n",
      "Long segment (1761 tokens) → chunking (max 1200)...\n",
      "Long segment (3202 tokens) → chunking (max 1200)...\n",
      "Long segment (1312 tokens) → chunking (max 1200)...\n",
      "Long segment (1568 tokens) → chunking (max 1200)...\n",
      "Long segment (2388 tokens) → chunking (max 1200)...\n",
      "Long segment (1576 tokens) → chunking (max 1200)...\n",
      "Long segment (1656 tokens) → chunking (max 1200)...\n",
      "Long segment (1661 tokens) → chunking (max 1200)...\n",
      "Long segment (1450 tokens) → chunking (max 1200)...\n",
      "Long segment (1820 tokens) → chunking (max 1200)...\n",
      "Long segment (1725 tokens) → chunking (max 1200)...\n",
      "Long segment (1317 tokens) → chunking (max 1200)...\n",
      "Long segment (1448 tokens) → chunking (max 1200)...\n",
      "Long segment (1427 tokens) → chunking (max 1200)...\n",
      "Long segment (1452 tokens) → chunking (max 1200)...\n",
      "Long segment (1415 tokens) → chunking (max 1200)...\n",
      "Long segment (1278 tokens) → chunking (max 1200)...\n",
      "Long segment (2157 tokens) → chunking (max 1200)...\n",
      "Long segment (2202 tokens) → chunking (max 1200)...\n",
      "Long segment (1688 tokens) → chunking (max 1200)...\n",
      "Long segment (1453 tokens) → chunking (max 1200)...\n",
      "Long segment (1521 tokens) → chunking (max 1200)...\n",
      "Long segment (3090 tokens) → chunking (max 1200)...\n",
      "Long segment (1201 tokens) → chunking (max 1200)...\n",
      "Long segment (1919 tokens) → chunking (max 1200)...\n",
      "Long segment (2218 tokens) → chunking (max 1200)...\n",
      "Long segment (1991 tokens) → chunking (max 1200)...\n",
      "Long segment (1269 tokens) → chunking (max 1200)...\n",
      "Long segment (1458 tokens) → chunking (max 1200)...\n",
      "Long segment (1662 tokens) → chunking (max 1200)...\n",
      "Long segment (2062 tokens) → chunking (max 1200)...\n",
      "Long segment (1384 tokens) → chunking (max 1200)...\n",
      "Long segment (1685 tokens) → chunking (max 1200)...\n",
      "Long segment (1783 tokens) → chunking (max 1200)...\n",
      "Long segment (1346 tokens) → chunking (max 1200)...\n",
      "Long segment (1445 tokens) → chunking (max 1200)...\n",
      "Long segment (1432 tokens) → chunking (max 1200)...\n",
      "Long segment (1939 tokens) → chunking (max 1200)...\n",
      "Long segment (2417 tokens) → chunking (max 1200)...\n",
      "Long segment (1341 tokens) → chunking (max 1200)...\n",
      "Long segment (1442 tokens) → chunking (max 1200)...\n",
      "Long segment (1236 tokens) → chunking (max 1200)...\n",
      "Long segment (1362 tokens) → chunking (max 1200)...\n",
      "Long segment (2062 tokens) → chunking (max 1200)...\n",
      "Long segment (1376 tokens) → chunking (max 1200)...\n",
      "Long segment (1891 tokens) → chunking (max 1200)...\n",
      "Long segment (1300 tokens) → chunking (max 1200)...\n",
      "Long segment (2172 tokens) → chunking (max 1200)...\n",
      "Long segment (2966 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  49%|████▊     | 243/500 [00:53<00:26,  9.53doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1751 tokens) → chunking (max 1200)...\n",
      "Long segment (1537 tokens) → chunking (max 1200)...\n",
      "Long segment (1553 tokens) → chunking (max 1200)...\n",
      "Long segment (1229 tokens) → chunking (max 1200)...\n",
      "Long segment (1467 tokens) → chunking (max 1200)...\n",
      "Long segment (1561 tokens) → chunking (max 1200)...\n",
      "Long segment (1224 tokens) → chunking (max 1200)...\n",
      "Long segment (1503 tokens) → chunking (max 1200)...\n",
      "Long segment (1437 tokens) → chunking (max 1200)...\n",
      "Long segment (1536 tokens) → chunking (max 1200)...\n",
      "Long segment (1415 tokens) → chunking (max 1200)...\n",
      "Long segment (1733 tokens) → chunking (max 1200)...\n",
      "Long segment (1970 tokens) → chunking (max 1200)...\n",
      "Long segment (2045 tokens) → chunking (max 1200)...\n",
      "Long segment (1308 tokens) → chunking (max 1200)...\n",
      "Long segment (1312 tokens) → chunking (max 1200)...\n",
      "Long segment (1450 tokens) → chunking (max 1200)...\n",
      "Long segment (1717 tokens) → chunking (max 1200)...\n",
      "Long segment (1279 tokens) → chunking (max 1200)...\n",
      "Long segment (1535 tokens) → chunking (max 1200)...\n",
      "Long segment (1549 tokens) → chunking (max 1200)...\n",
      "Long segment (2190 tokens) → chunking (max 1200)...\n",
      "Long segment (2936 tokens) → chunking (max 1200)...\n",
      "Long segment (2188 tokens) → chunking (max 1200)...\n",
      "Long segment (2261 tokens) → chunking (max 1200)...\n",
      "Long segment (1224 tokens) → chunking (max 1200)...\n",
      "Long segment (2017 tokens) → chunking (max 1200)...\n",
      "Long segment (1648 tokens) → chunking (max 1200)...\n",
      "Long segment (1641 tokens) → chunking (max 1200)...\n",
      "Long segment (2954 tokens) → chunking (max 1200)...\n",
      "Long segment (1332 tokens) → chunking (max 1200)...\n",
      "Long segment (1789 tokens) → chunking (max 1200)...\n",
      "Long segment (2002 tokens) → chunking (max 1200)...\n",
      "Long segment (1316 tokens) → chunking (max 1200)...\n",
      "Long segment (1465 tokens) → chunking (max 1200)...\n",
      "Long segment (2571 tokens) → chunking (max 1200)...\n",
      "Long segment (2467 tokens) → chunking (max 1200)...\n",
      "Long segment (1554 tokens) → chunking (max 1200)...\n",
      "Long segment (1786 tokens) → chunking (max 1200)...\n",
      "Long segment (1270 tokens) → chunking (max 1200)...\n",
      "Long segment (1242 tokens) → chunking (max 1200)...\n",
      "Long segment (1596 tokens) → chunking (max 1200)...\n",
      "Long segment (1292 tokens) → chunking (max 1200)...\n",
      "Long segment (2270 tokens) → chunking (max 1200)...\n",
      "Long segment (1494 tokens) → chunking (max 1200)...\n",
      "Long segment (1787 tokens) → chunking (max 1200)...\n",
      "Long segment (1825 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  51%|█████     | 255/500 [00:57<00:32,  7.58doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1930 tokens) → chunking (max 1200)...\n",
      "Long segment (1519 tokens) → chunking (max 1200)...\n",
      "Long segment (1420 tokens) → chunking (max 1200)...\n",
      "Long segment (2375 tokens) → chunking (max 1200)...\n",
      "Long segment (2531 tokens) → chunking (max 1200)...\n",
      "Long segment (1429 tokens) → chunking (max 1200)...\n",
      "Long segment (1772 tokens) → chunking (max 1200)...\n",
      "Long segment (1691 tokens) → chunking (max 1200)...\n",
      "Long segment (1764 tokens) → chunking (max 1200)...\n",
      "Long segment (2386 tokens) → chunking (max 1200)...\n",
      "Long segment (2264 tokens) → chunking (max 1200)...\n",
      "Long segment (1280 tokens) → chunking (max 1200)...\n",
      "Long segment (3245 tokens) → chunking (max 1200)...\n",
      "Long segment (1841 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  53%|█████▎    | 264/500 [00:58<00:31,  7.47doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1270 tokens) → chunking (max 1200)...Long segment (1768 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1624 tokens) → chunking (max 1200)...\n",
      "Long segment (1326 tokens) → chunking (max 1200)...\n",
      "Long segment (1671 tokens) → chunking (max 1200)...\n",
      "Long segment (2477 tokens) → chunking (max 1200)...\n",
      "Long segment (1365 tokens) → chunking (max 1200)...\n",
      "Long segment (1306 tokens) → chunking (max 1200)...\n",
      "Long segment (2273 tokens) → chunking (max 1200)...\n",
      "Long segment (1709 tokens) → chunking (max 1200)...\n",
      "Long segment (1262 tokens) → chunking (max 1200)...\n",
      "Long segment (1478 tokens) → chunking (max 1200)...\n",
      "Long segment (2479 tokens) → chunking (max 1200)...\n",
      "Long segment (1987 tokens) → chunking (max 1200)...\n",
      "Long segment (2263 tokens) → chunking (max 1200)...\n",
      "Long segment (1983 tokens) → chunking (max 1200)...\n",
      "Long segment (1631 tokens) → chunking (max 1200)...\n",
      "Long segment (2456 tokens) → chunking (max 1200)...\n",
      "Long segment (1304 tokens) → chunking (max 1200)...\n",
      "Long segment (1372 tokens) → chunking (max 1200)...\n",
      "Long segment (2433 tokens) → chunking (max 1200)...\n",
      "Long segment (1328 tokens) → chunking (max 1200)...\n",
      "Long segment (1446 tokens) → chunking (max 1200)...\n",
      "Long segment (1370 tokens) → chunking (max 1200)...\n",
      "Long segment (1446 tokens) → chunking (max 1200)...\n",
      "Long segment (1288 tokens) → chunking (max 1200)...\n",
      "Long segment (1256 tokens) → chunking (max 1200)...\n",
      "Long segment (1549 tokens) → chunking (max 1200)...\n",
      "Long segment (1535 tokens) → chunking (max 1200)...\n",
      "Long segment (1864 tokens) → chunking (max 1200)...\n",
      "Long segment (1458 tokens) → chunking (max 1200)...\n",
      "Long segment (1504 tokens) → chunking (max 1200)...\n",
      "Long segment (2688 tokens) → chunking (max 1200)...\n",
      "Long segment (2249 tokens) → chunking (max 1200)...\n",
      "Long segment (2932 tokens) → chunking (max 1200)...\n",
      "Long segment (1394 tokens) → chunking (max 1200)...\n",
      "Long segment (1465 tokens) → chunking (max 1200)...\n",
      "Long segment (2375 tokens) → chunking (max 1200)...\n",
      "Long segment (2019 tokens) → chunking (max 1200)...\n",
      "Long segment (1821 tokens) → chunking (max 1200)...\n",
      "Long segment (1766 tokens) → chunking (max 1200)...\n",
      "Long segment (1550 tokens) → chunking (max 1200)...\n",
      "Long segment (1307 tokens) → chunking (max 1200)...\n",
      "Long segment (1210 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  54%|█████▍    | 271/500 [01:01<00:40,  5.68doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1800 tokens) → chunking (max 1200)...\n",
      "Long segment (2458 tokens) → chunking (max 1200)...\n",
      "Long segment (2032 tokens) → chunking (max 1200)...\n",
      "Long segment (4295 tokens) → chunking (max 1200)...\n",
      "Long segment (1389 tokens) → chunking (max 1200)...\n",
      "Long segment (2109 tokens) → chunking (max 1200)...\n",
      "Long segment (2222 tokens) → chunking (max 1200)...\n",
      "Long segment (1519 tokens) → chunking (max 1200)...\n",
      "Long segment (2802 tokens) → chunking (max 1200)...\n",
      "Long segment (1864 tokens) → chunking (max 1200)...\n",
      "Long segment (1455 tokens) → chunking (max 1200)...\n",
      "Long segment (1786 tokens) → chunking (max 1200)...\n",
      "Long segment (1235 tokens) → chunking (max 1200)...\n",
      "Long segment (2181 tokens) → chunking (max 1200)...\n",
      "Long segment (1677 tokens) → chunking (max 1200)...\n",
      "Long segment (2605 tokens) → chunking (max 1200)...\n",
      "Long segment (2058 tokens) → chunking (max 1200)...\n",
      "Long segment (2008 tokens) → chunking (max 1200)...\n",
      "Long segment (1988 tokens) → chunking (max 1200)...\n",
      "Long segment (2770 tokens) → chunking (max 1200)...\n",
      "Long segment (2132 tokens) → chunking (max 1200)...\n",
      "Long segment (1297 tokens) → chunking (max 1200)...\n",
      "Long segment (1218 tokens) → chunking (max 1200)...\n",
      "Long segment (1365 tokens) → chunking (max 1200)...\n",
      "Long segment (1431 tokens) → chunking (max 1200)...\n",
      "Long segment (2538 tokens) → chunking (max 1200)...\n",
      "Long segment (1241 tokens) → chunking (max 1200)...\n",
      "Long segment (3379 tokens) → chunking (max 1200)...\n",
      "Long segment (1238 tokens) → chunking (max 1200)...\n",
      "Long segment (1425 tokens) → chunking (max 1200)...\n",
      "Long segment (2544 tokens) → chunking (max 1200)...\n",
      "Long segment (1313 tokens) → chunking (max 1200)...\n",
      "Long segment (4417 tokens) → chunking (max 1200)...\n",
      "Long segment (1951 tokens) → chunking (max 1200)...\n",
      "Long segment (1858 tokens) → chunking (max 1200)...\n",
      "Long segment (1423 tokens) → chunking (max 1200)...\n",
      "Long segment (1385 tokens) → chunking (max 1200)...\n",
      "Long segment (1695 tokens) → chunking (max 1200)...\n",
      "Long segment (2468 tokens) → chunking (max 1200)...\n",
      "Long segment (2102 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  56%|█████▌    | 280/500 [01:03<00:42,  5.22doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2370 tokens) → chunking (max 1200)...Long segment (1886 tokens) → chunking (max 1200)...\n",
      "Long segment (2189 tokens) → chunking (max 1200)...\n",
      "Long segment (3128 tokens) → chunking (max 1200)...\n",
      "Long segment (2083 tokens) → chunking (max 1200)...\n",
      "Long segment (1280 tokens) → chunking (max 1200)...\n",
      "Long segment (2263 tokens) → chunking (max 1200)...\n",
      "Long segment (1420 tokens) → chunking (max 1200)...\n",
      "Long segment (1698 tokens) → chunking (max 1200)...\n",
      "Long segment (2431 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1210 tokens) → chunking (max 1200)...\n",
      "Long segment (1356 tokens) → chunking (max 1200)...\n",
      "Long segment (1891 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  57%|█████▋    | 283/500 [01:04<00:43,  4.98doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2400 tokens) → chunking (max 1200)...\n",
      "Long segment (2297 tokens) → chunking (max 1200)...\n",
      "Long segment (1318 tokens) → chunking (max 1200)...\n",
      "Long segment (3669 tokens) → chunking (max 1200)...\n",
      "Long segment (1771 tokens) → chunking (max 1200)...\n",
      "Long segment (1941 tokens) → chunking (max 1200)...\n",
      "Long segment (1250 tokens) → chunking (max 1200)...\n",
      "Long segment (1690 tokens) → chunking (max 1200)...\n",
      "Long segment (1877 tokens) → chunking (max 1200)...\n",
      "Long segment (1207 tokens) → chunking (max 1200)...\n",
      "Long segment (1310 tokens) → chunking (max 1200)...\n",
      "Long segment (1430 tokens) → chunking (max 1200)...\n",
      "Long segment (1878 tokens) → chunking (max 1200)...\n",
      "Long segment (1753 tokens) → chunking (max 1200)...\n",
      "Long segment (1666 tokens) → chunking (max 1200)...\n",
      "Long segment (2184 tokens) → chunking (max 1200)...\n",
      "Long segment (2745 tokens) → chunking (max 1200)...\n",
      "Long segment (2712 tokens) → chunking (max 1200)...\n",
      "Long segment (1438 tokens) → chunking (max 1200)...\n",
      "Long segment (1333 tokens) → chunking (max 1200)...\n",
      "Long segment (3200 tokens) → chunking (max 1200)...\n",
      "Long segment (2419 tokens) → chunking (max 1200)...\n",
      "Long segment (1521 tokens) → chunking (max 1200)...\n",
      "Long segment (2352 tokens) → chunking (max 1200)...\n",
      "Long segment (2266 tokens) → chunking (max 1200)...\n",
      "Long segment (1563 tokens) → chunking (max 1200)...\n",
      "Long segment (1820 tokens) → chunking (max 1200)...\n",
      "Long segment (1258 tokens) → chunking (max 1200)...\n",
      "Long segment (1439 tokens) → chunking (max 1200)...\n",
      "Long segment (1732 tokens) → chunking (max 1200)...\n",
      "Long segment (2263 tokens) → chunking (max 1200)...\n",
      "Long segment (2344 tokens) → chunking (max 1200)...\n",
      "Long segment (1692 tokens) → chunking (max 1200)...\n",
      "Long segment (1545 tokens) → chunking (max 1200)...\n",
      "Long segment (1256 tokens) → chunking (max 1200)...\n",
      "Long segment (1649 tokens) → chunking (max 1200)...\n",
      "Long segment (2381 tokens) → chunking (max 1200)...\n",
      "Long segment (1330 tokens) → chunking (max 1200)...\n",
      "Long segment (1267 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  57%|█████▋    | 287/500 [01:08<01:13,  2.88doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1268 tokens) → chunking (max 1200)...\n",
      "Long segment (1263 tokens) → chunking (max 1200)...\n",
      "Long segment (1611 tokens) → chunking (max 1200)...\n",
      "Long segment (1387 tokens) → chunking (max 1200)...\n",
      "Long segment (1494 tokens) → chunking (max 1200)...\n",
      "Long segment (1311 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  58%|█████▊    | 289/500 [01:09<01:17,  2.73doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1391 tokens) → chunking (max 1200)...\n",
      "Long segment (1539 tokens) → chunking (max 1200)...\n",
      "Long segment (1205 tokens) → chunking (max 1200)...\n",
      "Long segment (1307 tokens) → chunking (max 1200)...\n",
      "Long segment (1232 tokens) → chunking (max 1200)...\n",
      "Long segment (1243 tokens) → chunking (max 1200)...\n",
      "Long segment (2854 tokens) → chunking (max 1200)...\n",
      "Long segment (1479 tokens) → chunking (max 1200)...\n",
      "Long segment (1902 tokens) → chunking (max 1200)...\n",
      "Long segment (1667 tokens) → chunking (max 1200)...\n",
      "Long segment (2585 tokens) → chunking (max 1200)...\n",
      "Long segment (1657 tokens) → chunking (max 1200)...\n",
      "Long segment (2999 tokens) → chunking (max 1200)...\n",
      "Long segment (1360 tokens) → chunking (max 1200)...\n",
      "Long segment (2138 tokens) → chunking (max 1200)...\n",
      "Long segment (1964 tokens) → chunking (max 1200)...\n",
      "Long segment (1227 tokens) → chunking (max 1200)...\n",
      "Long segment (1733 tokens) → chunking (max 1200)...\n",
      "Long segment (1322 tokens) → chunking (max 1200)...\n",
      "Long segment (2243 tokens) → chunking (max 1200)...\n",
      "Long segment (1460 tokens) → chunking (max 1200)...\n",
      "Long segment (1233 tokens) → chunking (max 1200)...\n",
      "Long segment (1246 tokens) → chunking (max 1200)...\n",
      "Long segment (1289 tokens) → chunking (max 1200)...\n",
      "Long segment (1634 tokens) → chunking (max 1200)...\n",
      "Long segment (1311 tokens) → chunking (max 1200)...\n",
      "Long segment (1837 tokens) → chunking (max 1200)...\n",
      "Long segment (1246 tokens) → chunking (max 1200)...\n",
      "Long segment (1202 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  58%|█████▊    | 292/500 [01:14<02:24,  1.44doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1936 tokens) → chunking (max 1200)...\n",
      "Long segment (1600 tokens) → chunking (max 1200)...\n",
      "Long segment (1521 tokens) → chunking (max 1200)...\n",
      "Long segment (1563 tokens) → chunking (max 1200)...\n",
      "Long segment (1236 tokens) → chunking (max 1200)...\n",
      "Long segment (1381 tokens) → chunking (max 1200)...\n",
      "Long segment (1531 tokens) → chunking (max 1200)...\n",
      "Long segment (2518 tokens) → chunking (max 1200)...\n",
      "Long segment (1761 tokens) → chunking (max 1200)...\n",
      "Long segment (1727 tokens) → chunking (max 1200)...\n",
      "Long segment (2107 tokens) → chunking (max 1200)...\n",
      "Long segment (1789 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  59%|█████▊    | 293/500 [01:15<02:29,  1.39doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1659 tokens) → chunking (max 1200)...\n",
      "Long segment (1251 tokens) → chunking (max 1200)...\n",
      "Long segment (1213 tokens) → chunking (max 1200)...\n",
      "Long segment (1801 tokens) → chunking (max 1200)...\n",
      "Long segment (1333 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  59%|█████▉    | 295/500 [01:16<02:23,  1.43doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2237 tokens) → chunking (max 1200)...\n",
      "Long segment (1492 tokens) → chunking (max 1200)...\n",
      "Long segment (2385 tokens) → chunking (max 1200)...\n",
      "Long segment (1618 tokens) → chunking (max 1200)...\n",
      "Long segment (1259 tokens) → chunking (max 1200)...\n",
      "Long segment (1613 tokens) → chunking (max 1200)...\n",
      "Long segment (2525 tokens) → chunking (max 1200)...\n",
      "Long segment (1347 tokens) → chunking (max 1200)...Long segment (1593 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  59%|█████▉    | 296/500 [01:18<02:39,  1.28doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1511 tokens) → chunking (max 1200)...\n",
      "Long segment (1564 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1206 tokens) → chunking (max 1200)...\n",
      "Long segment (1311 tokens) → chunking (max 1200)...\n",
      "Long segment (1395 tokens) → chunking (max 1200)...\n",
      "Long segment (1219 tokens) → chunking (max 1200)...\n",
      "Long segment (2242 tokens) → chunking (max 1200)...\n",
      "Long segment (1334 tokens) → chunking (max 1200)...\n",
      "Long segment (1392 tokens) → chunking (max 1200)...\n",
      "Long segment (1282 tokens) → chunking (max 1200)...\n",
      "Long segment (2125 tokens) → chunking (max 1200)...\n",
      "Long segment (1282 tokens) → chunking (max 1200)...\n",
      "Long segment (1589 tokens) → chunking (max 1200)...\n",
      "Long segment (2198 tokens) → chunking (max 1200)...\n",
      "Long segment (1204 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  60%|█████▉    | 299/500 [01:21<03:41,  1.10s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1357 tokens) → chunking (max 1200)...\n",
      "Long segment (1301 tokens) → chunking (max 1200)...\n",
      "Long segment (1575 tokens) → chunking (max 1200)...\n",
      "Long segment (1573 tokens) → chunking (max 1200)...\n",
      "Long segment (2022 tokens) → chunking (max 1200)...\n",
      "Long segment (1243 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  60%|██████    | 300/500 [01:23<03:45,  1.13s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (2270 tokens) → chunking (max 1200)...\n",
      "Long segment (1213 tokens) → chunking (max 1200)...\n",
      "Long segment (2248 tokens) → chunking (max 1200)...\n",
      "Long segment (1870 tokens) → chunking (max 1200)...\n",
      "Long segment (1940 tokens) → chunking (max 1200)...\n",
      "Long segment (2471 tokens) → chunking (max 1200)...\n",
      "Long segment (1757 tokens) → chunking (max 1200)...\n",
      "Long segment (1226 tokens) → chunking (max 1200)...\n",
      "Long segment (1452 tokens) → chunking (max 1200)...\n",
      "Long segment (1340 tokens) → chunking (max 1200)...\n",
      "Long segment (1675 tokens) → chunking (max 1200)...\n",
      "Long segment (1243 tokens) → chunking (max 1200)...\n",
      "Long segment (1375 tokens) → chunking (max 1200)...\n",
      "Long segment (1694 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  60%|██████    | 302/500 [01:26<04:44,  1.44s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1206 tokens) → chunking (max 1200)...\n",
      "Long segment (1282 tokens) → chunking (max 1200)...\n",
      "Long segment (1696 tokens) → chunking (max 1200)...\n",
      "Long segment (1389 tokens) → chunking (max 1200)...\n",
      "Long segment (1240 tokens) → chunking (max 1200)...\n",
      "Long segment (1318 tokens) → chunking (max 1200)...\n",
      "Long segment (1661 tokens) → chunking (max 1200)...\n",
      "Long segment (1302 tokens) → chunking (max 1200)...\n",
      "Long segment (1972 tokens) → chunking (max 1200)...\n",
      "Long segment (1405 tokens) → chunking (max 1200)...\n",
      "Long segment (2110 tokens) → chunking (max 1200)...\n",
      "Long segment (1668 tokens) → chunking (max 1200)...\n",
      "Long segment (1263 tokens) → chunking (max 1200)...\n",
      "Long segment (1467 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  61%|██████    | 303/500 [01:27<04:40,  1.43s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1224 tokens) → chunking (max 1200)...\n",
      "Long segment (1249 tokens) → chunking (max 1200)...\n",
      "Long segment (1601 tokens) → chunking (max 1200)...\n",
      "Long segment (1389 tokens) → chunking (max 1200)...\n",
      "Long segment (1232 tokens) → chunking (max 1200)...\n",
      "Long segment (1245 tokens) → chunking (max 1200)...\n",
      "Long segment (3202 tokens) → chunking (max 1200)...\n",
      "Long segment (1511 tokens) → chunking (max 1200)...\n",
      "Long segment (1524 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  61%|██████    | 305/500 [01:31<05:47,  1.78s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1431 tokens) → chunking (max 1200)...\n",
      "Long segment (1201 tokens) → chunking (max 1200)...\n",
      "Long segment (1403 tokens) → chunking (max 1200)...\n",
      "Long segment (1227 tokens) → chunking (max 1200)...\n",
      "Long segment (1531 tokens) → chunking (max 1200)...\n",
      "Long segment (1378 tokens) → chunking (max 1200)...\n",
      "Long segment (2155 tokens) → chunking (max 1200)...\n",
      "Long segment (2818 tokens) → chunking (max 1200)...\n",
      "Long segment (1400 tokens) → chunking (max 1200)...\n",
      "Long segment (1345 tokens) → chunking (max 1200)...\n",
      "Long segment (1301 tokens) → chunking (max 1200)...\n",
      "Long segment (1800 tokens) → chunking (max 1200)...\n",
      "Long segment (1545 tokens) → chunking (max 1200)...\n",
      "Long segment (1562 tokens) → chunking (max 1200)...\n",
      "Long segment (1288 tokens) → chunking (max 1200)...\n",
      "Long segment (1348 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  61%|██████▏   | 307/500 [01:35<06:37,  2.06s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1808 tokens) → chunking (max 1200)...\n",
      "Long segment (1217 tokens) → chunking (max 1200)...\n",
      "Long segment (2051 tokens) → chunking (max 1200)...\n",
      "Long segment (2252 tokens) → chunking (max 1200)...\n",
      "Long segment (2135 tokens) → chunking (max 1200)...\n",
      "Long segment (1472 tokens) → chunking (max 1200)...\n",
      "Long segment (1625 tokens) → chunking (max 1200)...\n",
      "Long segment (1832 tokens) → chunking (max 1200)...\n",
      "Long segment (1205 tokens) → chunking (max 1200)...\n",
      "Long segment (1384 tokens) → chunking (max 1200)...\n",
      "Long segment (1824 tokens) → chunking (max 1200)...\n",
      "Long segment (1579 tokens) → chunking (max 1200)...\n",
      "Long segment (1223 tokens) → chunking (max 1200)...\n",
      "Long segment (1299 tokens) → chunking (max 1200)...\n",
      "Long segment (1517 tokens) → chunking (max 1200)...\n",
      "Long segment (1352 tokens) → chunking (max 1200)...\n",
      "Long segment (1951 tokens) → chunking (max 1200)...\n",
      "Long segment (1626 tokens) → chunking (max 1200)...\n",
      "Long segment (1248 tokens) → chunking (max 1200)...\n",
      "Long segment (1623 tokens) → chunking (max 1200)...\n",
      "Long segment (2596 tokens) → chunking (max 1200)...\n",
      "Long segment (1285 tokens) → chunking (max 1200)...\n",
      "Long segment (1881 tokens) → chunking (max 1200)...\n",
      "Long segment (1667 tokens) → chunking (max 1200)...\n",
      "Long segment (2293 tokens) → chunking (max 1200)...\n",
      "Long segment (1213 tokens) → chunking (max 1200)...\n",
      "Long segment (1532 tokens) → chunking (max 1200)...\n",
      "Long segment (2170 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (1293 tokens) → chunking (max 1200)...\n",
      "Long segment (1481 tokens) → chunking (max 1200)...\n",
      "Long segment (2088 tokens) → chunking (max 1200)...\n",
      "Long segment (1306 tokens) → chunking (max 1200)...\n",
      "Long segment (1657 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  62%|██████▏   | 308/500 [01:36<05:31,  1.72s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1242 tokens) → chunking (max 1200)...\n",
      "Long segment (1612 tokens) → chunking (max 1200)...\n",
      "Long segment (1269 tokens) → chunking (max 1200)...\n",
      "Long segment (1262 tokens) → chunking (max 1200)...\n",
      "Long segment (1840 tokens) → chunking (max 1200)...\n",
      "Long segment (2753 tokens) → chunking (max 1200)...\n",
      "Long segment (1340 tokens) → chunking (max 1200)...\n",
      "Long segment (1408 tokens) → chunking (max 1200)...\n",
      "Long segment (1243 tokens) → chunking (max 1200)...\n",
      "Long segment (1265 tokens) → chunking (max 1200)...\n",
      "Long segment (1315 tokens) → chunking (max 1200)...\n",
      "Long segment (1786 tokens) → chunking (max 1200)...\n",
      "Long segment (1545 tokens) → chunking (max 1200)...\n",
      "Long segment (1675 tokens) → chunking (max 1200)...\n",
      "Long segment (1314 tokens) → chunking (max 1200)...\n",
      "Long segment (1244 tokens) → chunking (max 1200)...\n",
      "Long segment (1313 tokens) → chunking (max 1200)...\n",
      "Long segment (1383 tokens) → chunking (max 1200)...\n",
      "Long segment (1777 tokens) → chunking (max 1200)...\n",
      "Long segment (1590 tokens) → chunking (max 1200)...\n",
      "Long segment (1326 tokens) → chunking (max 1200)...\n",
      "Long segment (1343 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  62%|██████▏   | 310/500 [01:40<05:36,  1.77s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1224 tokens) → chunking (max 1200)...\n",
      "Long segment (1767 tokens) → chunking (max 1200)...\n",
      "Long segment (1240 tokens) → chunking (max 1200)...\n",
      "Long segment (2010 tokens) → chunking (max 1200)...\n",
      "Long segment (1867 tokens) → chunking (max 1200)...\n",
      "Long segment (1659 tokens) → chunking (max 1200)...\n",
      "Long segment (1417 tokens) → chunking (max 1200)...\n",
      "Long segment (1618 tokens) → chunking (max 1200)...\n",
      "Long segment (1332 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  62%|██████▏   | 312/500 [01:42<04:45,  1.52s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1704 tokens) → chunking (max 1200)...Long segment (1388 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (3562 tokens) → chunking (max 1200)...\n",
      "Long segment (1741 tokens) → chunking (max 1200)...\n",
      "Long segment (1646 tokens) → chunking (max 1200)...\n",
      "Long segment (1585 tokens) → chunking (max 1200)...\n",
      "Long segment (1268 tokens) → chunking (max 1200)...\n",
      "Long segment (1521 tokens) → chunking (max 1200)...\n",
      "Long segment (2082 tokens) → chunking (max 1200)...\n",
      "Long segment (2333 tokens) → chunking (max 1200)...\n",
      "Long segment (1538 tokens) → chunking (max 1200)...\n",
      "Long segment (1239 tokens) → chunking (max 1200)...\n",
      "Long segment (1768 tokens) → chunking (max 1200)...\n",
      "Long segment (1548 tokens) → chunking (max 1200)...\n",
      "Long segment (1427 tokens) → chunking (max 1200)...\n",
      "Long segment (1742 tokens) → chunking (max 1200)...\n",
      "Long segment (1420 tokens) → chunking (max 1200)...\n",
      "Long segment (1375 tokens) → chunking (max 1200)...\n",
      "Long segment (1455 tokens) → chunking (max 1200)...\n",
      "Long segment (1534 tokens) → chunking (max 1200)...\n",
      "Long segment (1321 tokens) → chunking (max 1200)...\n",
      "Long segment (2602 tokens) → chunking (max 1200)...\n",
      "Long segment (1529 tokens) → chunking (max 1200)...\n",
      "Long segment (1478 tokens) → chunking (max 1200)...\n",
      "Long segment (1865 tokens) → chunking (max 1200)...\n",
      "Long segment (1302 tokens) → chunking (max 1200)...\n",
      "Long segment (1831 tokens) → chunking (max 1200)...\n",
      "Long segment (1248 tokens) → chunking (max 1200)...\n",
      "Long segment (1884 tokens) → chunking (max 1200)...\n",
      "Long segment (1822 tokens) → chunking (max 1200)...\n",
      "Long segment (1616 tokens) → chunking (max 1200)...\n",
      "Long segment (2885 tokens) → chunking (max 1200)...\n",
      "Long segment (1544 tokens) → chunking (max 1200)...\n",
      "Long segment (1427 tokens) → chunking (max 1200)...\n",
      "Long segment (1633 tokens) → chunking (max 1200)...\n",
      "Long segment (1334 tokens) → chunking (max 1200)...\n",
      "Long segment (1374 tokens) → chunking (max 1200)...\n",
      "Long segment (1549 tokens) → chunking (max 1200)...\n",
      "Long segment (1349 tokens) → chunking (max 1200)...\n",
      "Long segment (1684 tokens) → chunking (max 1200)...\n",
      "Long segment (4181 tokens) → chunking (max 1200)...Long segment (1250 tokens) → chunking (max 1200)...\n",
      "Long segment (1436 tokens) → chunking (max 1200)...\n",
      "Long segment (1792 tokens) → chunking (max 1200)...\n",
      "Long segment (2105 tokens) → chunking (max 1200)...\n",
      "Long segment (1412 tokens) → chunking (max 1200)...\n",
      "Long segment (1694 tokens) → chunking (max 1200)...\n",
      "Long segment (1374 tokens) → chunking (max 1200)...\n",
      "Long segment (1387 tokens) → chunking (max 1200)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  63%|██████▎   | 314/500 [01:46<05:06,  1.65s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1582 tokens) → chunking (max 1200)...\n",
      "Long segment (1932 tokens) → chunking (max 1200)...\n",
      "Long segment (1619 tokens) → chunking (max 1200)...\n",
      "Long segment (2245 tokens) → chunking (max 1200)...\n",
      "Long segment (2080 tokens) → chunking (max 1200)...\n",
      "Long segment (1263 tokens) → chunking (max 1200)...\n",
      "Long segment (2112 tokens) → chunking (max 1200)...\n",
      "Long segment (2048 tokens) → chunking (max 1200)...\n",
      "Long segment (1261 tokens) → chunking (max 1200)...\n",
      "Long segment (2260 tokens) → chunking (max 1200)...\n",
      "Long segment (1203 tokens) → chunking (max 1200)...\n",
      "Long segment (1571 tokens) → chunking (max 1200)...\n",
      "Long segment (2231 tokens) → chunking (max 1200)...\n",
      "Long segment (1352 tokens) → chunking (max 1200)...\n",
      "Long segment (1642 tokens) → chunking (max 1200)...\n",
      "Long segment (2329 tokens) → chunking (max 1200)...\n",
      "Long segment (1249 tokens) → chunking (max 1200)...\n",
      "Long segment (1683 tokens) → chunking (max 1200)...\n",
      "Long segment (1336 tokens) → chunking (max 1200)...\n",
      "Long segment (2228 tokens) → chunking (max 1200)...\n",
      "Long segment (1575 tokens) → chunking (max 1200)...\n",
      "Long segment (2098 tokens) → chunking (max 1200)...\n",
      "Long segment (2280 tokens) → chunking (max 1200)...\n",
      "Long segment (1289 tokens) → chunking (max 1200)...\n",
      "Long segment (1369 tokens) → chunking (max 1200)...\n",
      "Long segment (1532 tokens) → chunking (max 1200)...\n",
      "Long segment (1805 tokens) → chunking (max 1200)...\n",
      "Long segment (4520 tokens) → chunking (max 1200)...\n",
      "Long segment (1592 tokens) → chunking (max 1200)...\n",
      "Long segment (2517 tokens) → chunking (max 1200)...\n",
      "Long segment (1480 tokens) → chunking (max 1200)...\n",
      "Long segment (1264 tokens) → chunking (max 1200)...\n",
      "Long segment (1575 tokens) → chunking (max 1200)...\n",
      "Long segment (1668 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  63%|██████▎   | 315/500 [01:48<05:06,  1.66s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1652 tokens) → chunking (max 1200)...\n",
      "Long segment (1713 tokens) → chunking (max 1200)...\n",
      "Long segment (2660 tokens) → chunking (max 1200)...\n",
      "Long segment (2696 tokens) → chunking (max 1200)...\n",
      "Long segment (1394 tokens) → chunking (max 1200)...\n",
      "Long segment (1544 tokens) → chunking (max 1200)...\n",
      "Long segment (1757 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  63%|██████▎   | 316/500 [01:49<04:29,  1.47s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1202 tokens) → chunking (max 1200)...\n",
      "Long segment (1884 tokens) → chunking (max 1200)...\n",
      "Long segment (2092 tokens) → chunking (max 1200)...\n",
      "Long segment (1595 tokens) → chunking (max 1200)...\n",
      "Long segment (1944 tokens) → chunking (max 1200)...\n",
      "Long segment (1207 tokens) → chunking (max 1200)...\n",
      "Long segment (1476 tokens) → chunking (max 1200)...\n",
      "Long segment (1691 tokens) → chunking (max 1200)...\n",
      "Long segment (1931 tokens) → chunking (max 1200)...\n",
      "Long segment (1348 tokens) → chunking (max 1200)...\n",
      "Long segment (1653 tokens) → chunking (max 1200)...\n",
      "Long segment (1591 tokens) → chunking (max 1200)...\n",
      "Long segment (1338 tokens) → chunking (max 1200)...\n",
      "Long segment (1349 tokens) → chunking (max 1200)...\n",
      "Long segment (1622 tokens) → chunking (max 1200)...\n",
      "Long segment (1383 tokens) → chunking (max 1200)...\n",
      "Long segment (2487 tokens) → chunking (max 1200)...\n",
      "Long segment (1396 tokens) → chunking (max 1200)...\n",
      "Long segment (1831 tokens) → chunking (max 1200)...\n",
      "Long segment (1684 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  64%|██████▎   | 318/500 [01:53<05:32,  1.82s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1244 tokens) → chunking (max 1200)...\n",
      "Long segment (1505 tokens) → chunking (max 1200)...\n",
      "Long segment (1509 tokens) → chunking (max 1200)...\n",
      "Long segment (1436 tokens) → chunking (max 1200)...\n",
      "Long segment (2718 tokens) → chunking (max 1200)...\n",
      "Long segment (1254 tokens) → chunking (max 1200)...\n",
      "Long segment (1259 tokens) → chunking (max 1200)...\n",
      "Long segment (1435 tokens) → chunking (max 1200)...\n",
      "Long segment (1466 tokens) → chunking (max 1200)...\n",
      "Long segment (2081 tokens) → chunking (max 1200)...\n",
      "Long segment (1686 tokens) → chunking (max 1200)...\n",
      "Long segment (1882 tokens) → chunking (max 1200)...\n",
      "Long segment (1463 tokens) → chunking (max 1200)...\n",
      "Long segment (2000 tokens) → chunking (max 1200)...\n",
      "Long segment (2318 tokens) → chunking (max 1200)...\n",
      "Long segment (1597 tokens) → chunking (max 1200)...\n",
      "Long segment (1207 tokens) → chunking (max 1200)...\n",
      "Long segment (1574 tokens) → chunking (max 1200)...\n",
      "Long segment (1222 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  64%|██████▍   | 320/500 [01:55<03:59,  1.33s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1231 tokens) → chunking (max 1200)...\n",
      "Long segment (1237 tokens) → chunking (max 1200)...\n",
      "Long segment (1315 tokens) → chunking (max 1200)...\n",
      "Long segment (1519 tokens) → chunking (max 1200)...\n",
      "Long segment (1208 tokens) → chunking (max 1200)...\n",
      "Long segment (1335 tokens) → chunking (max 1200)...\n",
      "Long segment (1318 tokens) → chunking (max 1200)...\n",
      "Long segment (2537 tokens) → chunking (max 1200)...\n",
      "Long segment (1254 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  64%|██████▍   | 322/500 [01:56<02:45,  1.07doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1301 tokens) → chunking (max 1200)...\n",
      "Long segment (1610 tokens) → chunking (max 1200)...\n",
      "Long segment (1509 tokens) → chunking (max 1200)...\n",
      "Long segment (1917 tokens) → chunking (max 1200)...\n",
      "Long segment (1487 tokens) → chunking (max 1200)...\n",
      "Long segment (9127 tokens) → chunking (max 1200)...\n",
      "Long segment (1561 tokens) → chunking (max 1200)...\n",
      "Long segment (1927 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  65%|██████▍   | 324/500 [01:59<03:11,  1.09s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1323 tokens) → chunking (max 1200)...\n",
      "Long segment (1236 tokens) → chunking (max 1200)...\n",
      "Long segment (1378 tokens) → chunking (max 1200)...\n",
      "Long segment (1617 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  65%|██████▌   | 326/500 [02:01<02:52,  1.01doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1879 tokens) → chunking (max 1200)...\n",
      "Long segment (1374 tokens) → chunking (max 1200)...\n",
      "Long segment (3048 tokens) → chunking (max 1200)...\n",
      "Long segment (2972 tokens) → chunking (max 1200)...\n",
      "Long segment (1236 tokens) → chunking (max 1200)...\n",
      "Long segment (2327 tokens) → chunking (max 1200)...\n",
      "Long segment (1313 tokens) → chunking (max 1200)...\n",
      "Long segment (2161 tokens) → chunking (max 1200)...\n",
      "Long segment (1589 tokens) → chunking (max 1200)...\n",
      "Long segment (1321 tokens) → chunking (max 1200)...\n",
      "Long segment (1458 tokens) → chunking (max 1200)...\n",
      "Long segment (1880 tokens) → chunking (max 1200)...\n",
      "Long segment (1801 tokens) → chunking (max 1200)...\n",
      "Long segment (1738 tokens) → chunking (max 1200)...\n",
      "Long segment (1486 tokens) → chunking (max 1200)...\n",
      "Long segment (3683 tokens) → chunking (max 1200)...\n",
      "Long segment (2107 tokens) → chunking (max 1200)...\n",
      "Long segment (1758 tokens) → chunking (max 1200)...\n",
      "Long segment (1267 tokens) → chunking (max 1200)...\n",
      "Long segment (1437 tokens) → chunking (max 1200)...\n",
      "Long segment (1415 tokens) → chunking (max 1200)...\n",
      "Long segment (1256 tokens) → chunking (max 1200)...\n",
      "Long segment (1323 tokens) → chunking (max 1200)...\n",
      "Long segment (1645 tokens) → chunking (max 1200)...\n",
      "Long segment (1316 tokens) → chunking (max 1200)...\n",
      "Long segment (1202 tokens) → chunking (max 1200)...\n",
      "Long segment (3849 tokens) → chunking (max 1200)...Long segment (1796 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  65%|██████▌   | 327/500 [02:02<02:36,  1.11doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1574 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1930 tokens) → chunking (max 1200)...\n",
      "Long segment (1245 tokens) → chunking (max 1200)...\n",
      "Long segment (1434 tokens) → chunking (max 1200)...\n",
      "Long segment (1691 tokens) → chunking (max 1200)...\n",
      "Long segment (4116 tokens) → chunking (max 1200)...\n",
      "Long segment (1233 tokens) → chunking (max 1200)...\n",
      "Long segment (1783 tokens) → chunking (max 1200)...\n",
      "Long segment (1436 tokens) → chunking (max 1200)...\n",
      "Long segment (1594 tokens) → chunking (max 1200)...\n",
      "Long segment (1392 tokens) → chunking (max 1200)...\n",
      "Long segment (1828 tokens) → chunking (max 1200)...\n",
      "Long segment (4269 tokens) → chunking (max 1200)...\n",
      "Long segment (1205 tokens) → chunking (max 1200)...\n",
      "Long segment (1211 tokens) → chunking (max 1200)...\n",
      "Long segment (1248 tokens) → chunking (max 1200)...\n",
      "Long segment (1442 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  66%|██████▌   | 328/500 [02:04<04:32,  1.58s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1811 tokens) → chunking (max 1200)...Long segment (1574 tokens) → chunking (max 1200)...\n",
      "Long segment (1987 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1299 tokens) → chunking (max 1200)...\n",
      "Long segment (2106 tokens) → chunking (max 1200)...\n",
      "Long segment (1374 tokens) → chunking (max 1200)...\n",
      "Long segment (1248 tokens) → chunking (max 1200)...\n",
      "Long segment (1381 tokens) → chunking (max 1200)...\n",
      "Long segment (1451 tokens) → chunking (max 1200)...\n",
      "Long segment (2869 tokens) → chunking (max 1200)...\n",
      "Long segment (1209 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  66%|██████▌   | 330/500 [02:08<04:44,  1.67s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1378 tokens) → chunking (max 1200)...\n",
      "Long segment (2117 tokens) → chunking (max 1200)...\n",
      "Long segment (1387 tokens) → chunking (max 1200)...\n",
      "Long segment (1498 tokens) → chunking (max 1200)...\n",
      "Long segment (1839 tokens) → chunking (max 1200)...\n",
      "Long segment (1780 tokens) → chunking (max 1200)...\n",
      "Long segment (1607 tokens) → chunking (max 1200)...\n",
      "Long segment (1900 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  66%|██████▌   | 331/500 [02:10<04:49,  1.71s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (3125 tokens) → chunking (max 1200)...\n",
      "Long segment (1410 tokens) → chunking (max 1200)...\n",
      "Long segment (1498 tokens) → chunking (max 1200)...\n",
      "Long segment (1942 tokens) → chunking (max 1200)...\n",
      "Long segment (1718 tokens) → chunking (max 1200)...\n",
      "Long segment (1233 tokens) → chunking (max 1200)...\n",
      "Long segment (1418 tokens) → chunking (max 1200)...\n",
      "Long segment (1282 tokens) → chunking (max 1200)...\n",
      "Long segment (1325 tokens) → chunking (max 1200)...\n",
      "Long segment (1827 tokens) → chunking (max 1200)...\n",
      "Long segment (2121 tokens) → chunking (max 1200)...\n",
      "Long segment (1494 tokens) → chunking (max 1200)...\n",
      "Long segment (1450 tokens) → chunking (max 1200)...\n",
      "Long segment (1463 tokens) → chunking (max 1200)...\n",
      "Long segment (2681 tokens) → chunking (max 1200)...\n",
      "Long segment (1299 tokens) → chunking (max 1200)...\n",
      "Long segment (1236 tokens) → chunking (max 1200)...\n",
      "Long segment (1255 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  66%|██████▋   | 332/500 [02:14<04:50,  1.73s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1231 tokens) → chunking (max 1200)...\n",
      "Long segment (1258 tokens) → chunking (max 1200)...\n",
      "Long segment (1398 tokens) → chunking (max 1200)...\n",
      "Long segment (1754 tokens) → chunking (max 1200)...\n",
      "Long segment (1473 tokens) → chunking (max 1200)...\n",
      "Long segment (1886 tokens) → chunking (max 1200)...\n",
      "Long segment (1539 tokens) → chunking (max 1200)...\n",
      "Long segment (1260 tokens) → chunking (max 1200)...\n",
      "Long segment (1206 tokens) → chunking (max 1200)...\n",
      "Long segment (1401 tokens) → chunking (max 1200)...\n",
      "Long segment (1532 tokens) → chunking (max 1200)...\n",
      "Long segment (1393 tokens) → chunking (max 1200)...\n",
      "Long segment (1276 tokens) → chunking (max 1200)...\n",
      "Long segment (2350 tokens) → chunking (max 1200)...\n",
      "Long segment (1469 tokens) → chunking (max 1200)...\n",
      "Long segment (1280 tokens) → chunking (max 1200)...\n",
      "Long segment (3182 tokens) → chunking (max 1200)...\n",
      "Long segment (1530 tokens) → chunking (max 1200)...\n",
      "Long segment (1537 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  67%|██████▋   | 334/500 [02:16<04:44,  1.71s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1664 tokens) → chunking (max 1200)...\n",
      "Long segment (1586 tokens) → chunking (max 1200)...\n",
      "Long segment (1487 tokens) → chunking (max 1200)...\n",
      "Long segment (1698 tokens) → chunking (max 1200)...\n",
      "Long segment (1683 tokens) → chunking (max 1200)...\n",
      "Long segment (2055 tokens) → chunking (max 1200)...\n",
      "Long segment (2048 tokens) → chunking (max 1200)...\n",
      "Long segment (1899 tokens) → chunking (max 1200)...\n",
      "Long segment (1558 tokens) → chunking (max 1200)...\n",
      "Long segment (1776 tokens) → chunking (max 1200)...\n",
      "Long segment (1278 tokens) → chunking (max 1200)...\n",
      "Long segment (2203 tokens) → chunking (max 1200)...\n",
      "Long segment (1454 tokens) → chunking (max 1200)...\n",
      "Long segment (1459 tokens) → chunking (max 1200)...\n",
      "Long segment (1333 tokens) → chunking (max 1200)...\n",
      "Long segment (1224 tokens) → chunking (max 1200)...\n",
      "Long segment (1365 tokens) → chunking (max 1200)...\n",
      "Long segment (1438 tokens) → chunking (max 1200)...\n",
      "Long segment (1649 tokens) → chunking (max 1200)...\n",
      "Long segment (1411 tokens) → chunking (max 1200)...\n",
      "Long segment (1645 tokens) → chunking (max 1200)...\n",
      "Long segment (1692 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  67%|██████▋   | 335/500 [02:16<03:31,  1.28s/doc]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1371 tokens) → chunking (max 1200)...\n",
      "Long segment (1379 tokens) → chunking (max 1200)...\n",
      "Long segment (1257 tokens) → chunking (max 1200)...\n",
      "Long segment (1622 tokens) → chunking (max 1200)...\n",
      "Long segment (1784 tokens) → chunking (max 1200)...\n",
      "Long segment (1384 tokens) → chunking (max 1200)...\n",
      "Long segment (1334 tokens) → chunking (max 1200)...\n",
      "Long segment (1287 tokens) → chunking (max 1200)...\n",
      "Long segment (1249 tokens) → chunking (max 1200)...\n",
      "Long segment (1484 tokens) → chunking (max 1200)...\n",
      "Long segment (1645 tokens) → chunking (max 1200)...\n",
      "Long segment (1244 tokens) → chunking (max 1200)...\n",
      "Long segment (1602 tokens) → chunking (max 1200)...\n",
      "Long segment (1243 tokens) → chunking (max 1200)...\n",
      "Long segment (1476 tokens) → chunking (max 1200)...\n",
      "Long segment (1270 tokens) → chunking (max 1200)...\n",
      "Long segment (1615 tokens) → chunking (max 1200)...\n",
      "Long segment (1299 tokens) → chunking (max 1200)...\n",
      "Long segment (1332 tokens) → chunking (max 1200)...\n",
      "Long segment (1764 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  68%|██████▊   | 338/500 [02:18<02:13,  1.21doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1235 tokens) → chunking (max 1200)...\n",
      "Long segment (1391 tokens) → chunking (max 1200)...\n",
      "Long segment (1405 tokens) → chunking (max 1200)...\n",
      "Long segment (1737 tokens) → chunking (max 1200)...\n",
      "Long segment (1632 tokens) → chunking (max 1200)...\n",
      "Long segment (1412 tokens) → chunking (max 1200)...\n",
      "Long segment (1720 tokens) → chunking (max 1200)...\n",
      "Long segment (1405 tokens) → chunking (max 1200)...\n",
      "Long segment (1320 tokens) → chunking (max 1200)...\n",
      "Long segment (2694 tokens) → chunking (max 1200)...\n",
      "Long segment (1355 tokens) → chunking (max 1200)...\n",
      "Long segment (1853 tokens) → chunking (max 1200)...\n",
      "Long segment (1712 tokens) → chunking (max 1200)...\n",
      "Long segment (1615 tokens) → chunking (max 1200)...\n",
      "Long segment (1439 tokens) → chunking (max 1200)...\n",
      "Long segment (1947 tokens) → chunking (max 1200)...\n",
      "Long segment (1532 tokens) → chunking (max 1200)...\n",
      "Long segment (1222 tokens) → chunking (max 1200)...\n",
      "Long segment (1635 tokens) → chunking (max 1200)...\n",
      "Long segment (1324 tokens) → chunking (max 1200)...\n",
      "Long segment (1422 tokens) → chunking (max 1200)...\n",
      "Long segment (1536 tokens) → chunking (max 1200)...\n",
      "Long segment (1305 tokens) → chunking (max 1200)...\n",
      "Long segment (1655 tokens) → chunking (max 1200)...\n",
      "Long segment (1680 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  91%|█████████ | 455/500 [02:19<00:00, 81.78doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1642 tokens) → chunking (max 1200)...Long segment (2071 tokens) → chunking (max 1200)...\n",
      "\n",
      "Long segment (1296 tokens) → chunking (max 1200)...\n",
      "Long segment (1551 tokens) → chunking (max 1200)...\n",
      "Long segment (1533 tokens) → chunking (max 1200)...\n",
      "Long segment (1241 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents:  99%|█████████▉| 496/500 [02:19<00:00, 96.42doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1470 tokens) → chunking (max 1200)...\n",
      "Long segment (1398 tokens) → chunking (max 1200)...\n",
      "Long segment (1441 tokens) → chunking (max 1200)...\n",
      "Long segment (1534 tokens) → chunking (max 1200)...\n",
      "Long segment (1396 tokens) → chunking (max 1200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new documents: 100%|██████████| 500/500 [02:20<00:00,  3.57doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long segment (1298 tokens) → chunking (max 1200)...\n",
      "\n",
      "Starting safe batch insert of 195055 new segments (batch size: 500)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Inserted 500, Modified 0\n",
      "Batch 2: Inserted 498, Modified 0\n",
      "Batch 3: Inserted 500, Modified 0\n",
      "Batch 4: Inserted 500, Modified 0\n",
      "Batch 5: Inserted 497, Modified 0\n",
      "Batch 6: Inserted 499, Modified 0\n",
      "Batch 7: Inserted 499, Modified 0\n",
      "Batch 8: Inserted 499, Modified 0\n",
      "Batch 9: Inserted 500, Modified 0\n",
      "Batch 10: Inserted 498, Modified 0\n",
      "Batch 11: Inserted 500, Modified 0\n",
      "Batch 12: Inserted 498, Modified 0\n",
      "Batch 13: Inserted 500, Modified 0\n",
      "Batch 14: Inserted 499, Modified 0\n",
      "Batch 15: Inserted 499, Modified 0\n",
      "Batch 16: Inserted 497, Modified 0\n",
      "Batch 17: Inserted 499, Modified 0\n",
      "Batch 18: Inserted 500, Modified 0\n",
      "Batch 19: Inserted 485, Modified 0\n",
      "Batch 20: Inserted 481, Modified 0\n",
      "Batch 21: Inserted 499, Modified 0\n",
      "Batch 22: Inserted 498, Modified 0\n",
      "Batch 23: Inserted 500, Modified 0\n",
      "Batch 24: Inserted 500, Modified 0\n",
      "Batch 25: Inserted 497, Modified 0\n",
      "Batch 26: Inserted 499, Modified 0\n",
      "Batch 27: Inserted 500, Modified 0\n",
      "Batch 28: Inserted 498, Modified 0\n",
      "Batch 29: Inserted 498, Modified 0\n",
      "Batch 30: Inserted 499, Modified 0\n",
      "Batch 31: Inserted 499, Modified 0\n",
      "Batch 32: Inserted 499, Modified 0\n",
      "Batch 33: Inserted 499, Modified 0\n",
      "Batch 34: Inserted 497, Modified 0\n",
      "Batch 35: Inserted 499, Modified 0\n",
      "Batch 36: Inserted 500, Modified 0\n",
      "Batch 37: Inserted 496, Modified 0\n",
      "Batch 38: Inserted 499, Modified 0\n",
      "Batch 39: Inserted 487, Modified 0\n",
      "Batch 40: Inserted 464, Modified 0\n",
      "Batch 41: Inserted 487, Modified 0\n",
      "Batch 42: Inserted 499, Modified 0\n",
      "Batch 43: Inserted 500, Modified 0\n",
      "Batch 44: Inserted 500, Modified 0\n",
      "Batch 45: Inserted 500, Modified 0\n",
      "Batch 46: Inserted 500, Modified 0\n",
      "Batch 47: Inserted 499, Modified 0\n",
      "Batch 48: Inserted 500, Modified 0\n",
      "Batch 49: Inserted 500, Modified 0\n",
      "Batch 50: Inserted 500, Modified 0\n",
      "Batch 51: Inserted 500, Modified 0\n",
      "Batch 52: Inserted 497, Modified 0\n",
      "Batch 53: Inserted 489, Modified 0\n",
      "Batch 54: Inserted 490, Modified 0\n",
      "Batch 55: Inserted 500, Modified 0\n",
      "Batch 56: Inserted 500, Modified 0\n",
      "Batch 57: Inserted 492, Modified 0\n",
      "Batch 58: Inserted 499, Modified 0\n",
      "Batch 59: Inserted 499, Modified 0\n",
      "Batch 60: Inserted 499, Modified 0\n",
      "Batch 61: Inserted 500, Modified 0\n",
      "Batch 62: Inserted 500, Modified 0\n",
      "Batch 63: Inserted 499, Modified 0\n",
      "Batch 64: Inserted 497, Modified 0\n",
      "Batch 65: Inserted 500, Modified 0\n",
      "Batch 66: Inserted 500, Modified 0\n",
      "Batch 67: Inserted 500, Modified 0\n",
      "Batch 68: Inserted 500, Modified 0\n",
      "Batch 69: Inserted 499, Modified 0\n",
      "Batch 70: Inserted 497, Modified 0\n",
      "Batch 71: Inserted 500, Modified 0\n",
      "Batch 72: Inserted 498, Modified 0\n",
      "Batch 73: Inserted 494, Modified 0\n",
      "Batch 74: Inserted 500, Modified 0\n",
      "Batch 75: Inserted 500, Modified 0\n",
      "Batch 76: Inserted 500, Modified 0\n",
      "Batch 77: Inserted 500, Modified 0\n",
      "Batch 78: Inserted 498, Modified 0\n",
      "Batch 79: Inserted 500, Modified 0\n",
      "Batch 80: Inserted 499, Modified 0\n",
      "Batch 81: Inserted 499, Modified 0\n",
      "Batch 82: Inserted 494, Modified 0\n",
      "Batch 83: Inserted 500, Modified 0\n",
      "Batch 84: Inserted 499, Modified 0\n",
      "Batch 85: Inserted 495, Modified 0\n",
      "Batch 86: Inserted 499, Modified 0\n",
      "Batch 87: Inserted 499, Modified 0\n",
      "Batch 88: Inserted 498, Modified 0\n",
      "Batch 89: Inserted 499, Modified 0\n",
      "Batch 90: Inserted 498, Modified 0\n",
      "Batch 91: Inserted 498, Modified 0\n",
      "Batch 92: Inserted 495, Modified 0\n",
      "Batch 93: Inserted 500, Modified 0\n",
      "Batch 94: Inserted 497, Modified 0\n",
      "Batch 95: Inserted 492, Modified 0\n",
      "Batch 96: Inserted 481, Modified 0\n",
      "Batch 97: Inserted 490, Modified 0\n",
      "Batch 98: Inserted 494, Modified 0\n",
      "Batch 99: Inserted 497, Modified 0\n",
      "Batch 100: Inserted 492, Modified 0\n",
      "Batch 101: Inserted 496, Modified 0\n",
      "Batch 102: Inserted 495, Modified 0\n",
      "Batch 103: Inserted 486, Modified 0\n",
      "Batch 104: Inserted 487, Modified 0\n",
      "Batch 105: Inserted 492, Modified 0\n",
      "Batch 106: Inserted 486, Modified 0\n",
      "Batch 107: Inserted 489, Modified 0\n",
      "Batch 108: Inserted 482, Modified 0\n",
      "Batch 109: Inserted 492, Modified 0\n",
      "Batch 110: Inserted 477, Modified 0\n",
      "Batch 111: Inserted 478, Modified 0\n",
      "Batch 112: Inserted 494, Modified 0\n",
      "Batch 113: Inserted 491, Modified 0\n",
      "Batch 114: Inserted 485, Modified 0\n",
      "Batch 115: Inserted 489, Modified 0\n",
      "Batch 116: Inserted 481, Modified 0\n",
      "Batch 117: Inserted 493, Modified 0\n",
      "Batch 118: Inserted 497, Modified 0\n",
      "Batch 119: Inserted 475, Modified 0\n",
      "Batch 120: Inserted 469, Modified 0\n",
      "Batch 121: Inserted 472, Modified 0\n",
      "Batch 122: Inserted 477, Modified 0\n",
      "Batch 123: Inserted 471, Modified 0\n",
      "Batch 124: Inserted 476, Modified 0\n",
      "Batch 125: Inserted 469, Modified 0\n",
      "Batch 126: Inserted 482, Modified 0\n",
      "Batch 127: Inserted 474, Modified 0\n",
      "Batch 128: Inserted 428, Modified 0\n",
      "Batch 129: Inserted 486, Modified 0\n",
      "Batch 130: Inserted 478, Modified 0\n",
      "Batch 131: Inserted 461, Modified 0\n",
      "Batch 132: Inserted 486, Modified 0\n",
      "Batch 133: Inserted 489, Modified 0\n",
      "Batch 134: Inserted 433, Modified 0\n",
      "Batch 135: Inserted 465, Modified 0\n",
      "Batch 136: Inserted 480, Modified 0\n",
      "Batch 137: Inserted 444, Modified 0\n",
      "Batch 138: Inserted 440, Modified 0\n",
      "Batch 139: Inserted 483, Modified 0\n",
      "Batch 140: Inserted 481, Modified 0\n",
      "Batch 141: Inserted 466, Modified 0\n",
      "Batch 142: Inserted 480, Modified 0\n",
      "Batch 143: Inserted 476, Modified 0\n",
      "Batch 144: Inserted 481, Modified 0\n",
      "Batch 145: Inserted 473, Modified 0\n",
      "Batch 146: Inserted 461, Modified 0\n",
      "Batch 147: Inserted 479, Modified 0\n",
      "Batch 148: Inserted 471, Modified 0\n",
      "Batch 149: Inserted 433, Modified 0\n",
      "Batch 150: Inserted 474, Modified 0\n",
      "Batch 151: Inserted 467, Modified 0\n",
      "Batch 152: Inserted 469, Modified 0\n",
      "Batch 153: Inserted 484, Modified 0\n",
      "Batch 154: Inserted 479, Modified 0\n",
      "Batch 155: Inserted 471, Modified 0\n",
      "Batch 156: Inserted 467, Modified 0\n",
      "Batch 157: Inserted 458, Modified 0\n",
      "Batch 158: Inserted 472, Modified 0\n",
      "Batch 159: Inserted 467, Modified 0\n",
      "Batch 160: Inserted 472, Modified 0\n",
      "Batch 161: Inserted 479, Modified 0\n",
      "Batch 162: Inserted 474, Modified 0\n",
      "Batch 163: Inserted 475, Modified 0\n",
      "Batch 164: Inserted 460, Modified 0\n",
      "Batch 165: Inserted 475, Modified 0\n",
      "Batch 166: Inserted 468, Modified 0\n",
      "Batch 167: Inserted 477, Modified 0\n",
      "Batch 168: Inserted 477, Modified 0\n",
      "Batch 169: Inserted 482, Modified 0\n",
      "Batch 170: Inserted 470, Modified 0\n",
      "Batch 171: Inserted 475, Modified 0\n",
      "Batch 172: Inserted 475, Modified 0\n",
      "Batch 173: Inserted 475, Modified 0\n",
      "Batch 174: Inserted 493, Modified 0\n",
      "Batch 175: Inserted 476, Modified 0\n",
      "Batch 176: Inserted 473, Modified 0\n",
      "Batch 177: Inserted 479, Modified 0\n",
      "Batch 178: Inserted 473, Modified 0\n",
      "Batch 179: Inserted 462, Modified 0\n",
      "Batch 180: Inserted 472, Modified 0\n",
      "Batch 181: Inserted 486, Modified 0\n",
      "Batch 182: Inserted 478, Modified 0\n",
      "Batch 183: Inserted 480, Modified 0\n",
      "Batch 184: Inserted 486, Modified 0\n",
      "Batch 185: Inserted 487, Modified 0\n",
      "Batch 186: Inserted 479, Modified 0\n",
      "Batch 187: Inserted 474, Modified 0\n",
      "Batch 188: Inserted 487, Modified 0\n",
      "Batch 189: Inserted 474, Modified 0\n",
      "Batch 190: Inserted 465, Modified 0\n",
      "Batch 191: Inserted 480, Modified 0\n",
      "Batch 192: Inserted 471, Modified 0\n",
      "Batch 193: Inserted 453, Modified 0\n",
      "Batch 194: Inserted 475, Modified 0\n",
      "Batch 195: Inserted 452, Modified 0\n",
      "Batch 196: Inserted 459, Modified 0\n",
      "Batch 197: Inserted 486, Modified 0\n",
      "Batch 198: Inserted 465, Modified 0\n",
      "Batch 199: Inserted 469, Modified 0\n",
      "Batch 200: Inserted 471, Modified 0\n",
      "Batch 201: Inserted 467, Modified 0\n",
      "Batch 202: Inserted 439, Modified 0\n",
      "Batch 203: Inserted 483, Modified 0\n",
      "Batch 204: Inserted 457, Modified 0\n",
      "Batch 205: Inserted 467, Modified 0\n",
      "Batch 206: Inserted 465, Modified 0\n",
      "Batch 207: Inserted 478, Modified 0\n",
      "Batch 208: Inserted 473, Modified 0\n",
      "Batch 209: Inserted 486, Modified 0\n",
      "Batch 210: Inserted 460, Modified 0\n",
      "Batch 211: Inserted 475, Modified 0\n",
      "Batch 212: Inserted 464, Modified 0\n",
      "Batch 213: Inserted 457, Modified 0\n",
      "Batch 214: Inserted 477, Modified 0\n",
      "Batch 215: Inserted 470, Modified 0\n",
      "Batch 216: Inserted 473, Modified 0\n",
      "Batch 217: Inserted 479, Modified 0\n",
      "Batch 218: Inserted 476, Modified 0\n",
      "Batch 219: Inserted 475, Modified 0\n",
      "Batch 220: Inserted 470, Modified 0\n",
      "Batch 221: Inserted 478, Modified 0\n",
      "Batch 222: Inserted 475, Modified 0\n",
      "Batch 223: Inserted 464, Modified 0\n",
      "Batch 224: Inserted 465, Modified 0\n",
      "Batch 225: Inserted 480, Modified 0\n",
      "Batch 226: Inserted 495, Modified 0\n",
      "Batch 227: Inserted 476, Modified 0\n",
      "Batch 228: Inserted 476, Modified 0\n",
      "Batch 229: Inserted 472, Modified 0\n",
      "Batch 230: Inserted 476, Modified 0\n",
      "Batch 231: Inserted 477, Modified 0\n",
      "Batch 232: Inserted 470, Modified 0\n",
      "Batch 233: Inserted 456, Modified 0\n",
      "Batch 234: Inserted 464, Modified 0\n",
      "Batch 235: Inserted 452, Modified 0\n",
      "Batch 236: Inserted 469, Modified 0\n",
      "Batch 237: Inserted 474, Modified 0\n",
      "Batch 238: Inserted 478, Modified 0\n",
      "Batch 239: Inserted 467, Modified 0\n",
      "Batch 240: Inserted 453, Modified 0\n",
      "Batch 241: Inserted 464, Modified 0\n",
      "Batch 242: Inserted 482, Modified 0\n",
      "Batch 243: Inserted 465, Modified 0\n",
      "Batch 244: Inserted 480, Modified 0\n",
      "Batch 245: Inserted 471, Modified 0\n",
      "Batch 246: Inserted 478, Modified 0\n",
      "Batch 247: Inserted 468, Modified 0\n",
      "Batch 248: Inserted 464, Modified 0\n",
      "Batch 249: Inserted 468, Modified 0\n",
      "Batch 250: Inserted 468, Modified 0\n",
      "Batch 251: Inserted 454, Modified 0\n",
      "Batch 252: Inserted 473, Modified 0\n",
      "Batch 253: Inserted 479, Modified 0\n",
      "Batch 254: Inserted 486, Modified 0\n",
      "Batch 255: Inserted 466, Modified 0\n",
      "Batch 256: Inserted 465, Modified 0\n",
      "Batch 257: Inserted 478, Modified 0\n",
      "Batch 258: Inserted 482, Modified 0\n",
      "Batch 259: Inserted 473, Modified 0\n",
      "Batch 260: Inserted 467, Modified 0\n",
      "Batch 261: Inserted 483, Modified 0\n",
      "Batch 262: Inserted 469, Modified 0\n",
      "Batch 263: Inserted 462, Modified 0\n",
      "Batch 264: Inserted 472, Modified 0\n",
      "Batch 265: Inserted 466, Modified 0\n",
      "Batch 266: Inserted 479, Modified 0\n",
      "Batch 267: Inserted 479, Modified 0\n",
      "Batch 268: Inserted 477, Modified 0\n",
      "Batch 269: Inserted 482, Modified 0\n",
      "Batch 270: Inserted 464, Modified 0\n",
      "Batch 271: Inserted 459, Modified 0\n",
      "Batch 272: Inserted 477, Modified 0\n",
      "Batch 273: Inserted 475, Modified 0\n",
      "Batch 274: Inserted 479, Modified 0\n",
      "Batch 275: Inserted 468, Modified 0\n",
      "Batch 276: Inserted 471, Modified 0\n",
      "Batch 277: Inserted 490, Modified 0\n",
      "Batch 278: Inserted 463, Modified 0\n",
      "Batch 279: Inserted 468, Modified 0\n",
      "Batch 280: Inserted 466, Modified 0\n",
      "Batch 281: Inserted 472, Modified 0\n",
      "Batch 282: Inserted 485, Modified 0\n",
      "Batch 283: Inserted 490, Modified 0\n",
      "Batch 284: Inserted 455, Modified 0\n",
      "Batch 285: Inserted 457, Modified 0\n",
      "Batch 286: Inserted 455, Modified 0\n",
      "Batch 287: Inserted 479, Modified 0\n",
      "Batch 288: Inserted 480, Modified 0\n",
      "Batch 289: Inserted 477, Modified 0\n",
      "Batch 290: Inserted 460, Modified 0\n",
      "Batch 291: Inserted 470, Modified 0\n",
      "Batch 292: Inserted 472, Modified 0\n",
      "Batch 293: Inserted 457, Modified 0\n",
      "Batch 294: Inserted 457, Modified 0\n",
      "Batch 295: Inserted 467, Modified 0\n",
      "Batch 296: Inserted 463, Modified 0\n",
      "Batch 297: Inserted 461, Modified 0\n",
      "Batch 298: Inserted 478, Modified 0\n",
      "Batch 299: Inserted 480, Modified 0\n",
      "Batch 300: Inserted 475, Modified 0\n",
      "Batch 301: Inserted 475, Modified 0\n",
      "Batch 302: Inserted 483, Modified 0\n",
      "Batch 303: Inserted 450, Modified 0\n",
      "Batch 304: Inserted 466, Modified 0\n",
      "Batch 305: Inserted 480, Modified 0\n",
      "Batch 306: Inserted 459, Modified 0\n",
      "Batch 307: Inserted 471, Modified 0\n",
      "Batch 308: Inserted 459, Modified 0\n",
      "Batch 309: Inserted 453, Modified 0\n",
      "Batch 310: Inserted 478, Modified 0\n",
      "Batch 311: Inserted 475, Modified 0\n",
      "Batch 312: Inserted 460, Modified 0\n",
      "Batch 313: Inserted 450, Modified 0\n",
      "Batch 314: Inserted 469, Modified 0\n",
      "Batch 315: Inserted 462, Modified 0\n",
      "Batch 316: Inserted 471, Modified 0\n",
      "Batch 317: Inserted 473, Modified 0\n",
      "Batch 318: Inserted 478, Modified 0\n",
      "Batch 319: Inserted 469, Modified 0\n",
      "Batch 320: Inserted 467, Modified 0\n",
      "Batch 321: Inserted 479, Modified 0\n",
      "Batch 322: Inserted 477, Modified 0\n",
      "Batch 323: Inserted 475, Modified 0\n",
      "Batch 324: Inserted 477, Modified 0\n",
      "Batch 325: Inserted 462, Modified 0\n",
      "Batch 326: Inserted 464, Modified 0\n",
      "Batch 327: Inserted 473, Modified 0\n",
      "Batch 328: Inserted 478, Modified 0\n",
      "Batch 329: Inserted 474, Modified 0\n",
      "Batch 330: Inserted 478, Modified 0\n",
      "Batch 331: Inserted 454, Modified 0\n",
      "Batch 332: Inserted 456, Modified 0\n",
      "Batch 333: Inserted 484, Modified 0\n",
      "Batch 334: Inserted 481, Modified 0\n",
      "Batch 335: Inserted 480, Modified 0\n",
      "Batch 336: Inserted 480, Modified 0\n",
      "Batch 337: Inserted 467, Modified 0\n",
      "Batch 338: Inserted 467, Modified 0\n",
      "Batch 339: Inserted 485, Modified 0\n",
      "Batch 340: Inserted 458, Modified 0\n",
      "Batch 341: Inserted 455, Modified 0\n",
      "Batch 342: Inserted 472, Modified 0\n",
      "Batch 343: Inserted 483, Modified 0\n",
      "Batch 344: Inserted 478, Modified 0\n",
      "Batch 345: Inserted 473, Modified 0\n",
      "Batch 346: Inserted 472, Modified 0\n",
      "Batch 347: Inserted 473, Modified 0\n",
      "Batch 348: Inserted 481, Modified 0\n",
      "Batch 349: Inserted 461, Modified 0\n",
      "Batch 350: Inserted 446, Modified 0\n",
      "Batch 351: Inserted 454, Modified 0\n",
      "Batch 352: Inserted 477, Modified 0\n",
      "Batch 353: Inserted 458, Modified 0\n",
      "Batch 354: Inserted 469, Modified 0\n",
      "Batch 355: Inserted 462, Modified 0\n",
      "Batch 356: Inserted 476, Modified 0\n",
      "Batch 357: Inserted 479, Modified 0\n",
      "Batch 358: Inserted 456, Modified 0\n",
      "Batch 359: Inserted 440, Modified 0\n",
      "Batch 360: Inserted 473, Modified 0\n",
      "Batch 361: Inserted 482, Modified 0\n",
      "Batch 362: Inserted 469, Modified 0\n",
      "Batch 363: Inserted 476, Modified 0\n",
      "Batch 364: Inserted 473, Modified 0\n",
      "Batch 365: Inserted 467, Modified 0\n",
      "Batch 366: Inserted 475, Modified 0\n",
      "Batch 367: Inserted 461, Modified 0\n",
      "Batch 368: Inserted 455, Modified 0\n",
      "Batch 369: Inserted 473, Modified 0\n",
      "Batch 370: Inserted 467, Modified 0\n",
      "Batch 371: Inserted 476, Modified 0\n",
      "Batch 372: Inserted 465, Modified 0\n",
      "Batch 373: Inserted 471, Modified 0\n",
      "Batch 374: Inserted 450, Modified 0\n",
      "Batch 375: Inserted 455, Modified 0\n",
      "Batch 376: Inserted 478, Modified 0\n",
      "Batch 377: Inserted 484, Modified 0\n",
      "Batch 378: Inserted 474, Modified 0\n",
      "Batch 379: Inserted 477, Modified 0\n",
      "Batch 380: Inserted 476, Modified 0\n",
      "Batch 381: Inserted 449, Modified 0\n",
      "Batch 382: Inserted 475, Modified 0\n",
      "Batch 383: Inserted 463, Modified 0\n",
      "Batch 384: Inserted 462, Modified 0\n",
      "Batch 385: Inserted 477, Modified 0\n",
      "Batch 386: Inserted 464, Modified 0\n",
      "Batch 387: Inserted 459, Modified 0\n",
      "Batch 388: Inserted 468, Modified 0\n",
      "Batch 389: Inserted 458, Modified 0\n",
      "Batch 390: Inserted 465, Modified 0\n",
      "Batch 391: Inserted 54, Modified 0\n",
      "\n",
      "=== Full Preprocessing Complete ===\n",
      "Total time: 2.5 minutes\n",
      "New segments inserted: 195055\n",
      "Overall reduction: 32.2%\n",
      "Collection: hansard_cpatf500\n",
      "\n",
      "CPATF preprocessing done\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from pymongo.errors import ConnectionFailure, BulkWriteError\n",
    "from pymongo import UpdateOne\n",
    "\n",
    "# Full preprocessing - resume-safe (avoid duplicates if interrupted)\n",
    "\n",
    "all_docs = list(segmented_col.find({}))\n",
    "total_docs = len(all_docs)\n",
    "\n",
    "if total_docs == 0:\n",
    "    print(\"Error: No documents found in segmented_col.\")\n",
    "else:\n",
    "    print(f\"Found {total_docs} documents in segmented_col.\")\n",
    "    print(\"Resume-safe preprocessing: skipping already processed parent_doc_id\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Get already processed parent_doc_ids to skip\n",
    "existing_parent_ids = set(\n",
    "    seg[\"parent_doc_id\"] for seg in cpatf_col.find({}, {\"parent_doc_id\": 1})\n",
    ")\n",
    "print(f\"Found {len(existing_parent_ids)} already processed parent_doc_ids - skipping them\")\n",
    "\n",
    "# Filter only unprocessed docs\n",
    "unprocessed_docs = [doc for doc in all_docs if doc[\"_id\"] not in existing_parent_ids]\n",
    "print(f\"Need to process {len(unprocessed_docs)} new documents\")\n",
    "\n",
    "all_cleaned_segments = []\n",
    "total_original_tokens = 0\n",
    "total_cleaned_tokens = 0\n",
    "\n",
    "BATCH_SIZE = 500  # Safe batch\n",
    "\n",
    "def process_full_document_wrapper(doc):\n",
    "    segments = process_full_document(doc)\n",
    "    return segments\n",
    "\n",
    "# Multi-thread processing only on unprocessed docs\n",
    "if unprocessed_docs:\n",
    "    with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        futures = [executor.submit(process_full_document_wrapper, doc) for doc in unprocessed_docs]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(unprocessed_docs), desc=\"Processing new documents\", unit=\"doc\"):\n",
    "            segments = future.result()\n",
    "            all_cleaned_segments.extend(segments)\n",
    "            \n",
    "            for seg in segments:\n",
    "                total_original_tokens += seg[\"original_token_count\"]\n",
    "                total_cleaned_tokens += seg[\"cleaned_token_count\"]\n",
    "            \n",
    "            if len(all_cleaned_segments) % 2000 == 0:\n",
    "                gc.collect()\n",
    "else:\n",
    "    print(\"All documents already processed - nothing to do!\")\n",
    "\n",
    "# Batch insert with upsert (avoid duplicates even if interrupted)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "if all_cleaned_segments:\n",
    "    print(f\"\\nStarting safe batch insert of {len(all_cleaned_segments)} new segments (batch size: {BATCH_SIZE})...\")\n",
    "    \n",
    "    # Use upsert with unique key (parent_doc_id + original_text hash)\n",
    "    # Create index first \n",
    "    cpatf_col.create_index([(\"parent_doc_id\", 1), (\"original_text_hash\", 1)], unique=True)\n",
    "    \n",
    "    for i in range(0, len(all_cleaned_segments), BATCH_SIZE):\n",
    "        batch = all_cleaned_segments[i:i + BATCH_SIZE]\n",
    "        operations = []\n",
    "        for seg in batch:\n",
    "            # Add hash for dedup\n",
    "            import hashlib\n",
    "            text_hash = hashlib.md5(seg[\"original_text\"].encode('utf-8')).hexdigest()\n",
    "            seg[\"original_text_hash\"] = text_hash\n",
    "            \n",
    "            operations.append(\n",
    "                UpdateOne(\n",
    "                    {\"parent_doc_id\": seg[\"parent_doc_id\"], \"original_text_hash\": text_hash},\n",
    "                    {\"$setOnInsert\": seg},\n",
    "                    upsert=True\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                result = cpatf_col.bulk_write(operations, ordered=False)\n",
    "                print(f\"Batch {i//BATCH_SIZE + 1}: Inserted {result.upserted_count}, Modified {result.modified_count}\")\n",
    "                break\n",
    "            except (ConnectionFailure, BulkWriteError) as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Batch failed (attempt {retry_count}/{max_retries}): {e}\")\n",
    "                time.sleep(5 * retry_count)\n",
    "        else:\n",
    "            print(\"Batch failed after max retries\")\n",
    "    \n",
    "    overall_reduction = 100 * (1 - total_cleaned_tokens / total_original_tokens) if total_original_tokens > 0 else 0\n",
    "    \n",
    "    print(f\"\\n=== Full Preprocessing Complete ===\")\n",
    "    print(f\"Total time: {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"New segments inserted: {len(all_cleaned_segments)}\")\n",
    "    print(f\"Overall reduction: {overall_reduction:.1f}%\")\n",
    "    print(f\"Collection: hansard_cpatf500\")\n",
    "else:\n",
    "    print(\"No new segments to process\")\n",
    "\n",
    "print(\"\\nCPATF preprocessing done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
