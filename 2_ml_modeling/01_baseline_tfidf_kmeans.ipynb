{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98757c5",
   "metadata": {},
   "source": [
    "# Pipeline 1: TF-IDF + KMeans Clustering (Classical Baseline)\n",
    "\n",
    "## Purpose\n",
    "This notebook implements **Pipeline 1** of the six comparative experiments defined in the FYP project \"NLP-based AI Platform for Parliament Proceedings Analysis to Enhance Governmental Transparency and Civic Engagement\".\n",
    "\n",
    "- **Input**: Raw full_text from hansard_core500 collection (train split only, **no CPATF preprocessing**)\n",
    "- **Method**: Classical TF-IDF vectorization + KMeans clustering\n",
    "- **Objective**: Establish the weakest baseline performance to highlight the incremental contributions of subsequent pipelines (especially CPATF preprocessing, MEHTC hybrid similarity, neural embeddings, and domain-specific fine-tuning).\n",
    "- **Clustering**: Fixed to 20 clusters (standard baseline setting)\n",
    "- **Hardware**: c2d-highcpu-32 (32 vCPUs, 64 GB RAM) with multi-threading optimization\n",
    "- **Metrics** (uniform across all 6 pipelines):\n",
    "  - Silhouette Score (extrinsic)\n",
    "  - C_V Coherence\n",
    "  - NPMI Coherence\n",
    "  - Topic Diversity\n",
    "- **Visualization**: Interactive network graph (big nodes = topics, small nodes = top words/sub-issues)\n",
    "- **Output**: Results saved as `results/results_pipeline1.json` for final comparison in `07_comparison_summary.ipynb`\n",
    "\n",
    "This pipeline intentionally uses raw noisy text and simple TF-IDF to produce the lowest expected scores, serving as the reference point for ablation study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14243e6",
   "metadata": {},
   "source": [
    "### Imports and MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e00a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 350 raw documents for Pipeline 1 (TF-IDF + KMeans)\n",
      "Using n_jobs=-1\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "import warnings\n",
    "from joblib import parallel_backend \n",
    "import multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_jobs = -1  # Use all available cores \n",
    "\n",
    "# Load environment variables\n",
    "project_root = Path.cwd().parents[0] if 'parents' in dir(Path.cwd()) else Path.cwd()\n",
    "backend_env_path = project_root / \"3_app_system\" / \"backend\" / \".env\"\n",
    "load_dotenv(backend_env_path)\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(os.getenv(\"MONGO_URI\"))\n",
    "db = client[\"MyParliament\"]\n",
    "\n",
    "# For Pipeline 1: Use RAW text from hansard_core500 (not CPATF cleaned)\n",
    "raw_col = db[\"hansard_core500\"]\n",
    "docs = list(raw_col.find({\"split_type\": \"train\"}, {\"full_text\": 1}))\n",
    "texts = [doc[\"full_text\"] for doc in docs if doc.get(\"full_text\") and doc[\"full_text\"].strip()]\n",
    "print(f\"Loaded {len(texts)} raw documents for Pipeline 1 (TF-IDF + KMeans)\")\n",
    "print(f\"Using n_jobs={n_jobs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4cdbd",
   "metadata": {},
   "source": [
    "### Bilingual TF-IDF Vectorization and KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c18af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 198 English + 475 Malay stopwords\n",
      "Total bilingual stopwords: 673\n",
      "TF-IDF matrix shape: (350, 226228)\n",
      "Pipeline 1 (Bilingual) - Silhouette Score: 0.0382\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK English stopwords if not already (run once)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load custom Malay stopwords from local file (stopwords-ms collection)\n",
    "malay_stopwords_path = \"stopwords-ms-MannualOp.txt\" \n",
    "with open(malay_stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    malay_stopwords = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Combine English (NLTK) + Malay (stopwords-ms)\n",
    "english_stopwords = list(nltk_stopwords.words('english'))\n",
    "combined_stopwords = english_stopwords + malay_stopwords\n",
    "\n",
    "print(f\"Loaded {len(english_stopwords)} English + {len(malay_stopwords)} Malay stopwords\")\n",
    "print(f\"Total bilingual stopwords: {len(combined_stopwords)}\")\n",
    "\n",
    "# Optimized TF-IDF for Malay-English code-switched parliament text\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.7,           \n",
    "    min_df=5,            \n",
    "    stop_words=combined_stopwords,   \n",
    "    ngram_range=(1, 3),   \n",
    "    sublinear_tf=True,    \n",
    "    lowercase=True  \n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# KMeans clustering \n",
    "n_clusters = 20\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=15,\n",
    ")\n",
    "labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Sampled silhouette for large matrices\n",
    "silhouette = silhouette_score(tfidf_matrix, labels, sample_size=15000, random_state=42)\n",
    "print(f\"Pipeline 1 (Bilingual) - Silhouette Score: {silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c47a9f",
   "metadata": {},
   "source": [
    "### Parallel Coherence (C_V and NPMI) and Topic Diversity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d38395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 empty clusters\n",
      "Pipeline 1 - C_V Coherence: 0.7946\n",
      "Pipeline 1 - NPMI Coherence: nan\n",
      "Pipeline 1 - Topic Diversity: 0.8500 (based on 18 topics)\n"
     ]
    }
   ],
   "source": [
    "def get_top_words_per_cluster(tfidf_matrix, labels, feature_names, top_n=10):\n",
    "    top_words = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_mask = (labels == i)\n",
    "        if cluster_mask.sum() == 0:\n",
    "            top_words.append([])\n",
    "            continue\n",
    "        cluster_tfidf = tfidf_matrix[cluster_mask].mean(axis=0)\n",
    "        cluster_tfidf = np.array(cluster_tfidf).flatten()\n",
    "        top_indices = cluster_tfidf.argsort()[-top_n:][::-1]\n",
    "        top_words.append([feature_names[idx] for idx in top_indices])\n",
    "    return top_words\n",
    "\n",
    "# Get top words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words_str_per_cluster = get_top_words_per_cluster(tfidf_matrix, labels, feature_names, top_n=10)\n",
    "\n",
    "# Tokenize for gensim\n",
    "tokenized_texts = [text.lower().split() for text in texts]\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Convert to token IDs for CoherenceModel\n",
    "top_words_ids_per_cluster = []\n",
    "for cluster_words in top_words_str_per_cluster:\n",
    "    ids = [dictionary.token2id[word] for word in cluster_words if word in dictionary.token2id]\n",
    "    top_words_ids_per_cluster.append(ids)\n",
    "\n",
    "# Filter empty clusters\n",
    "valid_top_words_ids = [ids for ids in top_words_ids_per_cluster if ids]\n",
    "valid_n_clusters = len(valid_top_words_ids)\n",
    "print(f\"Found {n_clusters - valid_n_clusters} empty clusters\")\n",
    "\n",
    "if valid_n_clusters == 0:\n",
    "    coherence_cv = coherence_npmi = 0.0\n",
    "    td = 0.0\n",
    "else:\n",
    "    coherence_cv = CoherenceModel(topics=valid_top_words_ids, texts=tokenized_texts, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_npmi = CoherenceModel(topics=valid_top_words_ids, texts=tokenized_texts, dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
    "\n",
    "# Topic Diversity\n",
    "all_top_words_set = set(word for cluster in top_words_str_per_cluster if cluster for word in cluster[:10])\n",
    "td = len(all_top_words_set) / (valid_n_clusters * 10) if valid_n_clusters > 0 else 0\n",
    "\n",
    "print(f\"Pipeline 1 - C_V Coherence: {coherence_cv:.4f}\")\n",
    "print(f\"Pipeline 1 - NPMI Coherence: {coherence_npmi:.4f}\")\n",
    "print(f\"Pipeline 1 - Topic Diversity: {td:.4f} (based on {valid_n_clusters} topics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64754d5b",
   "metadata": {},
   "source": [
    "### Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b17c6bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network graph saved: /mnt/data/MyParliament/2_ml_modeling/network_graph/pipeline1_network.html\n"
     ]
    }
   ],
   "source": [
    "def generate_network_graph(top_words_str_per_cluster, n_clusters):\n",
    "    # Create the directory if it doesn't exist\n",
    "    network_graph_dir = project_root / \"2_ml_modeling/network_graph\"\n",
    "    network_graph_dir.mkdir(parents=True, exist_ok=True)\n",
    "    html_path = network_graph_dir / \"pipeline1_network.html\"\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for cluster_id, words in enumerate(top_words_str_per_cluster):\n",
    "        if not words:\n",
    "            continue\n",
    "        topic_node = f\"Topic_{cluster_id}\"\n",
    "        G.add_node(topic_node, size=60, group=cluster_id, title=f\"Topic {cluster_id}\", shape='ellipse')\n",
    "        for word in words:\n",
    "            G.add_node(word, size=25, group=cluster_id)\n",
    "            G.add_edge(topic_node, word, weight=3)\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i+1, len(words)):\n",
    "                G.add_edge(words[i], words[j], weight=1)\n",
    "    \n",
    "    net = Network(height=\"900px\", width=\"100%\", notebook=True, cdn_resources='in_line')\n",
    "    net.from_nx(G)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    net.save_graph(str(html_path))\n",
    "    print(f\"Network graph saved: {html_path}\")\n",
    "\n",
    "generate_network_graph(top_words_str_per_cluster, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819aafb",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65630ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1 COMPLETED! All results saved.\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"pipeline\": \"01_tfidf_kmeans\",\n",
    "    \"silhouette\": float(silhouette),\n",
    "    \"coherence_cv\": float(coherence_cv),\n",
    "    \"coherence_npmi\": float(coherence_npmi),\n",
    "    \"topic_diversity\": float(td),\n",
    "    \"valid_clusters\": valid_n_clusters,\n",
    "    \"total_clusters\": n_clusters\n",
    "}\n",
    "\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "with open(\"results/pipeline1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Pipeline 1 COMPLETED! All results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92442f7",
   "metadata": {},
   "source": [
    "### Save the TF-IDF + KMeans model to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4dbe02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved to: /mnt/data/MyParliament/2_ml_modeling/model/tfidf_kmeans_model.pkl\n",
      "Included objects: kmeans, vectorizer, top words, cluster labels, evaluation metrics, etc.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "save_dir = project_root / \"2_ml_modeling/model\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "model_filename = save_dir / \"tfidf_kmeans_model.pkl\"\n",
    "\n",
    "# Bundle all important objects for future reuse\n",
    "model_data = {\n",
    "    \"kmeans\": kmeans,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"n_clusters\": n_clusters,\n",
    "    \"feature_names\": feature_names.tolist(),  \n",
    "    \"top_words_per_cluster\": top_words_str_per_cluster,\n",
    "    \"labels\": labels.tolist(),                \n",
    "    \"silhouette_score\": silhouette,\n",
    "    \"coherence_cv\": coherence_cv,\n",
    "    \"coherence_npmi\": coherence_npmi,\n",
    "    \"topic_diversity\": td,\n",
    "    \"saved_at\": datetime.now().isoformat(),\n",
    "    \"description\": \"Pipeline 1: TF-IDF + KMeans (20 clusters) on raw hansard_core500 with bilingual stopwords and n-grams (1-3)\"\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"Model successfully saved to: {model_filename}\")\n",
    "print(f\"Included objects: kmeans, vectorizer, top words, cluster labels, evaluation metrics, etc.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
